
=============== [2017/03/27 17:19:24, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:19:24] INFO:    SSHExec initializing ...
[17:19:24] INFO:    Session initialized and associated with user credential Hzint2011
[17:19:24] INFO:    SSHExec initialized successfully
[17:19:24] INFO:    SSHExec trying to connect root@10.1.20.137
[17:19:45] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: java.net.ConnectException: Connection timed out: connect
[17:19:45] ERROR:   session is down
[17:19:45] ERROR:   session is down
[17:19:45] INFO:    SSH connection shutdown

=============== [2017/03/27 17:26:17, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:26:17] INFO:    SSHExec initializing ...
[17:26:17] INFO:    Session initialized and associated with user credential Hzint2011
[17:26:17] INFO:    SSHExec initialized successfully
[17:26:17] INFO:    SSHExec trying to connect root@10.1.20.137
[17:26:37] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: java.net.ConnectException: Connection timed out: connect
[17:26:37] ERROR:   session is down
[17:26:37] ERROR:   session is down
[17:26:37] INFO:    SSH connection shutdown

=============== [2017/03/27 17:31:41, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:31:41] INFO:    SSHExec initializing ...
[17:31:41] INFO:    Session initialized and associated with user credential Hzint2011
[17:31:41] INFO:    SSHExec initialized successfully
[17:31:41] INFO:    SSHExec trying to connect root@10.1.20.137
[17:32:02] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: java.net.ConnectException: Connection timed out: connect
[17:32:02] ERROR:   session is down
[17:32:02] ERROR:   session is down
[17:32:02] INFO:    SSH connection shutdown

=============== [2017/03/27 17:48:10, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:48:10] INFO:    SSHExec initializing ...
[17:48:10] INFO:    Session initialized and associated with user credential Hzint2011
[17:48:10] INFO:    SSHExec initialized successfully
[17:48:10] INFO:    SSHExec trying to connect root@10.1.20.137
[17:48:31] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: java.net.ConnectException: Connection timed out: connect
[17:48:31] ERROR:   session is down
[17:48:31] ERROR:   session is down
[17:48:31] INFO:    SSH connection shutdown

=============== [2017/03/28 13:54:07, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[13:54:07] INFO:    SSHExec initializing ...
[13:54:07] INFO:    Session initialized and associated with user credential 123456
[13:54:07] INFO:    SSHExec initialized successfully
[13:54:07] INFO:    SSHExec trying to connect root@172.16.110.200
[13:54:08] INFO:    SSH connection established
[13:54:08] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dc_system;ALTER TABLE dc_system RENAME TO hl_bak.dc_system'
[13:54:08] INFO:    Connection channel established succesfully
[13:54:08] INFO:    Start to run command
[13:54:23] INFO:    Connection channel closed
[13:54:23] INFO:    Check if exec success or not ... 
[13:54:23] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dc_system;ALTER TABLE dc_system RENAME TO hl_bak.dc_system'
[13:54:23] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 13:54:34 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.721 secondsFAILED: SemanticException [Error 10001]: Table not found default.dc_system
[13:54:23] INFO:    Now wait 5 seconds to begin next task ...
[13:54:28] INFO:    Connection channel disconnect
[13:54:28] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table dc_system --columns SYSTEM_ID,SYSTEM_NUMBER,SYSTEM_NAME,SYSTEM_HOMES   --hive-table dc_system --hive-import --connect jdbc:mysql://10.1.20.86:3306/hzhldc_khsm --username hldc_h5 --password hldc_h5
[13:54:28] INFO:    Connection channel established succesfully
[13:54:28] INFO:    Start to run command
[13:54:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[13:54:33] INFO:    Connection channel closed
[13:54:33] INFO:    Check if exec success or not ... 
[13:54:33] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table dc_system --columns SYSTEM_ID,SYSTEM_NUMBER,SYSTEM_NAME,SYSTEM_HOMES   --hive-table dc_system --hive-import --connect jdbc:mysql://10.1.20.86:3306/hzhldc_khsm --username hldc_h5 --password hldc_h5
[13:54:33] INFO:    Error message: 17/03/28 13:54:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 13:54:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 13:54:54 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 13:54:54 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 13:54:54 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 13:54:54 INFO tool.CodeGenTool: Beginning code generation17/03/28 13:54:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_system` AS t LIMIT 117/03/28 13:54:55 ERROR util.SqlTypeMap: It seems like you are looking up a column that does not17/03/28 13:54:55 ERROR util.SqlTypeMap: exist in the table. Please ensure that you've specified17/03/28 13:54:55 ERROR util.SqlTypeMap: correct column names in Sqoop options.17/03/28 13:54:55 ERROR tool.ImportTool: Imported Failed: column not found: SYSTEM_NUMBER
[13:54:33] INFO:    Now wait 5 seconds to begin next task ...
[13:54:38] INFO:    Connection channel disconnect
[13:54:38] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS dc_system; ALTER TABLE hl_bak.dc_system RENAME TO dc_system'
[13:54:38] INFO:    Connection channel established succesfully
[13:54:38] INFO:    Start to run command
[13:54:51] INFO:    Connection channel closed
[13:54:51] INFO:    Check if exec success or not ... 
[13:54:51] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS dc_system; ALTER TABLE hl_bak.dc_system RENAME TO dc_system'
[13:54:51] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 13:55:06 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.658 secondsFAILED: SemanticException [Error 10001]: Table not found hl_bak.dc_system
[13:54:51] INFO:    Now wait 5 seconds to begin next task ...
[13:54:56] INFO:    Connection channel disconnect
[13:54:56] INFO:    SSH connection shutdown

=============== [2017/03/28 13:55:11, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[13:55:11] INFO:    SSHExec initializing ...
[13:55:11] INFO:    Session initialized and associated with user credential 123456
[13:55:11] INFO:    SSHExec initialized successfully
[13:55:11] INFO:    SSHExec trying to connect root@172.16.110.200
[13:55:12] INFO:    SSH connection established
[13:55:12] INFO:    Command is  sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[13:55:12] INFO:    Connection channel established succesfully
[13:55:12] INFO:    Start to run command
[13:55:12] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[13:55:22] INFO:    Connection channel closed
[13:55:22] INFO:    Check if exec success or not ... 
[13:55:22] INFO:    Execution failed while executing command:  sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[13:55:22] INFO:    Error message: 17/03/28 13:55:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 13:55:40 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 13:55:40 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 13:55:40 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 13:55:41 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 13:55:41 INFO tool.CodeGenTool: Beginning code generation17/03/28 13:55:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 13:55:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 13:55:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-root/compile/65ba7b49f9419f49537fe43aa2905c47/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 13:55:46 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/65ba7b49f9419f49537fe43aa2905c47/dc_obj_main.jar17/03/28 13:55:46 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 13:55:46 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 13:55:46 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 13:55:46 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 13:55:46 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 13:55:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 13:55:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 13:55:48 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 13:55:49 WARN security.UserGroupInformation: PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)17/03/28 13:55:49 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3084)	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3049)	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:957)	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:953)	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:953)	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:946)	at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:133)	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:148)	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)	at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:203)	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:176)	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)	at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:127)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)	at org.apache.hadoop.ipc.Client.call(Client.java:1471)	at org.apache.hadoop.ipc.Client.call(Client.java:1408)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)	at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:549)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)	at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3082)	... 28 more
[13:55:22] INFO:    Now wait 5 seconds to begin next task ...
[13:55:27] INFO:    Connection channel disconnect
[13:55:27] INFO:    SSH connection shutdown

=============== [2017/03/28 13:58:48, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[13:58:48] INFO:    SSHExec initializing ...
[13:58:48] INFO:    Session initialized and associated with user credential 123456
[13:58:48] INFO:    SSHExec initialized successfully
[13:58:48] INFO:    SSHExec trying to connect root@172.16.110.200
[13:58:49] INFO:    SSH connection established
[13:58:49] INFO:    Command is  sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[13:58:49] INFO:    Connection channel established succesfully
[13:58:49] INFO:    Start to run command
[13:58:49] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[13:58:59] INFO:    Connection channel closed
[13:58:59] INFO:    Check if exec success or not ... 
[13:58:59] INFO:    Execution failed while executing command:  sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[13:58:59] INFO:    Error message: 17/03/28 13:59:37 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 13:59:37 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 13:59:37 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 13:59:37 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 13:59:37 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 13:59:37 INFO tool.CodeGenTool: Beginning code generation17/03/28 13:59:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 13:59:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 13:59:39 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-root/compile/3953c9f0f73187ab5bee0d80b06edcf2/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 13:59:43 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-root/compile/3953c9f0f73187ab5bee0d80b06edcf2/dc_obj_main.jar17/03/28 13:59:43 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 13:59:43 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 13:59:43 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 13:59:43 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 13:59:43 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 13:59:43 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 13:59:45 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 13:59:45 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 13:59:45 WARN security.UserGroupInformation: PriviledgedActionException as:root (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)17/03/28 13:59:45 ERROR tool.ImportTool: Encountered IOException running import job: org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3084)	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3049)	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:957)	at org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:953)	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:953)	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:946)	at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:133)	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:148)	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307)	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304)	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1325)	at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:203)	at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:176)	at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:273)	at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)	at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:127)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:507)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:281)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:262)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:242)	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:169)	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6621)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6603)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6555)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4350)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4320)	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4293)	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:869)	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:608)	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2086)	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2082)	at java.security.AccessController.doPrivileged(Native Method)	at javax.security.auth.Subject.doAs(Subject.java:422)	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2080)	at org.apache.hadoop.ipc.Client.call(Client.java:1471)	at org.apache.hadoop.ipc.Client.call(Client.java:1408)	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)	at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:549)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)	at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3082)	... 28 more
[13:58:59] INFO:    Now wait 5 seconds to begin next task ...
[13:59:04] INFO:    Connection channel disconnect
[13:59:04] INFO:    SSH connection shutdown

=============== [2017/03/28 14:08:55, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:08:55] INFO:    SSHExec initializing ...
[14:08:55] INFO:    Session initialized and associated with user credential 123456
[14:08:55] INFO:    SSHExec initialized successfully
[14:08:55] INFO:    SSHExec trying to connect root@172.16.110.200
[14:08:57] INFO:    SSH connection established
[14:08:57] INFO:    Command is sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[14:08:57] INFO:    Connection channel established succesfully
[14:08:57] INFO:    Start to run command
[14:08:57] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:09:53] INFO:    Connection channel closed
[14:09:53] INFO:    Check if exec success or not ... 
[14:09:53] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[14:09:53] INFO:    Error message: 17/03/28 14:08:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 14:08:56 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 14:08:56 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 14:08:56 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 14:08:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 14:08:56 INFO tool.CodeGenTool: Beginning code generation17/03/28 14:08:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:08:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:08:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/971a3242faf117b35e8ac1a84b741aff/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 14:09:01 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/971a3242faf117b35e8ac1a84b741aff/dc_obj_main.jar17/03/28 14:09:01 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 14:09:01 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 14:09:01 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 14:09:01 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 14:09:01 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 14:09:02 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 14:09:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 14:09:04 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 14:09:10 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 14:09:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `dc_obj_main`17/03/28 14:09:10 WARN db.TextSplitter: Generating splits for a textual index column.17/03/28 14:09:10 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/28 14:09:10 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/28 14:09:11 INFO mapreduce.JobSubmitter: number of splits:617/03/28 14:09:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_001517/03/28 14:09:13 INFO impl.YarnClientImpl: Submitted application application_1490408992134_001517/03/28 14:09:13 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0015/17/03/28 14:09:13 INFO mapreduce.Job: Running job: job_1490408992134_001517/03/28 14:09:23 INFO mapreduce.Job: Job job_1490408992134_0015 running in uber mode : false17/03/28 14:09:23 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 14:09:30 INFO mapreduce.Job:  map 33% reduce 0%17/03/28 14:09:33 INFO mapreduce.Job:  map 67% reduce 0%17/03/28 14:09:34 INFO mapreduce.Job:  map 83% reduce 0%17/03/28 14:09:37 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 14:09:37 INFO mapreduce.Job: Job job_1490408992134_0015 completed successfully17/03/28 14:09:37 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862680		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=1547586		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=19273		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=19273		Total vcore-seconds taken by all map tasks=19273		Total megabyte-seconds taken by all map tasks=19735552	Map-Reduce Framework		Map input records=10140		Map output records=10140		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=311		CPU time spent (ms)=5280		Physical memory (bytes) snapshot=1142988800		Virtual memory (bytes) snapshot=16691560448		Total committed heap usage (bytes)=1114636288	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=154758617/03/28 14:09:37 INFO mapreduce.ImportJobBase: Transferred 1.4759 MB in 33.2994 seconds (45.3856 KB/sec)17/03/28 14:09:37 INFO mapreduce.ImportJobBase: Retrieved 10140 records.17/03/28 14:09:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:09:37 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/28 14:09:37 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/28 14:09:37 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.928 secondsLoading data to table default.obj_lpTable default.obj_lp stats: [numFiles=24, totalSize=6190344]OKTime taken: 1.367 seconds
[14:09:53] INFO:    Now wait 5 seconds to begin next task ...
[14:09:58] INFO:    Connection channel disconnect
[14:09:58] INFO:    SSH connection shutdown

=============== [2017/03/28 14:11:49, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:11:49] INFO:    SSHExec initializing ...
[14:11:49] INFO:    Session initialized and associated with user credential 123456
[14:11:49] INFO:    SSHExec initialized successfully
[14:11:49] INFO:    SSHExec trying to connect root@172.16.110.200
[14:11:50] INFO:    SSH connection established
[14:11:50] INFO:    Command is sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[14:11:50] INFO:    Connection channel established succesfully
[14:11:50] INFO:    Start to run command
[14:11:50] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:12:40] INFO:    Connection channel closed
[14:12:40] INFO:    Check if exec success or not ... 
[14:12:40] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[14:12:40] INFO:    Error message: 17/03/28 14:11:46 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 14:11:46 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 14:11:46 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 14:11:46 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 14:11:47 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 14:11:47 INFO tool.CodeGenTool: Beginning code generation17/03/28 14:11:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:11:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:11:48 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/1b96bd8f90b5ed541d744cc543f3b5c2/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 14:11:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/1b96bd8f90b5ed541d744cc543f3b5c2/dc_obj_main.jar17/03/28 14:11:52 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 14:11:52 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 14:11:52 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 14:11:52 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 14:11:52 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 14:11:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 14:11:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 14:11:54 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 14:12:02 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 14:12:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `dc_obj_main`17/03/28 14:12:02 WARN db.TextSplitter: Generating splits for a textual index column.17/03/28 14:12:02 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/28 14:12:02 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/28 14:12:02 INFO mapreduce.JobSubmitter: number of splits:617/03/28 14:12:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_001717/03/28 14:12:03 INFO impl.YarnClientImpl: Submitted application application_1490408992134_001717/03/28 14:12:04 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0017/17/03/28 14:12:04 INFO mapreduce.Job: Running job: job_1490408992134_001717/03/28 14:12:14 INFO mapreduce.Job: Job job_1490408992134_0017 running in uber mode : false17/03/28 14:12:14 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 14:12:20 INFO mapreduce.Job:  map 33% reduce 0%17/03/28 14:12:22 INFO mapreduce.Job:  map 50% reduce 0%17/03/28 14:12:23 INFO mapreduce.Job:  map 67% reduce 0%17/03/28 14:12:24 INFO mapreduce.Job:  map 83% reduce 0%17/03/28 14:12:28 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 14:12:29 INFO mapreduce.Job: Job job_1490408992134_0017 completed successfully17/03/28 14:12:29 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862680		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=1547586		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=22060		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=22060		Total vcore-seconds taken by all map tasks=22060		Total megabyte-seconds taken by all map tasks=22589440	Map-Reduce Framework		Map input records=10140		Map output records=10140		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=379		CPU time spent (ms)=6310		Physical memory (bytes) snapshot=1114079232		Virtual memory (bytes) snapshot=16688857088		Total committed heap usage (bytes)=1077411840	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=154758617/03/28 14:12:29 INFO mapreduce.ImportJobBase: Transferred 1.4759 MB in 35.0922 seconds (43.0669 KB/sec)17/03/28 14:12:29 INFO mapreduce.ImportJobBase: Retrieved 10140 records.17/03/28 14:12:29 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 14:12:29 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/28 14:12:29 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/28 14:12:29 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.717 secondsLoading data to table default.obj_lpTable default.obj_lp stats: [numFiles=30, totalSize=7737930]OKTime taken: 1.891 seconds
[14:12:40] INFO:    Now wait 5 seconds to begin next task ...
[14:12:45] INFO:    Connection channel disconnect
[14:12:45] INFO:    SSH connection shutdown

=============== [2017/03/28 17:03:26, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:03:26] INFO:    SSHExec initializing ...
[17:03:26] INFO:    Session initialized and associated with user credential 123456
[17:03:26] INFO:    SSHExec initialized successfully
[17:03:26] INFO:    SSHExec trying to connect root@172.16.110.200
[17:03:26] INFO:    SSH connection established
[17:03:26] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:03:26] INFO:    Connection channel established succesfully
[17:03:26] INFO:    Start to run command
[17:03:40] INFO:    Connection channel closed
[17:03:40] INFO:    Check if exec success or not ... 
[17:03:40] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:03:40] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 17:02:37 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.304 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[17:03:40] INFO:    Now wait 5 seconds to begin next task ...
[17:03:45] INFO:    Connection channel disconnect
[17:03:45] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.130:3306/hldc_h5 --username root --password root
[17:03:46] INFO:    Connection channel established succesfully
[17:03:46] INFO:    Start to run command
[17:03:46] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:03:50] INFO:    Connection channel closed
[17:03:50] INFO:    Check if exec success or not ... 
[17:03:50] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.130:3306/hldc_h5 --username root --password root
[17:03:50] INFO:    Error message: 17/03/28 17:02:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 17:02:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 17:02:54 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 17:02:54 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 17:02:54 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 17:02:55 INFO tool.CodeGenTool: Beginning code generation17/03/28 17:02:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:02:55 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hldc_h5.sys_user' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hldc_h5.sys_user' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/28 17:02:55 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[17:03:50] INFO:    Now wait 5 seconds to begin next task ...
[17:03:55] INFO:    Connection channel disconnect
[17:03:55] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS xxxxxxxx; ALTER TABLE hl_bak.xxxxxxxx RENAME TO xxxxxxxx'
[17:03:55] INFO:    Connection channel established succesfully
[17:03:55] INFO:    Start to run command
[17:04:11] INFO:    Connection channel closed
[17:04:11] INFO:    Check if exec success or not ... 
[17:04:11] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS xxxxxxxx; ALTER TABLE hl_bak.xxxxxxxx RENAME TO xxxxxxxx'
[17:04:11] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 17:03:07 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.226 secondsFAILED: SemanticException [Error 10001]: Table not found hl_bak.xxxxxxxx
[17:04:11] INFO:    Now wait 5 seconds to begin next task ...
[17:04:16] INFO:    Connection channel disconnect
[17:04:16] INFO:    SSH connection shutdown

=============== [2017/03/28 17:16:05, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:16:05] INFO:    SSHExec initializing ...
[17:16:05] INFO:    Session initialized and associated with user credential 123456
[17:16:05] INFO:    SSHExec initialized successfully
[17:16:05] INFO:    SSHExec trying to connect root@172.16.110.200
[17:16:05] INFO:    SSH connection established
[17:16:05] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:16:05] INFO:    Connection channel established succesfully
[17:16:05] INFO:    Start to run command
[17:16:20] INFO:    Connection channel closed
[17:16:20] INFO:    Check if exec success or not ... 
[17:16:20] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:16:20] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 17:15:20 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.221 secondsFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. Unable to change partition or table. Database default does not exist Check metastore logs for detailed stack.There is no database named hl_bak
[17:16:20] INFO:    Now wait 5 seconds to begin next task ...
[17:16:25] INFO:    Connection channel disconnect
[17:16:25] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:16:25] INFO:    Connection channel established succesfully
[17:16:25] INFO:    Start to run command
[17:16:25] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[17:16:25] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:17:03] INFO:    Connection channel closed
[17:17:03] INFO:    Check if exec success or not ... 
[17:17:03] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:17:03] INFO:    Error message: 17/03/28 17:15:39 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 17:15:39 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 17:15:39 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 17:15:39 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 17:15:39 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 17:15:39 INFO tool.CodeGenTool: Beginning code generation17/03/28 17:15:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:15:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:15:40 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/5ffcfbba251e6ab061c0a259b4657d11/sys_user.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 17:15:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/5ffcfbba251e6ab061c0a259b4657d11/sys_user.jar17/03/28 17:15:45 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 17:15:45 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 17:15:45 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 17:15:45 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 17:15:45 INFO mapreduce.ImportJobBase: Beginning import of sys_user17/03/28 17:15:45 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 17:15:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 17:15:47 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 17:15:53 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 17:15:53 INFO mapreduce.JobSubmitter: number of splits:117/03/28 17:15:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_002217/03/28 17:15:54 INFO impl.YarnClientImpl: Submitted application application_1490408992134_002217/03/28 17:15:55 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0022/17/03/28 17:15:55 INFO mapreduce.Job: Running job: job_1490408992134_002217/03/28 17:16:03 INFO mapreduce.Job: Job job_1490408992134_0022 running in uber mode : false17/03/28 17:16:03 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 17:16:09 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 17:16:10 INFO mapreduce.Job: Job job_1490408992134_0022 completed successfully17/03/28 17:16:10 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=3754		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=2806		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=2806		Total vcore-seconds taken by all map tasks=2806		Total megabyte-seconds taken by all map tasks=2873344	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=58		CPU time spent (ms)=730		Physical memory (bytes) snapshot=172990464		Virtual memory (bytes) snapshot=2780094464		Total committed heap usage (bytes)=166199296	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=375417/03/28 17:16:10 INFO mapreduce.ImportJobBase: Transferred 3.666 KB in 23.2162 seconds (161.6974 bytes/sec)17/03/28 17:16:10 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/28 17:16:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:16:10 WARN hive.TableDefWriter: Column login_date had to be cast to a less precise type in Hive17/03/28 17:16:10 WARN hive.TableDefWriter: Column create_date had to be cast to a less precise type in Hive17/03/28 17:16:10 WARN hive.TableDefWriter: Column update_date had to be cast to a less precise type in Hive17/03/28 17:16:10 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.768 secondsLoading data to table default.xxxxxxxxTable default.xxxxxxxx stats: [numFiles=2, totalSize=7508]OKTime taken: 1.785 seconds
[17:17:03] INFO:    Now wait 5 seconds to begin next task ...
[17:17:08] INFO:    Connection channel disconnect
[17:17:08] INFO:    SSH connection shutdown

=============== [2017/03/28 17:19:44, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:19:44] INFO:    SSHExec initializing ...
[17:19:44] INFO:    Session initialized and associated with user credential 123456
[17:19:44] INFO:    SSHExec initialized successfully
[17:19:44] INFO:    SSHExec trying to connect root@172.16.110.200
[17:19:44] INFO:    SSH connection established
[17:19:44] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:19:44] INFO:    Connection channel established succesfully
[17:19:44] INFO:    Start to run command
[17:19:59] INFO:    Connection channel closed
[17:19:59] INFO:    Check if exec success or not ... 
[17:19:59] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:19:59] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 17:19:16 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.637 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[17:19:59] INFO:    Now wait 5 seconds to begin next task ...
[17:20:04] INFO:    Connection channel disconnect
[17:20:04] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:20:04] INFO:    Connection channel established succesfully
[17:20:04] INFO:    Start to run command
[17:20:04] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[17:20:04] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:20:47] INFO:    Connection channel closed
[17:20:47] INFO:    Check if exec success or not ... 
[17:20:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:20:47] INFO:    Error message: 17/03/28 17:19:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 17:19:35 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 17:19:35 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 17:19:35 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 17:19:36 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 17:19:36 INFO tool.CodeGenTool: Beginning code generation17/03/28 17:19:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:19:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:19:37 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/9c93a6f9d7077b17fa46fe6e0fab7d05/sys_user.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 17:19:40 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/9c93a6f9d7077b17fa46fe6e0fab7d05/sys_user.jar17/03/28 17:19:40 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 17:19:40 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 17:19:40 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 17:19:40 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 17:19:40 INFO mapreduce.ImportJobBase: Beginning import of sys_user17/03/28 17:19:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 17:19:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 17:19:42 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 17:19:50 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 17:19:50 INFO mapreduce.JobSubmitter: number of splits:117/03/28 17:19:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_002317/03/28 17:19:51 INFO impl.YarnClientImpl: Submitted application application_1490408992134_002317/03/28 17:19:51 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0023/17/03/28 17:19:51 INFO mapreduce.Job: Running job: job_1490408992134_002317/03/28 17:20:02 INFO mapreduce.Job: Job job_1490408992134_0023 running in uber mode : false17/03/28 17:20:02 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 17:20:08 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 17:20:09 INFO mapreduce.Job: Job job_1490408992134_0023 completed successfully17/03/28 17:20:09 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=3754		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=2756		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=2756		Total vcore-seconds taken by all map tasks=2756		Total megabyte-seconds taken by all map tasks=2822144	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=52		CPU time spent (ms)=750		Physical memory (bytes) snapshot=172916736		Virtual memory (bytes) snapshot=2779795456		Total committed heap usage (bytes)=166723584	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=375417/03/28 17:20:09 INFO mapreduce.ImportJobBase: Transferred 3.666 KB in 27.2463 seconds (137.7799 bytes/sec)17/03/28 17:20:09 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/28 17:20:09 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:20:09 WARN hive.TableDefWriter: Column login_date had to be cast to a less precise type in Hive17/03/28 17:20:09 WARN hive.TableDefWriter: Column create_date had to be cast to a less precise type in Hive17/03/28 17:20:09 WARN hive.TableDefWriter: Column update_date had to be cast to a less precise type in Hive17/03/28 17:20:09 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.268 secondsLoading data to table default.xxxxxxxxTable default.xxxxxxxx stats: [numFiles=3, numRows=0, totalSize=11262, rawDataSize=0]OKTime taken: 0.875 seconds
[17:20:47] INFO:    Now wait 5 seconds to begin next task ...
[17:20:52] INFO:    Connection channel disconnect
[17:20:52] INFO:    SSH connection shutdown

=============== [2017/03/28 17:30:29, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:30:29] INFO:    SSHExec initializing ...
[17:30:29] INFO:    Session initialized and associated with user credential 123456
[17:30:29] INFO:    SSHExec initialized successfully
[17:30:29] INFO:    SSHExec trying to connect root@172.16.110.200
[17:30:29] INFO:    SSH connection established
[17:30:29] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:30:29] INFO:    Connection channel established succesfully
[17:30:29] INFO:    Start to run command
[17:30:45] INFO:    Connection channel closed
[17:30:45] INFO:    Check if exec success or not ... 
[17:30:45] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[17:30:45] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 17:30:19 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.336 secondsFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. Unable to change partition or table. Database default does not exist Check metastore logs for detailed stack.There is no database named hl_bak
[17:30:45] INFO:    Now wait 5 seconds to begin next task ...
[17:30:50] INFO:    Connection channel disconnect
[17:30:50] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:30:50] INFO:    Connection channel established succesfully
[17:30:50] INFO:    Start to run command
[17:30:50] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[17:30:50] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:31:33] INFO:    Connection channel closed
[17:31:33] INFO:    Check if exec success or not ... 
[17:31:33] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table sys_user --hive-table xxxxxxxx --hive-import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:31:33] INFO:    Error message: 17/03/28 17:30:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 17:30:40 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 17:30:40 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 17:30:40 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 17:30:40 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 17:30:40 INFO tool.CodeGenTool: Beginning code generation17/03/28 17:30:41 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:30:41 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:30:41 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/994106b22b3780b9817d896b6f8b739f/sys_user.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 17:30:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/994106b22b3780b9817d896b6f8b739f/sys_user.jar17/03/28 17:30:45 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 17:30:45 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 17:30:45 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 17:30:45 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 17:30:45 INFO mapreduce.ImportJobBase: Beginning import of sys_user17/03/28 17:30:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 17:30:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 17:30:47 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 17:30:54 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 17:30:54 INFO mapreduce.JobSubmitter: number of splits:117/03/28 17:30:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_002417/03/28 17:30:55 INFO impl.YarnClientImpl: Submitted application application_1490408992134_002417/03/28 17:30:55 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0024/17/03/28 17:30:55 INFO mapreduce.Job: Running job: job_1490408992134_002417/03/28 17:31:05 INFO mapreduce.Job: Job job_1490408992134_0024 running in uber mode : false17/03/28 17:31:05 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 17:31:12 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 17:31:12 INFO mapreduce.Job: Job job_1490408992134_0024 completed successfully17/03/28 17:31:12 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=3754		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=2974		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=2974		Total vcore-seconds taken by all map tasks=2974		Total megabyte-seconds taken by all map tasks=3045376	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=46		CPU time spent (ms)=710		Physical memory (bytes) snapshot=186433536		Virtual memory (bytes) snapshot=2778648576		Total committed heap usage (bytes)=188743680	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=375417/03/28 17:31:12 INFO mapreduce.ImportJobBase: Transferred 3.666 KB in 25.3246 seconds (148.235 bytes/sec)17/03/28 17:31:12 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/28 17:31:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `sys_user` AS t LIMIT 117/03/28 17:31:13 WARN hive.TableDefWriter: Column login_date had to be cast to a less precise type in Hive17/03/28 17:31:13 WARN hive.TableDefWriter: Column create_date had to be cast to a less precise type in Hive17/03/28 17:31:13 WARN hive.TableDefWriter: Column update_date had to be cast to a less precise type in Hive17/03/28 17:31:13 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.855 secondsLoading data to table default.xxxxxxxxTable default.xxxxxxxx stats: [numFiles=4, numRows=0, totalSize=15016, rawDataSize=0]OKTime taken: 1.392 seconds
[17:31:33] INFO:    Now wait 5 seconds to begin next task ...
[17:31:38] INFO:    Connection channel disconnect
[17:31:42] INFO:    SSH connection shutdown

=============== [2017/03/28 19:18:45, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[19:18:45] INFO:    SSHExec initializing ...
[19:18:45] INFO:    Session initialized and associated with user credential 123456
[19:18:45] INFO:    SSHExec initialized successfully
[19:18:45] INFO:    SSHExec trying to connect root@172.16.110.200
[19:18:54] INFO:    SSH connection established
[19:18:54] INFO:    Command is sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[19:18:54] INFO:    Connection channel established succesfully
[19:18:54] INFO:    Start to run command
[19:18:54] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[19:19:45] INFO:    Connection channel closed
[19:19:45] INFO:    Check if exec success or not ... 
[19:19:45] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[19:19:45] INFO:    Error message: 17/03/28 19:18:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 19:18:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 19:18:06 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 19:18:06 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 19:18:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 19:18:06 INFO tool.CodeGenTool: Beginning code generation17/03/28 19:18:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:18:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:18:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/2b72a00c084caf44bc63449560ac19d9/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 19:18:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/2b72a00c084caf44bc63449560ac19d9/dc_obj_main.jar17/03/28 19:18:11 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 19:18:11 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 19:18:11 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 19:18:11 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 19:18:11 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 19:18:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 19:18:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 19:18:13 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 19:18:20 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 19:18:20 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `dc_obj_main`17/03/28 19:18:20 WARN db.TextSplitter: Generating splits for a textual index column.17/03/28 19:18:20 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/28 19:18:20 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/28 19:18:21 INFO mapreduce.JobSubmitter: number of splits:617/03/28 19:18:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_002717/03/28 19:18:22 INFO impl.YarnClientImpl: Submitted application application_1490408992134_002717/03/28 19:18:22 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0027/17/03/28 19:18:22 INFO mapreduce.Job: Running job: job_1490408992134_002717/03/28 19:18:31 INFO mapreduce.Job: Job job_1490408992134_0027 running in uber mode : false17/03/28 19:18:31 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 19:18:36 INFO mapreduce.Job:  map 17% reduce 0%17/03/28 19:18:37 INFO mapreduce.Job:  map 33% reduce 0%17/03/28 19:18:39 INFO mapreduce.Job:  map 50% reduce 0%17/03/28 19:18:40 INFO mapreduce.Job:  map 83% reduce 0%17/03/28 19:18:44 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 19:18:45 INFO mapreduce.Job: Job job_1490408992134_0027 completed successfully17/03/28 19:18:46 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862680		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=1548916		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20354		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20354		Total vcore-seconds taken by all map tasks=20354		Total megabyte-seconds taken by all map tasks=20842496	Map-Reduce Framework		Map input records=10145		Map output records=10145		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=361		CPU time spent (ms)=5830		Physical memory (bytes) snapshot=1114316800		Virtual memory (bytes) snapshot=16685027328		Total committed heap usage (bytes)=1049624576	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=154891617/03/28 19:18:46 INFO mapreduce.ImportJobBase: Transferred 1.4772 MB in 32.3356 seconds (46.7786 KB/sec)17/03/28 19:18:46 INFO mapreduce.ImportJobBase: Retrieved 10145 records.17/03/28 19:18:46 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:18:46 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/28 19:18:46 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/28 19:18:46 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.753 secondsLoading data to table default.obj_lpTable default.obj_lp stats: [numFiles=36, numRows=0, totalSize=9286846, rawDataSize=0]OKTime taken: 1.627 seconds
[19:19:45] INFO:    Now wait 5 seconds to begin next task ...
[19:19:50] INFO:    Connection channel disconnect
[19:19:50] INFO:    Command is sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp1
[19:19:50] INFO:    Connection channel established succesfully
[19:19:50] INFO:    Start to run command
[19:19:50] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[19:20:39] INFO:    Connection channel closed
[19:20:39] INFO:    Check if exec success or not ... 
[19:20:39] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp1
[19:20:39] INFO:    Error message: 17/03/28 19:19:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 19:19:03 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 19:19:03 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 19:19:03 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 19:19:03 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 19:19:03 INFO tool.CodeGenTool: Beginning code generation17/03/28 19:19:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:19:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:19:04 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/91887681a26d92ae8ba9d323adab3e42/dc_obj_main.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 19:19:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/91887681a26d92ae8ba9d323adab3e42/dc_obj_main.jar17/03/28 19:19:08 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 19:19:08 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 19:19:08 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 19:19:08 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 19:19:08 INFO mapreduce.ImportJobBase: Beginning import of dc_obj_main17/03/28 19:19:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 19:19:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 19:19:10 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 19:19:16 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 19:19:16 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `dc_obj_main`17/03/28 19:19:16 WARN db.TextSplitter: Generating splits for a textual index column.17/03/28 19:19:16 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/28 19:19:16 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/28 19:19:17 INFO mapreduce.JobSubmitter: number of splits:617/03/28 19:19:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_002817/03/28 19:19:18 INFO impl.YarnClientImpl: Submitted application application_1490408992134_002817/03/28 19:19:18 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0028/17/03/28 19:19:18 INFO mapreduce.Job: Running job: job_1490408992134_002817/03/28 19:19:28 INFO mapreduce.Job: Job job_1490408992134_0028 running in uber mode : false17/03/28 19:19:28 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 19:19:34 INFO mapreduce.Job:  map 33% reduce 0%17/03/28 19:19:36 INFO mapreduce.Job:  map 50% reduce 0%17/03/28 19:19:37 INFO mapreduce.Job:  map 67% reduce 0%17/03/28 19:19:38 INFO mapreduce.Job:  map 83% reduce 0%17/03/28 19:19:42 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 19:19:42 INFO mapreduce.Job: Job job_1490408992134_0028 completed successfully17/03/28 19:19:42 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862686		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=1548916		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20142		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20142		Total vcore-seconds taken by all map tasks=20142		Total megabyte-seconds taken by all map tasks=20625408	Map-Reduce Framework		Map input records=10145		Map output records=10145		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=345		CPU time spent (ms)=6060		Physical memory (bytes) snapshot=1084366848		Virtual memory (bytes) snapshot=16690819072		Total committed heap usage (bytes)=1050673152	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=154891617/03/28 19:19:42 INFO mapreduce.ImportJobBase: Transferred 1.4772 MB in 32.0191 seconds (47.2409 KB/sec)17/03/28 19:19:42 INFO mapreduce.ImportJobBase: Retrieved 10145 records.17/03/28 19:19:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dc_obj_main` AS t LIMIT 117/03/28 19:19:42 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/28 19:19:42 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/28 19:19:42 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.345 secondsLoading data to table default.obj_lp1Table default.obj_lp1 stats: [numFiles=6, totalSize=1548916]OKTime taken: 1.011 seconds
[19:20:39] INFO:    Now wait 5 seconds to begin next task ...
[19:20:44] INFO:    Connection channel disconnect
[19:20:44] INFO:    SSH connection shutdown

=============== [2017/03/28 20:50:29, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:50:29] INFO:    SSHExec initializing ...
[20:50:29] INFO:    Session initialized and associated with user credential 123456
[20:50:29] INFO:    SSHExec initialized successfully
[20:50:29] INFO:    SSHExec trying to connect root@172.16.110.200
[20:50:30] INFO:    SSH connection established
[20:50:30] INFO:    Command is sudo -u hdfs sqoop import --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --table dc_obj_main --username hldc_h5 --password hldc_h5 --hive-import --hive-table obj_lp
[20:50:30] INFO:    Connection channel established succesfully
[20:50:30] INFO:    Start to run command
[20:50:30] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.


=============== [2017/03/28 20:51:07, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:51:07] INFO:    SSHExec initializing ...
[20:51:07] INFO:    Session initialized and associated with user credential 123456
[20:51:07] INFO:    Session initialized and associated with user credential 123456
[20:51:07] INFO:    Session initialized and associated with user credential 123456
[20:51:07] INFO:    Session initialized and associated with user credential 123456
[20:51:07] INFO:    SSHExec initialized successfully
[20:51:07] INFO:    SSHExec initialized successfully
[20:51:07] INFO:    SSHExec trying to connect root@172.16.110.200
[20:51:07] INFO:    SSHExec initialized successfully
[20:51:07] INFO:    SSHExec trying to connect root@172.16.110.200
[20:51:07] INFO:    SSHExec initialized successfully
[20:51:07] INFO:    SSHExec trying to connect root@172.16.110.200
[20:51:07] INFO:    SSHExec trying to connect root@172.16.110.200
[20:53:07] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[20:53:07] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[20:53:07] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[20:53:07] ERROR:   session is down
[20:53:07] ERROR:   session is down
[20:53:07] ERROR:   session is down
[20:53:07] ERROR:   session is down
[20:53:07] ERROR:   session is down
[20:53:07] INFO:    SSH connection shutdown
[20:53:07] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[20:53:07] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[20:53:07] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[20:53:07] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException

=============== [2017/03/28 20:54:58, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:54:58] INFO:    SSHExec initializing ...
[20:54:58] INFO:    Session initialized and associated with user credential 123456
[20:54:58] INFO:    SSHExec initialized successfully
[20:54:58] INFO:    SSHExec trying to connect root@172.16.110.200
[20:54:58] INFO:    SSH connection established
[20:54:58] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:54:58] INFO:    Connection channel established succesfully
[20:54:58] INFO:    Start to run command
[20:55:13] INFO:    Connection channel closed
[20:55:13] INFO:    Check if exec success or not ... 
[20:55:13] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:55:13] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 20:54:14 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.517 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[20:55:13] INFO:    Now wait 5 seconds to begin next task ...
[20:55:18] INFO:    Connection channel disconnect
[20:55:18] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table SYS_USER --hive-table SYS_USER --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:55:18] INFO:    Connection channel established succesfully
[20:55:18] INFO:    Start to run command
[20:55:18] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[20:55:18] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[20:56:03] INFO:    Connection channel closed
[20:56:03] INFO:    Check if exec success or not ... 
[20:56:03] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table SYS_USER --hive-table SYS_USER --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:56:03] INFO:    Error message: 17/03/28 20:54:31 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 20:54:31 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 20:54:31 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 20:54:31 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 20:54:32 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 20:54:32 INFO tool.CodeGenTool: Beginning code generation17/03/28 20:54:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 20:54:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 20:54:33 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/2f8dcad839e66290a723dcf8da0c92da/SYS_USER.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 20:54:37 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/2f8dcad839e66290a723dcf8da0c92da/SYS_USER.jar17/03/28 20:54:37 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 20:54:37 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 20:54:37 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 20:54:37 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 20:54:37 INFO mapreduce.ImportJobBase: Beginning import of SYS_USER17/03/28 20:54:37 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 20:54:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 20:54:39 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 20:54:47 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 20:54:48 INFO mapreduce.JobSubmitter: number of splits:117/03/28 20:54:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003017/03/28 20:54:49 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003017/03/28 20:54:49 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0030/17/03/28 20:54:49 INFO mapreduce.Job: Running job: job_1490408992134_003017/03/28 20:54:59 INFO mapreduce.Job: Job job_1490408992134_0030 running in uber mode : false17/03/28 20:54:59 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 20:55:05 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 20:55:06 INFO mapreduce.Job: Job job_1490408992134_0030 completed successfully17/03/28 20:55:06 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=3754		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=3032		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=3032		Total vcore-seconds taken by all map tasks=3032		Total megabyte-seconds taken by all map tasks=3104768	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=55		CPU time spent (ms)=780		Physical memory (bytes) snapshot=171593728		Virtual memory (bytes) snapshot=2778869760		Total committed heap usage (bytes)=166723584	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=375417/03/28 20:55:06 INFO mapreduce.ImportJobBase: Transferred 3.666 KB in 27.0786 seconds (138.6336 bytes/sec)17/03/28 20:55:06 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/28 20:55:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 20:55:06 WARN hive.TableDefWriter: Column login_date had to be cast to a less precise type in Hive17/03/28 20:55:06 WARN hive.TableDefWriter: Column create_date had to be cast to a less precise type in Hive17/03/28 20:55:06 WARN hive.TableDefWriter: Column update_date had to be cast to a less precise type in Hive17/03/28 20:55:06 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.368 secondsLoading data to table default.sys_userTable default.sys_user stats: [numFiles=1, totalSize=3754]OKTime taken: 1.303 seconds
[20:56:03] INFO:    Now wait 5 seconds to begin next task ...
[20:56:08] INFO:    Connection channel disconnect
[20:56:08] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:56:08] INFO:    Connection channel established succesfully
[20:56:08] INFO:    Start to run command
[20:56:24] INFO:    Connection channel closed
[20:56:24] INFO:    Check if exec success or not ... 
[20:56:24] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:56:24] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 20:55:24 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.263 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[20:56:24] INFO:    Now wait 5 seconds to begin next task ...
[20:56:29] INFO:    Connection channel disconnect
[20:56:29] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA --hive-table DC_JOB_TRANSDATA --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:56:29] INFO:    Connection channel established succesfully
[20:56:29] INFO:    Start to run command
[20:56:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[20:57:13] INFO:    Connection channel closed
[20:57:13] INFO:    Check if exec success or not ... 
[20:57:13] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA --hive-table DC_JOB_TRANSDATA --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:57:13] INFO:    Error message: 17/03/28 20:55:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 20:55:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 20:55:44 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 20:55:44 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 20:55:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 20:55:44 INFO tool.CodeGenTool: Beginning code generation17/03/28 20:55:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA` AS t LIMIT 117/03/28 20:55:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA` AS t LIMIT 117/03/28 20:55:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/86dd87e0277f0a06bac727c857a306ff/DC_JOB_TRANSDATA.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 20:55:49 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/86dd87e0277f0a06bac727c857a306ff/DC_JOB_TRANSDATA.jar17/03/28 20:55:49 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 20:55:49 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 20:55:49 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 20:55:49 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 20:55:49 INFO mapreduce.ImportJobBase: Beginning import of DC_JOB_TRANSDATA17/03/28 20:55:50 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 20:55:51 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 20:55:51 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 20:55:58 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 20:55:58 INFO mapreduce.JobSubmitter: number of splits:117/03/28 20:55:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003117/03/28 20:55:59 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003117/03/28 20:56:00 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0031/17/03/28 20:56:00 INFO mapreduce.Job: Running job: job_1490408992134_003117/03/28 20:56:10 INFO mapreduce.Job: Job job_1490408992134_0031 running in uber mode : false17/03/28 20:56:10 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 20:56:16 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 20:56:16 INFO mapreduce.Job: Job job_1490408992134_0031 completed successfully17/03/28 20:56:17 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143764		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=20655		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=2921		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=2921		Total vcore-seconds taken by all map tasks=2921		Total megabyte-seconds taken by all map tasks=2991104	Map-Reduce Framework		Map input records=54		Map output records=54		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=51		CPU time spent (ms)=810		Physical memory (bytes) snapshot=174141440		Virtual memory (bytes) snapshot=2778865664		Total committed heap usage (bytes)=166723584	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=2065517/03/28 20:56:17 INFO mapreduce.ImportJobBase: Transferred 20.1709 KB in 25.4994 seconds (810.0189 bytes/sec)17/03/28 20:56:17 INFO mapreduce.ImportJobBase: Retrieved 54 records.17/03/28 20:56:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA` AS t LIMIT 117/03/28 20:56:17 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/28 20:56:17 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/28 20:56:17 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.902 secondsLoading data to table default.dc_job_transdataTable default.dc_job_transdata stats: [numFiles=1, totalSize=20655]OKTime taken: 1.402 seconds
[20:57:13] INFO:    Now wait 5 seconds to begin next task ...
[20:57:18] INFO:    Connection channel disconnect
[20:57:18] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:57:19] INFO:    Connection channel established succesfully
[20:57:19] INFO:    Start to run command
[20:57:33] INFO:    Connection channel closed
[20:57:33] INFO:    Check if exec success or not ... 
[20:57:33] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:57:33] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 20:56:34 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.714 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[20:57:33] INFO:    Now wait 5 seconds to begin next task ...
[20:57:38] INFO:    Connection channel disconnect
[20:57:38] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA_LINK_DB --hive-table DC_JOB_TRANSDATA_LINK_DB --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:57:38] INFO:    Connection channel established succesfully
[20:57:38] INFO:    Start to run command
[20:57:38] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[20:58:25] INFO:    Connection channel closed
[20:58:25] INFO:    Check if exec success or not ... 
[20:58:25] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA_LINK_DB --hive-table DC_JOB_TRANSDATA_LINK_DB --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:58:25] INFO:    Error message: 17/03/28 20:56:53 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 20:56:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 20:56:54 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 20:56:54 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 20:56:54 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 20:56:54 INFO tool.CodeGenTool: Beginning code generation17/03/28 20:56:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_DB` AS t LIMIT 117/03/28 20:56:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_DB` AS t LIMIT 117/03/28 20:56:55 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/79059b945c0ddd74a33ab9bcb2ea3a01/DC_JOB_TRANSDATA_LINK_DB.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 20:57:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/79059b945c0ddd74a33ab9bcb2ea3a01/DC_JOB_TRANSDATA_LINK_DB.jar17/03/28 20:57:00 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 20:57:00 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 20:57:00 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 20:57:00 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 20:57:00 INFO mapreduce.ImportJobBase: Beginning import of DC_JOB_TRANSDATA_LINK_DB17/03/28 20:57:00 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 20:57:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 20:57:02 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 20:57:10 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 20:57:10 INFO mapreduce.JobSubmitter: number of splits:117/03/28 20:57:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003217/03/28 20:57:12 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003217/03/28 20:57:12 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0032/17/03/28 20:57:12 INFO mapreduce.Job: Running job: job_1490408992134_003217/03/28 20:57:22 INFO mapreduce.Job: Job job_1490408992134_0032 running in uber mode : false17/03/28 20:57:22 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 20:57:28 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 20:57:29 INFO mapreduce.Job: Job job_1490408992134_0032 completed successfully17/03/28 20:57:30 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=8834		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=3052		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=3052		Total vcore-seconds taken by all map tasks=3052		Total megabyte-seconds taken by all map tasks=3125248	Map-Reduce Framework		Map input records=53		Map output records=53		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=55		CPU time spent (ms)=780		Physical memory (bytes) snapshot=173486080		Virtual memory (bytes) snapshot=2777956352		Total committed heap usage (bytes)=167247872	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=883417/03/28 20:57:30 INFO mapreduce.ImportJobBase: Transferred 8.627 KB in 27.7119 seconds (318.7803 bytes/sec)17/03/28 20:57:30 INFO mapreduce.ImportJobBase: Retrieved 53 records.17/03/28 20:57:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_DB` AS t LIMIT 117/03/28 20:57:30 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.661 secondsLoading data to table default.dc_job_transdata_link_dbTable default.dc_job_transdata_link_db stats: [numFiles=1, totalSize=8834]OKTime taken: 1.029 seconds
[20:58:25] INFO:    Now wait 5 seconds to begin next task ...
[20:58:30] INFO:    Connection channel disconnect
[20:58:30] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:58:30] INFO:    Connection channel established succesfully
[20:58:30] INFO:    Start to run command
[20:58:44] INFO:    Connection channel closed
[20:58:44] INFO:    Check if exec success or not ... 
[20:58:44] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[20:58:44] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 20:57:48 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.149 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[20:58:44] INFO:    Now wait 5 seconds to begin next task ...
[20:58:49] INFO:    Connection channel disconnect
[20:58:49] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA_LINK_HDFS --hive-table DC_JOB_TRANSDATA_LINK_HDFS --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:58:49] INFO:    Connection channel established succesfully
[20:58:49] INFO:    Start to run command
[20:58:49] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[20:59:33] INFO:    Connection channel closed
[20:59:33] INFO:    Check if exec success or not ... 
[20:59:33] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA_LINK_HDFS --hive-table DC_JOB_TRANSDATA_LINK_HDFS --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[20:59:33] INFO:    Error message: 17/03/28 20:58:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 20:58:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 20:58:05 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 20:58:05 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 20:58:05 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 20:58:05 INFO tool.CodeGenTool: Beginning code generation17/03/28 20:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_HDFS` AS t LIMIT 117/03/28 20:58:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_HDFS` AS t LIMIT 117/03/28 20:58:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/e5529364a34e257056447ac779cd6929/DC_JOB_TRANSDATA_LINK_HDFS.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 20:58:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/e5529364a34e257056447ac779cd6929/DC_JOB_TRANSDATA_LINK_HDFS.jar17/03/28 20:58:10 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 20:58:10 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 20:58:10 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 20:58:10 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 20:58:10 INFO mapreduce.ImportJobBase: Beginning import of DC_JOB_TRANSDATA_LINK_HDFS17/03/28 20:58:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 20:58:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 20:58:13 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 20:58:19 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 20:58:19 INFO mapreduce.JobSubmitter: number of splits:117/03/28 20:58:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003317/03/28 20:58:21 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003317/03/28 20:58:21 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0033/17/03/28 20:58:21 INFO mapreduce.Job: Running job: job_1490408992134_003317/03/28 20:58:30 INFO mapreduce.Job: Job job_1490408992134_0033 running in uber mode : false17/03/28 20:58:30 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 20:58:37 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 20:58:38 INFO mapreduce.Job: Job job_1490408992134_0033 completed successfully17/03/28 20:58:38 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143881		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=6746		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=3060		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=3060		Total vcore-seconds taken by all map tasks=3060		Total megabyte-seconds taken by all map tasks=3133440	Map-Reduce Framework		Map input records=51		Map output records=51		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=61		CPU time spent (ms)=790		Physical memory (bytes) snapshot=175558656		Virtual memory (bytes) snapshot=2781147136		Total committed heap usage (bytes)=169345024	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=674617/03/28 20:58:38 INFO mapreduce.ImportJobBase: Transferred 6.5879 KB in 25.5311 seconds (264.2273 bytes/sec)17/03/28 20:58:38 INFO mapreduce.ImportJobBase: Retrieved 51 records.17/03/28 20:58:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_TRANSDATA_LINK_HDFS` AS t LIMIT 117/03/28 20:58:38 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.707 secondsLoading data to table default.dc_job_transdata_link_hdfsTable default.dc_job_transdata_link_hdfs stats: [numFiles=1, totalSize=6746]OKTime taken: 0.907 seconds
[20:59:33] INFO:    Now wait 5 seconds to begin next task ...
[20:59:38] INFO:    Connection channel disconnect
[20:59:38] INFO:    SSH connection shutdown
[20:59:38] INFO:    Session initialized and associated with user credential 123456
[20:59:38] INFO:    Session initialized and associated with user credential 123456
[20:59:38] INFO:    Session initialized and associated with user credential 123456
[20:59:38] INFO:    SSHExec initialized successfully
[20:59:38] INFO:    SSHExec initialized successfully
[20:59:38] INFO:    SSHExec initialized successfully
[20:59:38] INFO:    SSHExec trying to connect root@172.16.110.200
[20:59:38] INFO:    SSHExec trying to connect root@172.16.110.200
[20:59:38] INFO:    SSHExec trying to connect root@172.16.110.200
[21:01:38] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[21:01:38] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[21:01:38] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[21:01:38] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[21:01:38] INFO:    SSH connection shutdown
[21:01:38] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException

=============== [2017/03/28 21:01:41, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[21:01:41] INFO:    SSHExec initializing ...
[21:01:42] INFO:    Session initialized and associated with user credential 123456
[21:01:42] INFO:    SSHExec initialized successfully
[21:01:42] INFO:    SSHExec trying to connect root@172.16.110.200
[21:01:42] INFO:    SSH connection established
[21:01:42] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[21:01:42] INFO:    Connection channel established succesfully
[21:01:42] INFO:    Start to run command
[21:01:56] INFO:    Connection channel closed
[21:01:56] INFO:    Check if exec success or not ... 
[21:01:56] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[21:01:56] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 21:01:00 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.261 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[21:01:56] INFO:    Now wait 5 seconds to begin next task ...
[21:02:01] INFO:    Connection channel disconnect
[21:02:01] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table SYS_USER --hive-table SYS_USER --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[21:02:01] INFO:    Connection channel established succesfully
[21:02:01] INFO:    Start to run command
[21:02:02] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[21:02:47] INFO:    Connection channel closed
[21:02:47] INFO:    Check if exec success or not ... 
[21:02:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 1 --table SYS_USER --hive-table SYS_USER --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[21:02:47] INFO:    Error message: 17/03/28 21:01:17 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/28 21:01:17 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/28 21:01:17 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/28 21:01:17 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/28 21:01:18 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/28 21:01:18 INFO tool.CodeGenTool: Beginning code generation17/03/28 21:01:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 21:01:19 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 21:01:19 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/bd7875649f1cec04510ada2f0f1bcab8/SYS_USER.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/28 21:01:23 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/bd7875649f1cec04510ada2f0f1bcab8/SYS_USER.jar17/03/28 21:01:23 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/28 21:01:23 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/28 21:01:23 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/28 21:01:23 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/28 21:01:23 INFO mapreduce.ImportJobBase: Beginning import of SYS_USER17/03/28 21:01:24 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/28 21:01:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/28 21:01:25 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/28 21:01:33 INFO db.DBInputFormat: Using read commited transaction isolation17/03/28 21:01:33 INFO mapreduce.JobSubmitter: number of splits:117/03/28 21:01:33 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003517/03/28 21:01:34 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003517/03/28 21:01:34 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0035/17/03/28 21:01:34 INFO mapreduce.Job: Running job: job_1490408992134_003517/03/28 21:01:45 INFO mapreduce.Job: Job job_1490408992134_0035 running in uber mode : false17/03/28 21:01:45 INFO mapreduce.Job:  map 0% reduce 0%17/03/28 21:01:51 INFO mapreduce.Job:  map 100% reduce 0%17/03/28 21:01:52 INFO mapreduce.Job: Job job_1490408992134_0035 completed successfully17/03/28 21:01:52 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143787		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=87		HDFS: Number of bytes written=3754		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=3033		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=3033		Total vcore-seconds taken by all map tasks=3033		Total megabyte-seconds taken by all map tasks=3105792	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=87		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=47		CPU time spent (ms)=750		Physical memory (bytes) snapshot=186785792		Virtual memory (bytes) snapshot=2779492352		Total committed heap usage (bytes)=191365120	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=375417/03/28 21:01:52 INFO mapreduce.ImportJobBase: Transferred 3.666 KB in 26.5732 seconds (141.27 bytes/sec)17/03/28 21:01:52 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/28 21:01:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `SYS_USER` AS t LIMIT 117/03/28 21:01:52 WARN hive.TableDefWriter: Column login_date had to be cast to a less precise type in Hive17/03/28 21:01:52 WARN hive.TableDefWriter: Column create_date had to be cast to a less precise type in Hive17/03/28 21:01:52 WARN hive.TableDefWriter: Column update_date had to be cast to a less precise type in Hive17/03/28 21:01:52 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.701 secondsLoading data to table default.sys_userTable default.sys_user stats: [numFiles=2, numRows=0, totalSize=7508, rawDataSize=0]OKTime taken: 1.231 seconds
[21:02:47] INFO:    Now wait 5 seconds to begin next task ...
[21:02:52] INFO:    Connection channel disconnect
[21:03:08] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[21:03:08] INFO:    Connection channel established succesfully
[21:03:08] INFO:    Start to run command
[21:03:22] INFO:    Connection channel closed
[21:03:22] INFO:    Check if exec success or not ... 
[21:03:22] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.xxxxxxxx;ALTER TABLE xxxxxxxx RENAME TO hl_bak.xxxxxxxx'
[21:03:22] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/28 21:02:24 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.324 secondsFAILED: SemanticException [Error 10001]: Table not found default.xxxxxxxx
[21:03:22] INFO:    Now wait 5 seconds to begin next task ...
[21:03:27] INFO:    Connection channel disconnect
[21:03:27] INFO:    Command is sudo -u hdfs sqoop import -m 1 --table DC_JOB_TRANSDATA --hive-table DC_JOB_TRANSDATA --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[21:03:27] INFO:    Connection channel established succesfully
[21:03:27] INFO:    Start to run command
[21:03:27] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.


=============== [2017/03/29 09:06:30, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[09:06:30] INFO:    SSHExec initializing ...
[09:06:30] INFO:    Session initialized and associated with user credential 123456
[09:06:30] INFO:    Session initialized and associated with user credential 123456
[09:06:30] INFO:    Session initialized and associated with user credential 123456
[09:06:30] INFO:    Session initialized and associated with user credential 123456
[09:06:30] INFO:    SSHExec initialized successfully
[09:06:30] INFO:    SSHExec initialized successfully
[09:06:30] INFO:    SSHExec initialized successfully
[09:06:30] INFO:    SSHExec trying to connect root@172.16.110.200
[09:06:30] INFO:    SSHExec trying to connect root@172.16.110.200
[09:06:30] INFO:    SSHExec initialized successfully
[09:06:30] INFO:    SSHExec trying to connect root@172.16.110.200
[09:06:30] INFO:    SSHExec trying to connect root@172.16.110.200

=============== [2017/03/29 15:00:13, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:00:13] INFO:    SSHExec initializing ...
[15:00:13] INFO:    Session initialized and associated with user credential 123456
[15:00:13] INFO:    Session initialized and associated with user credential 123456
[15:00:13] INFO:    Session initialized and associated with user credential 123456
[15:00:13] INFO:    Session initialized and associated with user credential 123456
[15:00:13] INFO:    SSHExec initialized successfully
[15:00:13] INFO:    SSHExec initialized successfully
[15:00:13] INFO:    SSHExec initialized successfully
[15:00:13] INFO:    SSHExec trying to connect root@172.16.110.200
[15:00:13] INFO:    SSHExec trying to connect root@172.16.110.200
[15:00:13] INFO:    SSHExec initialized successfully
[15:00:13] INFO:    SSHExec trying to connect root@172.16.110.200
[15:00:13] INFO:    SSHExec trying to connect root@172.16.110.200
[15:00:14] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:00:14] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:00:14] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:00:14] ERROR:   session is down
[15:00:14] ERROR:   session is down
[15:00:14] ERROR:   session is down
[15:00:14] ERROR:   session is down
[15:00:14] ERROR:   session is down
[15:00:14] ERROR:   session is down
[15:00:14] INFO:    SSH connection shutdown
[15:00:14] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[15:00:14] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[15:00:14] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:00:14] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException

=============== [2017/03/29 15:03:09, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:03:09] INFO:    SSHExec initializing ...
[15:03:09] INFO:    Session initialized and associated with user credential 123456
[15:03:09] INFO:    SSHExec initialized successfully
[15:03:09] INFO:    SSHExec trying to connect root@172.16.110.200
[15:03:09] INFO:    SSH connection established
[15:03:16] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:03:16] INFO:    Connection channel established succesfully
[15:03:16] INFO:    Start to run command
[15:03:31] INFO:    Connection channel closed
[15:03:31] INFO:    Check if exec success or not ... 
[15:03:31] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:03:31] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:02:46 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.232 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:03:31] INFO:    Now wait 5 seconds to begin next task ...
[15:03:36] INFO:    Connection channel disconnect
[15:03:36] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:03:36] INFO:    Connection channel established succesfully
[15:03:36] INFO:    Start to run command
[15:03:36] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:04:28] INFO:    Connection channel closed
[15:04:28] INFO:    Check if exec success or not ... 
[15:04:28] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:04:28] INFO:    Error message: 17/03/29 15:03:04 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:03:04 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:03:04 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:03:04 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:03:05 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:03:05 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:03:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:03:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:03:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d22e668f652b46834f55723d56b57f91/DC_CATA_DETAIL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:03:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/d22e668f652b46834f55723d56b57f91/DC_CATA_DETAIL.jar17/03/29 15:03:09 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:03:09 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:03:09 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:03:09 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:03:09 INFO mapreduce.ImportJobBase: Beginning import of DC_CATA_DETAIL17/03/29 15:03:10 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:03:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:03:12 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:03:19 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:03:19 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_CATA_DETAIL`17/03/29 15:03:19 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:03:19 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:03:19 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:03:19 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:03:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003717/03/29 15:03:20 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003717/03/29 15:03:20 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0037/17/03/29 15:03:20 INFO mapreduce.Job: Running job: job_1490408992134_003717/03/29 15:03:31 INFO mapreduce.Job: Job job_1490408992134_0037 running in uber mode : false17/03/29 15:03:31 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:03:38 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:03:40 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:03:41 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:03:45 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:03:45 INFO mapreduce.Job: Job job_1490408992134_0037 completed successfully17/03/29 15:03:45 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862440		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=8698		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20513		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20513		Total vcore-seconds taken by all map tasks=20513		Total megabyte-seconds taken by all map tasks=21005312	Map-Reduce Framework		Map input records=33		Map output records=33		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=399		CPU time spent (ms)=4930		Physical memory (bytes) snapshot=1042354176		Virtual memory (bytes) snapshot=16677257216		Total committed heap usage (bytes)=1008205824	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=869817/03/29 15:03:45 INFO mapreduce.ImportJobBase: Transferred 8.4941 KB in 33.6981 seconds (258.1155 bytes/sec)17/03/29 15:03:45 INFO mapreduce.ImportJobBase: Retrieved 33 records.17/03/29 15:03:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:03:45 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:03:45 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:03:45 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.126 secondsLoading data to table default.dc_cata_detailTable default.dc_cata_detail stats: [numFiles=6, totalSize=8698]OKTime taken: 1.692 seconds
[15:04:28] INFO:    Now wait 5 seconds to begin next task ...
[15:04:33] INFO:    Connection channel disconnect
[15:04:39] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:04:39] INFO:    Connection channel established succesfully
[15:04:39] INFO:    Start to run command
[15:04:53] INFO:    Connection channel closed
[15:04:53] INFO:    Check if exec success or not ... 
[15:04:53] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:04:53] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:04:11 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.28 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:04:53] INFO:    Now wait 5 seconds to begin next task ...
[15:04:58] INFO:    Connection channel disconnect
[15:04:58] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:04:58] INFO:    Connection channel established succesfully
[15:04:58] INFO:    Start to run command
[15:04:58] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.


=============== [2017/03/29 15:07:54, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:07:54] INFO:    SSHExec initializing ...
[15:07:54] INFO:    Session initialized and associated with user credential 123456
[15:07:54] INFO:    SSHExec initialized successfully
[15:07:54] INFO:    SSHExec trying to connect root@172.16.110.200
[15:07:54] INFO:    SSH connection established
[15:07:56] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:07:56] INFO:    Connection channel established succesfully
[15:07:56] INFO:    Start to run command
[15:08:12] INFO:    Connection channel closed
[15:08:12] INFO:    Check if exec success or not ... 
[15:08:12] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:08:12] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:07:29 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.64 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:08:12] INFO:    Now wait 5 seconds to begin next task ...
[15:08:17] INFO:    Connection channel disconnect
[15:08:17] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:08:17] INFO:    Connection channel established succesfully
[15:08:17] INFO:    Start to run command
[15:08:17] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:09:08] INFO:    Connection channel closed
[15:09:08] INFO:    Check if exec success or not ... 
[15:09:08] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:09:08] INFO:    Error message: 17/03/29 15:07:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:07:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:07:48 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:07:48 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:07:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:07:48 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:07:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:07:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:07:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/b648955b8b42913ae3a7a7266cfece0f/DC_CATA_DETAIL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:07:53 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/b648955b8b42913ae3a7a7266cfece0f/DC_CATA_DETAIL.jar17/03/29 15:07:53 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:07:53 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:07:53 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:07:53 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:07:53 INFO mapreduce.ImportJobBase: Beginning import of DC_CATA_DETAIL17/03/29 15:07:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:07:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:07:55 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:08:03 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:08:03 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_CATA_DETAIL`17/03/29 15:08:03 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:08:03 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:08:03 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:08:03 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:08:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_003917/03/29 15:08:04 INFO impl.YarnClientImpl: Submitted application application_1490408992134_003917/03/29 15:08:04 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0039/17/03/29 15:08:04 INFO mapreduce.Job: Running job: job_1490408992134_003917/03/29 15:08:14 INFO mapreduce.Job: Job job_1490408992134_0039 running in uber mode : false17/03/29 15:08:14 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:08:20 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:08:22 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:08:23 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 15:08:24 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:08:28 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:08:29 INFO mapreduce.Job: Job job_1490408992134_0039 completed successfully17/03/29 15:08:29 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862440		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=8698		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20521		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20521		Total vcore-seconds taken by all map tasks=20521		Total megabyte-seconds taken by all map tasks=21013504	Map-Reduce Framework		Map input records=33		Map output records=33		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=372		CPU time spent (ms)=5110		Physical memory (bytes) snapshot=1050378240		Virtual memory (bytes) snapshot=16678047744		Total committed heap usage (bytes)=1051721728	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=869817/03/29 15:08:29 INFO mapreduce.ImportJobBase: Transferred 8.4941 KB in 34.2039 seconds (254.2982 bytes/sec)17/03/29 15:08:29 INFO mapreduce.ImportJobBase: Retrieved 33 records.17/03/29 15:08:29 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:08:29 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:08:29 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:08:29 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.605 secondsLoading data to table default.dc_cata_detailTable default.dc_cata_detail stats: [numFiles=12, totalSize=17396]OKTime taken: 1.625 seconds
[15:09:08] INFO:    Now wait 5 seconds to begin next task ...
[15:09:13] INFO:    Connection channel disconnect
[15:09:16] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:09:16] INFO:    Connection channel established succesfully
[15:09:16] INFO:    Start to run command
[15:09:29] INFO:    Connection channel closed
[15:09:29] INFO:    Check if exec success or not ... 
[15:09:29] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:09:29] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:08:48 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.2 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:09:29] INFO:    Now wait 5 seconds to begin next task ...
[15:09:34] INFO:    Connection channel disconnect
[15:09:34] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:09:34] INFO:    Connection channel established succesfully
[15:09:34] INFO:    Start to run command
[15:09:35] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[15:09:35] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:10:24] INFO:    Connection channel closed
[15:10:24] INFO:    Check if exec success or not ... 
[15:10:24] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:10:24] INFO:    Error message: 17/03/29 15:09:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:09:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:09:05 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:09:05 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:09:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:09:06 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:09:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:09:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:09:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/dccafe2546e6d18ed4bb4fecffafaca6/DC_DATA_SOURCE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:09:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/dccafe2546e6d18ed4bb4fecffafaca6/DC_DATA_SOURCE.jar17/03/29 15:09:11 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:09:11 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:09:11 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:09:11 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:09:11 INFO mapreduce.ImportJobBase: Beginning import of DC_DATA_SOURCE17/03/29 15:09:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:09:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:09:13 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:09:19 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:09:19 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_DATA_SOURCE`17/03/29 15:09:19 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:09:19 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:09:19 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:09:19 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:09:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004017/03/29 15:09:20 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004017/03/29 15:09:21 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0040/17/03/29 15:09:21 INFO mapreduce.Job: Running job: job_1490408992134_004017/03/29 15:09:31 INFO mapreduce.Job: Job job_1490408992134_0040 running in uber mode : false17/03/29 15:09:31 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:09:37 INFO mapreduce.Job:  map 17% reduce 0%17/03/29 15:09:38 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:09:39 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:09:40 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 15:09:41 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:09:44 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:09:44 INFO mapreduce.Job: Job job_1490408992134_0040 completed successfully17/03/29 15:09:45 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862908		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=3306		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=21998		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=21998		Total vcore-seconds taken by all map tasks=21998		Total megabyte-seconds taken by all map tasks=22525952	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=384		CPU time spent (ms)=5300		Physical memory (bytes) snapshot=1054109696		Virtual memory (bytes) snapshot=16679411712		Total committed heap usage (bytes)=1028128768	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=330617/03/29 15:09:45 INFO mapreduce.ImportJobBase: Transferred 3.2285 KB in 31.7219 seconds (104.2184 bytes/sec)17/03/29 15:09:45 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/29 15:09:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:09:45 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:09:45 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:09:45 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.612 secondsLoading data to table default.dc_data_sourceTable default.dc_data_source stats: [numFiles=12, totalSize=6612]OKTime taken: 1.714 seconds
[15:10:24] INFO:    Now wait 5 seconds to begin next task ...
[15:10:29] INFO:    Connection channel disconnect
[15:10:53] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_HIVE_DATABASE --hive-table DC_HIVE_DATABASE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:10:53] INFO:    Connection channel established succesfully
[15:10:53] INFO:    Start to run command
[15:11:08] INFO:    Connection channel closed
[15:11:08] INFO:    Check if exec success or not ... 
[15:11:08] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_HIVE_DATABASE --hive-table DC_HIVE_DATABASE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:11:08] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:10:25 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.627 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:11:08] INFO:    Now wait 5 seconds to begin next task ...
[15:11:13] INFO:    Connection channel disconnect
[15:11:13] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_HIVE_DATABASE --hive-table DC_HIVE_DATABASE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:11:13] INFO:    Connection channel established succesfully
[15:11:13] INFO:    Start to run command
[15:11:13] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:12:01] INFO:    Connection channel closed
[15:12:01] INFO:    Check if exec success or not ... 
[15:12:01] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_HIVE_DATABASE --hive-table DC_HIVE_DATABASE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:12:01] INFO:    Error message: 17/03/29 15:10:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:10:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:10:44 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:10:44 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:10:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:10:44 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:10:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_DATABASE` AS t LIMIT 117/03/29 15:10:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_DATABASE` AS t LIMIT 117/03/29 15:10:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/e6f0399b5ff5068068d2e9f458f13e25/DC_HIVE_DATABASE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:10:48 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/e6f0399b5ff5068068d2e9f458f13e25/DC_HIVE_DATABASE.jar17/03/29 15:10:48 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:10:49 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:10:49 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:10:49 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:10:49 INFO mapreduce.ImportJobBase: Beginning import of DC_HIVE_DATABASE17/03/29 15:10:49 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:10:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:10:50 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:10:57 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:10:57 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_HIVE_DATABASE`17/03/29 15:10:57 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:10:57 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:10:57 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:10:57 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:10:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004117/03/29 15:10:58 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004117/03/29 15:10:58 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0041/17/03/29 15:10:58 INFO mapreduce.Job: Running job: job_1490408992134_004117/03/29 15:11:09 INFO mapreduce.Job: Job job_1490408992134_0041 running in uber mode : false17/03/29 15:11:09 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:11:14 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:11:17 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:11:18 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 15:11:19 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:11:22 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:11:22 INFO mapreduce.Job: Job job_1490408992134_0041 completed successfully17/03/29 15:11:22 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862152		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=661		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20384		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20384		Total vcore-seconds taken by all map tasks=20384		Total megabyte-seconds taken by all map tasks=20873216	Map-Reduce Framework		Map input records=4		Map output records=4		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=371		CPU time spent (ms)=5010		Physical memory (bytes) snapshot=1038180352		Virtual memory (bytes) snapshot=16676290560		Total committed heap usage (bytes)=1047527424	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=66117/03/29 15:11:22 INFO mapreduce.ImportJobBase: Transferred 661 bytes in 32.2853 seconds (20.4737 bytes/sec)17/03/29 15:11:22 INFO mapreduce.ImportJobBase: Retrieved 4 records.17/03/29 15:11:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_DATABASE` AS t LIMIT 117/03/29 15:11:22 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:11:22 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:11:22 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.653 secondsLoading data to table default.dc_hive_databaseTable default.dc_hive_database stats: [numFiles=6, totalSize=661]OKTime taken: 1.07 seconds
[15:12:01] INFO:    Now wait 5 seconds to begin next task ...
[15:12:06] INFO:    Connection channel disconnect
[15:12:10] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_HIVE_TABLE --hive-table DC_HIVE_TABLE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:12:10] INFO:    Connection channel established succesfully
[15:12:10] INFO:    Start to run command
[15:12:25] INFO:    Connection channel closed
[15:12:25] INFO:    Check if exec success or not ... 
[15:12:25] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_HIVE_TABLE --hive-table DC_HIVE_TABLE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:12:25] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:11:43 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.377 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:12:25] INFO:    Now wait 5 seconds to begin next task ...
[15:12:30] INFO:    Connection channel disconnect
[15:12:30] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_HIVE_TABLE --hive-table DC_HIVE_TABLE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:12:30] INFO:    Connection channel established succesfully
[15:12:30] INFO:    Start to run command
[15:12:30] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:13:23] INFO:    Connection channel closed
[15:13:23] INFO:    Check if exec success or not ... 
[15:13:23] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_HIVE_TABLE --hive-table DC_HIVE_TABLE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:13:23] INFO:    Error message: 17/03/29 15:12:01 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:12:01 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:12:01 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:12:01 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:12:01 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:12:01 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:12:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_TABLE` AS t LIMIT 117/03/29 15:12:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_TABLE` AS t LIMIT 117/03/29 15:12:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/6fa1e69970cf5dd457d411a615921bb5/DC_HIVE_TABLE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:12:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/6fa1e69970cf5dd457d411a615921bb5/DC_HIVE_TABLE.jar17/03/29 15:12:06 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:12:06 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:12:06 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:12:06 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:12:06 INFO mapreduce.ImportJobBase: Beginning import of DC_HIVE_TABLE17/03/29 15:12:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:12:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:12:09 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:12:16 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:12:16 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_HIVE_TABLE`17/03/29 15:12:16 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:12:16 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:12:16 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:12:16 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:12:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004217/03/29 15:12:17 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004217/03/29 15:12:18 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0042/17/03/29 15:12:18 INFO mapreduce.Job: Running job: job_1490408992134_004217/03/29 15:12:27 INFO mapreduce.Job: Job job_1490408992134_0042 running in uber mode : false17/03/29 15:12:27 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:12:33 INFO mapreduce.Job:  map 17% reduce 0%17/03/29 15:12:34 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:12:35 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:12:36 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 15:12:37 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:12:40 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:12:41 INFO mapreduce.Job: Job job_1490408992134_0042 completed successfully17/03/29 15:12:42 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862266		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=3434		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=22302		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=22302		Total vcore-seconds taken by all map tasks=22302		Total megabyte-seconds taken by all map tasks=22837248	Map-Reduce Framework		Map input records=23		Map output records=23		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=386		CPU time spent (ms)=5830		Physical memory (bytes) snapshot=1052069888		Virtual memory (bytes) snapshot=16676515840		Total committed heap usage (bytes)=1028653056	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=343417/03/29 15:12:42 INFO mapreduce.ImportJobBase: Transferred 3.3535 KB in 32.9735 seconds (104.1444 bytes/sec)17/03/29 15:12:42 INFO mapreduce.ImportJobBase: Retrieved 23 records.17/03/29 15:12:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_HIVE_TABLE` AS t LIMIT 117/03/29 15:12:42 WARN hive.TableDefWriter: Column CREATE_TIME had to be cast to a less precise type in Hive17/03/29 15:12:42 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.71 secondsLoading data to table default.dc_hive_tableTable default.dc_hive_table stats: [numFiles=6, totalSize=3434]OKTime taken: 1.412 seconds
[15:13:23] INFO:    Now wait 5 seconds to begin next task ...
[15:13:28] INFO:    Connection channel disconnect
[15:13:30] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:13:30] INFO:    Connection channel established succesfully
[15:13:30] INFO:    Start to run command
[15:13:46] INFO:    Connection channel closed
[15:13:46] INFO:    Check if exec success or not ... 
[15:13:46] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.null'
[15:13:46] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:13:01 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.243 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:13:46] INFO:    Now wait 5 seconds to begin next task ...
[15:13:51] INFO:    Connection channel disconnect
[15:13:51] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:13:51] INFO:    Connection channel established succesfully
[15:13:51] INFO:    Start to run command
[15:13:51] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[15:13:51] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:14:00] INFO:    Connection channel closed
[15:14:00] INFO:    Check if exec success or not ... 
[15:14:00] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:14:00] INFO:    Error message: 17/03/29 15:13:20 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:13:20 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:13:20 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:13:20 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:13:20 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:13:20 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:13:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_HDFSLOG` AS t LIMIT 117/03/29 15:13:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_JOB_HDFSLOG` AS t LIMIT 117/03/29 15:13:21 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/225f96dbeea6136c8aa2bb9d52551862/DC_JOB_HDFSLOG.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:13:25 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/225f96dbeea6136c8aa2bb9d52551862/DC_JOB_HDFSLOG.jar17/03/29 15:13:25 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:13:25 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:13:25 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:13:25 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:13:25 ERROR tool.ImportTool: Error during import: No primary key could be found for table DC_JOB_HDFSLOG. Please specify one with --split-by or perform a sequential import with '-m 1'.
[15:14:00] INFO:    Now wait 5 seconds to begin next task ...
[15:14:05] INFO:    Connection channel disconnect
[15:14:05] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5; ALTER TABLE hl_bak.null RENAME TO sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5'
[15:14:05] INFO:    Connection channel established succesfully
[15:14:05] INFO:    Start to run command
[15:14:20] INFO:    Connection channel closed
[15:14:20] INFO:    Check if exec success or not ... 
[15:14:20] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5; ALTER TABLE hl_bak.null RENAME TO sudo -u hdfs sqoop import -m 4 --table DC_JOB_HDFSLOG --hive-table DC_JOB_HDFSLOG --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5'
[15:14:20] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:13:36 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.dropTableStatement(HiveParser.java:6929)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2572)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:26 cannot recognize input near 'sudo' '-' 'u' in table name
[15:14:20] INFO:    Now wait 5 seconds to begin next task ...
[15:14:25] INFO:    Connection channel disconnect
[15:14:28] INFO:    SSH connection shutdown

=============== [2017/03/29 15:35:02, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:35:02] INFO:    SSHExec initializing ...
[15:35:02] INFO:    Session initialized and associated with user credential 123456
[15:35:02] INFO:    SSHExec initialized successfully
[15:35:02] INFO:    SSHExec trying to connect root@172.16.110.200
[15:35:03] INFO:    SSH connection established
[15:35:03] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.null'
[15:35:03] INFO:    Connection channel established succesfully
[15:35:03] INFO:    Start to run command
[15:35:19] INFO:    Connection channel closed
[15:35:19] INFO:    Check if exec success or not ... 
[15:35:19] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.null'
[15:35:19] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:34:27 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.922 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:35:19] INFO:    Now wait 5 seconds to begin next task ...
[15:35:24] INFO:    Connection channel disconnect
[15:35:24] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:35:24] INFO:    Connection channel established succesfully
[15:35:24] INFO:    Start to run command
[15:35:25] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:36:16] INFO:    Connection channel closed
[15:36:16] INFO:    Check if exec success or not ... 
[15:36:16] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:36:16] INFO:    Error message: 17/03/29 15:34:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:34:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:34:48 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:34:48 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:34:48 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:34:48 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:34:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:34:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:34:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/4d44f0fabda4e4d1c3b3636fb35ce284/DC_CATA_DETAIL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:34:53 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/4d44f0fabda4e4d1c3b3636fb35ce284/DC_CATA_DETAIL.jar17/03/29 15:34:53 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:34:53 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:34:53 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:34:53 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:34:53 INFO mapreduce.ImportJobBase: Beginning import of DC_CATA_DETAIL17/03/29 15:34:53 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:34:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:34:55 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:35:01 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:35:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_CATA_DETAIL`17/03/29 15:35:01 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:35:01 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:35:01 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:35:01 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:35:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004317/03/29 15:35:02 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004317/03/29 15:35:02 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0043/17/03/29 15:35:02 INFO mapreduce.Job: Running job: job_1490408992134_004317/03/29 15:35:14 INFO mapreduce.Job: Job job_1490408992134_0043 running in uber mode : false17/03/29 15:35:14 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:35:19 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:35:21 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:35:23 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:35:27 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:35:27 INFO mapreduce.Job: Job job_1490408992134_0043 completed successfully17/03/29 15:35:27 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862440		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=8698		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20299		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20299		Total vcore-seconds taken by all map tasks=20299		Total megabyte-seconds taken by all map tasks=20786176	Map-Reduce Framework		Map input records=33		Map output records=33		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=411		CPU time spent (ms)=4960		Physical memory (bytes) snapshot=1038209024		Virtual memory (bytes) snapshot=16676438016		Total committed heap usage (bytes)=1007157248	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=869817/03/29 15:35:27 INFO mapreduce.ImportJobBase: Transferred 8.4941 KB in 32.5114 seconds (267.5371 bytes/sec)17/03/29 15:35:27 INFO mapreduce.ImportJobBase: Retrieved 33 records.17/03/29 15:35:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 15:35:27 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:35:27 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:35:27 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.534 secondsLoading data to table default.dc_cata_detailTable default.dc_cata_detail stats: [numFiles=18, numRows=0, totalSize=26094, rawDataSize=0]OKTime taken: 1.863 seconds
[15:36:16] INFO:    Now wait 5 seconds to begin next task ...
[15:36:21] INFO:    Connection channel disconnect
[15:36:21] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.null'
[15:36:21] INFO:    Connection channel established succesfully
[15:36:21] INFO:    Start to run command
[15:36:36] INFO:    Connection channel closed
[15:36:36] INFO:    Check if exec success or not ... 
[15:36:36] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.null;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.null'
[15:36:36] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 15:35:46 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.413 secondsNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.alterStatement(HiveParser.java:7183)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2602)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:17 cannot recognize input near 'sudo' '-' 'u' in table name
[15:36:36] INFO:    Now wait 5 seconds to begin next task ...
[15:36:41] INFO:    Connection channel disconnect
[15:36:41] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:36:41] INFO:    Connection channel established succesfully
[15:36:41] INFO:    Start to run command
[15:36:41] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:37:31] INFO:    Connection channel closed
[15:37:31] INFO:    Check if exec success or not ... 
[15:37:31] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[15:37:31] INFO:    Error message: 17/03/29 15:36:04 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 15:36:04 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 15:36:04 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 15:36:04 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 15:36:04 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 15:36:04 INFO tool.CodeGenTool: Beginning code generation17/03/29 15:36:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:36:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:36:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/2d522139839aa42df3ec5f38f5fa9e31/DC_DATA_SOURCE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 15:36:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/2d522139839aa42df3ec5f38f5fa9e31/DC_DATA_SOURCE.jar17/03/29 15:36:10 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 15:36:10 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 15:36:10 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 15:36:10 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 15:36:10 INFO mapreduce.ImportJobBase: Beginning import of DC_DATA_SOURCE17/03/29 15:36:10 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 15:36:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 15:36:12 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 15:36:18 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 15:36:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_DATA_SOURCE`17/03/29 15:36:18 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 15:36:18 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 15:36:18 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 15:36:18 INFO mapreduce.JobSubmitter: number of splits:617/03/29 15:36:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004417/03/29 15:36:19 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004417/03/29 15:36:19 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0044/17/03/29 15:36:19 INFO mapreduce.Job: Running job: job_1490408992134_004417/03/29 15:36:29 INFO mapreduce.Job: Job job_1490408992134_0044 running in uber mode : false17/03/29 15:36:29 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 15:36:36 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 15:36:38 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 15:36:39 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 15:36:43 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 15:36:44 INFO mapreduce.Job: Job job_1490408992134_0044 completed successfully17/03/29 15:36:44 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862908		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=3306		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20024		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20024		Total vcore-seconds taken by all map tasks=20024		Total megabyte-seconds taken by all map tasks=20504576	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=380		CPU time spent (ms)=5030		Physical memory (bytes) snapshot=1044484096		Virtual memory (bytes) snapshot=16679751680		Total committed heap usage (bytes)=1002962944	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=330617/03/29 15:36:45 INFO mapreduce.ImportJobBase: Transferred 3.2285 KB in 32.5781 seconds (101.4791 bytes/sec)17/03/29 15:36:45 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/29 15:36:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 15:36:45 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 15:36:45 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 15:36:45 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.401 secondsLoading data to table default.dc_data_sourceTable default.dc_data_source stats: [numFiles=18, numRows=0, totalSize=9918, rawDataSize=0]OKTime taken: 1.001 seconds
[15:37:31] INFO:    Now wait 5 seconds to begin next task ...
[15:37:36] INFO:    Connection channel disconnect
[15:37:36] INFO:    SSH connection shutdown
[15:37:36] INFO:    Session initialized and associated with user credential 123456
[15:37:36] INFO:    Session initialized and associated with user credential 123456
[15:37:36] INFO:    SSHExec initialized successfully
[15:37:36] INFO:    Session initialized and associated with user credential 123456
[15:37:36] INFO:    SSHExec initialized successfully
[15:37:36] INFO:    SSHExec initialized successfully
[15:37:36] INFO:    SSHExec trying to connect root@172.16.110.200
[15:37:36] INFO:    SSHExec trying to connect root@172.16.110.200
[15:37:36] INFO:    SSHExec trying to connect root@172.16.110.200
[15:37:36] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:37:36] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:37:36] ERROR:   Connect fails with the following exception: com.jcraft.jsch.JSchException: connection is closed by foreign host
[15:37:36] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[15:37:36] INFO:    SSH connection shutdown
[15:37:36] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException

=============== [2017/03/29 17:24:03, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:24:03] INFO:    SSHExec initializing ...
[17:24:03] INFO:    Session initialized and associated with user credential 123456
[17:24:03] INFO:    SSHExec initialized successfully
[17:24:03] INFO:    SSHExec trying to connect root@172.16.110.200
[17:24:04] INFO:    SSH connection established
[17:24:04] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:24:04] INFO:    Connection channel established succesfully
[17:24:04] INFO:    Start to run command
[17:24:19] INFO:    Connection channel closed
[17:24:19] INFO:    Check if exec success or not ... 
[17:24:19] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:24:19] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 17:23:32 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[17:24:19] INFO:    Now wait 5 seconds to begin next task ...
[17:24:24] INFO:    Connection channel disconnect
[17:24:24] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:24:24] INFO:    Connection channel established succesfully
[17:24:24] INFO:    Start to run command
[17:24:24] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:25:15] INFO:    Connection channel closed
[17:25:15] INFO:    Check if exec success or not ... 
[17:25:15] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:25:15] INFO:    Error message: 17/03/29 17:23:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 17:23:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 17:23:50 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 17:23:50 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 17:23:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 17:23:50 INFO tool.CodeGenTool: Beginning code generation17/03/29 17:23:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:23:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:23:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/fed53734a536490c4623154082128252/DC_CATA_DETAIL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 17:23:55 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/fed53734a536490c4623154082128252/DC_CATA_DETAIL.jar17/03/29 17:23:55 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 17:23:55 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 17:23:55 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 17:23:55 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 17:23:55 INFO mapreduce.ImportJobBase: Beginning import of DC_CATA_DETAIL17/03/29 17:23:56 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 17:23:57 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 17:23:57 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 17:24:05 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 17:24:05 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_CATA_DETAIL`17/03/29 17:24:05 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 17:24:05 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 17:24:05 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 17:24:05 INFO mapreduce.JobSubmitter: number of splits:617/03/29 17:24:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004717/03/29 17:24:06 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004717/03/29 17:24:06 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0047/17/03/29 17:24:06 INFO mapreduce.Job: Running job: job_1490408992134_004717/03/29 17:24:16 INFO mapreduce.Job: Job job_1490408992134_0047 running in uber mode : false17/03/29 17:24:16 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 17:24:23 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 17:24:25 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 17:24:26 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 17:24:30 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 17:24:31 INFO mapreduce.Job: Job job_1490408992134_0047 completed successfully17/03/29 17:24:31 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862440		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=8698		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20530		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20530		Total vcore-seconds taken by all map tasks=20530		Total megabyte-seconds taken by all map tasks=21022720	Map-Reduce Framework		Map input records=33		Map output records=33		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=342		CPU time spent (ms)=5340		Physical memory (bytes) snapshot=1055555584		Virtual memory (bytes) snapshot=16676470784		Total committed heap usage (bytes)=1071120384	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=869817/03/29 17:24:31 INFO mapreduce.ImportJobBase: Transferred 8.4941 KB in 34.1641 seconds (254.595 bytes/sec)17/03/29 17:24:31 INFO mapreduce.ImportJobBase: Retrieved 33 records.17/03/29 17:24:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:24:31 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 17:24:31 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 17:24:31 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.909 secondsLoading data to table default.dc_cata_detailTable default.dc_cata_detail stats: [numFiles=24, numRows=0, totalSize=34792, rawDataSize=0]OKTime taken: 1.796 seconds
[17:25:15] INFO:    Now wait 5 seconds to begin next task ...
[17:25:20] INFO:    Connection channel disconnect
[17:25:20] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:25:20] INFO:    Connection channel established succesfully
[17:25:20] INFO:    Start to run command
[17:25:35] INFO:    Connection channel closed
[17:25:35] INFO:    Check if exec success or not ... 
[17:25:35] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:25:35] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 17:24:49 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[17:25:35] INFO:    Now wait 5 seconds to begin next task ...
[17:25:40] INFO:    Connection channel disconnect
[17:25:40] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:25:40] INFO:    Connection channel established succesfully
[17:25:40] INFO:    Start to run command
[17:25:40] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:26:31] INFO:    Connection channel closed
[17:26:31] INFO:    Check if exec success or not ... 
[17:26:31] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:26:31] INFO:    Error message: 17/03/29 17:25:06 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 17:25:06 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 17:25:06 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 17:25:06 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 17:25:07 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 17:25:07 INFO tool.CodeGenTool: Beginning code generation17/03/29 17:25:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:25:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:25:08 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/6b2024f1f59bdc4439f4763da53b8ce0/DC_DATA_SOURCE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 17:25:12 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/6b2024f1f59bdc4439f4763da53b8ce0/DC_DATA_SOURCE.jar17/03/29 17:25:12 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 17:25:12 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 17:25:12 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 17:25:12 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 17:25:12 INFO mapreduce.ImportJobBase: Beginning import of DC_DATA_SOURCE17/03/29 17:25:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 17:25:14 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 17:25:14 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 17:25:21 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 17:25:21 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_DATA_SOURCE`17/03/29 17:25:21 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 17:25:21 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 17:25:21 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 17:25:22 INFO mapreduce.JobSubmitter: number of splits:617/03/29 17:25:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_004817/03/29 17:25:23 INFO impl.YarnClientImpl: Submitted application application_1490408992134_004817/03/29 17:25:23 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0048/17/03/29 17:25:23 INFO mapreduce.Job: Running job: job_1490408992134_004817/03/29 17:25:34 INFO mapreduce.Job: Job job_1490408992134_0048 running in uber mode : false17/03/29 17:25:34 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 17:25:40 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 17:25:42 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 17:25:43 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 17:25:44 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 17:25:47 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 17:25:47 INFO mapreduce.Job: Job job_1490408992134_0048 completed successfully17/03/29 17:25:47 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862908		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=3306		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=20418		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20418		Total vcore-seconds taken by all map tasks=20418		Total megabyte-seconds taken by all map tasks=20908032	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=403		CPU time spent (ms)=4940		Physical memory (bytes) snapshot=1051406336		Virtual memory (bytes) snapshot=16677732352		Total committed heap usage (bytes)=1031798784	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=330617/03/29 17:25:47 INFO mapreduce.ImportJobBase: Transferred 3.2285 KB in 33.4997 seconds (98.6874 bytes/sec)17/03/29 17:25:47 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/29 17:25:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:25:48 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 17:25:48 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 17:25:48 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.041 secondsLoading data to table default.dc_data_sourceTable default.dc_data_source stats: [numFiles=24, numRows=0, totalSize=13224, rawDataSize=0]OKTime taken: 2.138 seconds
[17:26:31] INFO:    Now wait 5 seconds to begin next task ...
[17:26:36] INFO:    Connection channel disconnect
[17:26:36] INFO:    SSH connection shutdown
[17:26:36] INFO:    Session initialized and associated with user credential 123456
[17:26:36] INFO:    SSHExec initialized successfully
[17:26:36] INFO:    SSHExec trying to connect root@172.16.110.200
[17:26:36] INFO:    SSH connection established
[17:26:36] INFO:    SSH connection shutdown

=============== [2017/03/29 17:30:10, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[17:30:10] INFO:    SSHExec initializing ...
[17:30:10] INFO:    Session initialized and associated with user credential 123456
[17:30:10] INFO:    SSHExec initialized successfully
[17:30:10] INFO:    SSHExec trying to connect root@172.16.110.200
[17:30:10] INFO:    SSH connection established
[17:30:10] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:30:10] INFO:    Connection channel established succesfully
[17:30:10] INFO:    Start to run command
[17:30:24] INFO:    Connection channel closed
[17:30:24] INFO:    Check if exec success or not ... 
[17:30:24] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:30:24] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 17:29:45 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[17:30:24] INFO:    Now wait 5 seconds to begin next task ...
[17:30:29] INFO:    Connection channel disconnect
[17:30:29] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:30:29] INFO:    Connection channel established succesfully
[17:30:29] INFO:    Start to run command
[17:30:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:31:18] INFO:    Connection channel closed
[17:31:18] INFO:    Check if exec success or not ... 
[17:31:18] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_CATA_DETAIL --hive-table DC_CATA_DETAIL_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:31:18] INFO:    Error message: 17/03/29 17:30:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 17:30:03 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 17:30:03 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 17:30:03 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 17:30:03 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 17:30:03 INFO tool.CodeGenTool: Beginning code generation17/03/29 17:30:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:30:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:30:04 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/0681d7ad951e9962a15b69a6dd08e6c1/DC_CATA_DETAIL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 17:30:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/0681d7ad951e9962a15b69a6dd08e6c1/DC_CATA_DETAIL.jar17/03/29 17:30:08 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 17:30:08 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 17:30:08 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 17:30:08 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 17:30:08 INFO mapreduce.ImportJobBase: Beginning import of DC_CATA_DETAIL17/03/29 17:30:09 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 17:30:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 17:30:10 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 17:30:17 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 17:30:17 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_CATA_DETAIL`17/03/29 17:30:17 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 17:30:17 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 17:30:17 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 17:30:17 INFO mapreduce.JobSubmitter: number of splits:617/03/29 17:30:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_005017/03/29 17:30:18 INFO impl.YarnClientImpl: Submitted application application_1490408992134_005017/03/29 17:30:18 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0050/17/03/29 17:30:18 INFO mapreduce.Job: Running job: job_1490408992134_005017/03/29 17:30:27 INFO mapreduce.Job: Job job_1490408992134_0050 running in uber mode : false17/03/29 17:30:27 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 17:30:32 INFO mapreduce.Job:  map 17% reduce 0%17/03/29 17:30:33 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 17:30:34 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 17:30:36 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 17:30:37 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 17:30:40 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 17:30:41 INFO mapreduce.Job: Job job_1490408992134_0050 completed successfully17/03/29 17:30:42 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862458		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=797		HDFS: Number of bytes written=8698		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=22533		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=22533		Total vcore-seconds taken by all map tasks=22533		Total megabyte-seconds taken by all map tasks=23073792	Map-Reduce Framework		Map input records=33		Map output records=33		Input split bytes=797		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=347		CPU time spent (ms)=5700		Physical memory (bytes) snapshot=1059545088		Virtual memory (bytes) snapshot=16680640512		Total committed heap usage (bytes)=1077411840	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=869817/03/29 17:30:42 INFO mapreduce.ImportJobBase: Transferred 8.4941 KB in 31.2566 seconds (278.2769 bytes/sec)17/03/29 17:30:42 INFO mapreduce.ImportJobBase: Retrieved 33 records.17/03/29 17:30:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_CATA_DETAIL` AS t LIMIT 117/03/29 17:30:42 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 17:30:42 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 17:30:42 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.266 secondsLoading data to table default.dc_cata_detail_lpTable default.dc_cata_detail_lp stats: [numFiles=6, totalSize=8698]OKTime taken: 1.377 seconds
[17:31:18] INFO:    Now wait 5 seconds to begin next task ...
[17:31:23] INFO:    Connection channel disconnect
[17:31:23] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:31:23] INFO:    Connection channel established succesfully
[17:31:23] INFO:    Start to run command
[17:31:37] INFO:    Connection channel closed
[17:31:37] INFO:    Check if exec success or not ... 
[17:31:37] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5_lp'
[17:31:37] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 17:30:59 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[17:31:37] INFO:    Now wait 5 seconds to begin next task ...
[17:31:42] INFO:    Connection channel disconnect
[17:31:42] INFO:    Command is sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:31:42] INFO:    Connection channel established succesfully
[17:31:42] INFO:    Start to run command
[17:31:42] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[17:32:40] INFO:    Connection channel closed
[17:32:40] INFO:    Check if exec success or not ... 
[17:32:40] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import -m 4 --table DC_DATA_SOURCE --hive-table DC_DATA_SOURCE_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/hldc_h5 --username hldc_h5 --password hldc_h5
[17:32:40] INFO:    Error message: 17/03/29 17:31:17 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 17:31:17 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 17:31:17 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 17:31:17 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 17:31:17 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 17:31:17 INFO tool.CodeGenTool: Beginning code generation17/03/29 17:31:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:31:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:31:18 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/14f889f2341ed32cb3e396204f53b27b/DC_DATA_SOURCE.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/29 17:31:22 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/14f889f2341ed32cb3e396204f53b27b/DC_DATA_SOURCE.jar17/03/29 17:31:22 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/29 17:31:22 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/29 17:31:22 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/29 17:31:22 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/29 17:31:22 INFO mapreduce.ImportJobBase: Beginning import of DC_DATA_SOURCE17/03/29 17:31:23 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/29 17:31:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/29 17:31:25 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/29 17:31:33 INFO db.DBInputFormat: Using read commited transaction isolation17/03/29 17:31:33 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`ID`), MAX(`ID`) FROM `DC_DATA_SOURCE`17/03/29 17:31:33 WARN db.TextSplitter: Generating splits for a textual index column.17/03/29 17:31:33 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.17/03/29 17:31:33 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.17/03/29 17:31:33 INFO mapreduce.JobSubmitter: number of splits:617/03/29 17:31:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_005117/03/29 17:31:35 INFO impl.YarnClientImpl: Submitted application application_1490408992134_005117/03/29 17:31:35 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0051/17/03/29 17:31:35 INFO mapreduce.Job: Running job: job_1490408992134_005117/03/29 17:31:43 INFO mapreduce.Job: Job job_1490408992134_0051 running in uber mode : false17/03/29 17:31:43 INFO mapreduce.Job:  map 0% reduce 0%17/03/29 17:31:49 INFO mapreduce.Job:  map 33% reduce 0%17/03/29 17:31:52 INFO mapreduce.Job:  map 50% reduce 0%17/03/29 17:31:53 INFO mapreduce.Job:  map 67% reduce 0%17/03/29 17:31:55 INFO mapreduce.Job:  map 83% reduce 0%17/03/29 17:32:03 INFO mapreduce.Job:  map 100% reduce 0%17/03/29 17:32:03 INFO mapreduce.Job: Job job_1490408992134_0051 completed successfully17/03/29 17:32:03 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=862926		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=801		HDFS: Number of bytes written=3306		HDFS: Number of read operations=24		HDFS: Number of large read operations=0		HDFS: Number of write operations=12	Job Counters 		Launched map tasks=6		Other local map tasks=6		Total time spent by all maps in occupied slots (ms)=31143		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=31143		Total vcore-seconds taken by all map tasks=31143		Total megabyte-seconds taken by all map tasks=31890432	Map-Reduce Framework		Map input records=12		Map output records=12		Input split bytes=801		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=358		CPU time spent (ms)=5640		Physical memory (bytes) snapshot=1039347712		Virtual memory (bytes) snapshot=16675401728		Total committed heap usage (bytes)=1027604480	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=330617/03/29 17:32:03 INFO mapreduce.ImportJobBase: Transferred 3.2285 KB in 38.7348 seconds (85.3496 bytes/sec)17/03/29 17:32:03 INFO mapreduce.ImportJobBase: Retrieved 12 records.17/03/29 17:32:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DC_DATA_SOURCE` AS t LIMIT 117/03/29 17:32:03 WARN hive.TableDefWriter: Column CREATE_DATE had to be cast to a less precise type in Hive17/03/29 17:32:03 WARN hive.TableDefWriter: Column UPDATE_DATE had to be cast to a less precise type in Hive17/03/29 17:32:04 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.878 secondsLoading data to table default.dc_data_source_lpTable default.dc_data_source_lp stats: [numFiles=6, totalSize=3306]OKTime taken: 1.823 seconds
[17:32:40] INFO:    Now wait 5 seconds to begin next task ...
[17:32:45] INFO:    Connection channel disconnect
[17:32:45] INFO:    SSH connection shutdown
[17:32:45] INFO:    Session initialized and associated with user credential 123456
[17:32:45] INFO:    SSHExec initialized successfully
[17:32:45] INFO:    SSHExec trying to connect root@172.16.110.200
[17:32:46] INFO:    SSH connection established
[17:32:46] INFO:    SSH connection shutdown

=============== [2017/03/29 20:43:07, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:43:07] INFO:    SSHExec initializing ...
[20:43:07] INFO:    Session initialized and associated with user credential 123456
[20:43:07] INFO:    SSHExec initialized successfully
[20:43:07] INFO:    SSHExec trying to connect root@172.16.110.200
[20:43:07] INFO:    SSH connection established
[20:43:07] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[20:43:07] INFO:    Connection channel established succesfully
[20:43:07] INFO:    Start to run command
[20:43:25] INFO:    Connection channel closed
[20:43:25] INFO:    Check if exec success or not ... 
[20:43:25] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[20:43:25] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 20:42:28 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[20:43:25] INFO:    Now wait 5 seconds to begin next task ...
[20:43:30] INFO:    Connection channel disconnect
[20:43:30] INFO:    Command is sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[20:43:30] INFO:    Connection channel established succesfully
[20:43:30] INFO:    Start to run command
[20:43:30] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[20:43:35] INFO:    Connection channel closed
[20:43:35] INFO:    Check if exec success or not ... 
[20:43:35] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[20:43:35] INFO:    Error message: 17/03/29 20:42:50 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/29 20:42:50 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/29 20:42:50 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/29 20:42:50 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/29 20:42:50 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/29 20:42:50 INFO tool.CodeGenTool: Beginning code generation17/03/29 20:42:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ALL-TABLES` AS t LIMIT 117/03/29 20:42:51 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/29 20:42:51 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[20:43:35] INFO:    Now wait 5 seconds to begin next task ...
[20:43:40] INFO:    Connection channel disconnect
[20:43:40] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[20:43:40] INFO:    Connection channel established succesfully
[20:43:40] INFO:    Start to run command
[20:43:53] INFO:    Connection channel closed
[20:43:53] INFO:    Check if exec success or not ... 
[20:43:53] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[20:43:53] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/29 20:43:02 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.dropTableStatement(HiveParser.java:6929)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2572)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:26 cannot recognize input near 'sudo' '-' 'u' in table name
[20:43:53] INFO:    Now wait 5 seconds to begin next task ...
[20:43:58] INFO:    Connection channel disconnect
[20:43:58] INFO:    SSH connection shutdown

=============== [2017/03/30 10:03:37, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:03:37] INFO:    SSHExec initializing ...
[10:03:37] INFO:    Session initialized and associated with user credential 123456
[10:03:37] INFO:    SSHExec initialized successfully
[10:03:37] INFO:    SSHExec trying to connect root@172.16.110.200
[10:03:43] INFO:    SSH connection established
[10:03:43] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[10:03:43] INFO:    Connection channel established succesfully
[10:03:43] INFO:    Start to run command
[10:03:58] INFO:    Connection channel closed
[10:03:58] INFO:    Check if exec success or not ... 
[10:03:58] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[10:03:58] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:04:26 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[10:03:58] INFO:    Now wait 5 seconds to begin next task ...
[10:04:03] INFO:    Connection channel disconnect
[10:04:03] INFO:    Command is sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:04:03] INFO:    Connection channel established succesfully
[10:04:03] INFO:    Start to run command
[10:04:03] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[10:04:03] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:04:11] INFO:    Connection channel closed
[10:04:11] INFO:    Check if exec success or not ... 
[10:04:11] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:04:11] INFO:    Error message: 17/03/30 10:04:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:04:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:04:47 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:04:47 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:04:47 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:04:47 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:04:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ALL-TABLES` AS t LIMIT 117/03/30 10:04:49 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/30 10:04:49 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[10:04:11] INFO:    Now wait 5 seconds to begin next task ...
[10:04:16] INFO:    Connection channel disconnect
[10:04:16] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[10:04:16] INFO:    Connection channel established succesfully
[10:04:16] INFO:    Start to run command
[10:04:31] INFO:    Connection channel closed
[10:04:31] INFO:    Check if exec success or not ... 
[10:04:31] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES_lp --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5_lp'
[10:04:31] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:05:01 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.dropTableStatement(HiveParser.java:6929)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2572)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:26 cannot recognize input near 'sudo' '-' 'u' in table name
[10:04:31] INFO:    Now wait 5 seconds to begin next task ...
[10:04:36] INFO:    Connection channel disconnect
[10:04:36] INFO:    SSH connection shutdown

=============== [2017/03/30 10:12:20, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:12:20] INFO:    SSHExec initializing ...
[10:12:20] INFO:    Session initialized and associated with user credential 123456
[10:12:20] INFO:    SSHExec initialized successfully
[10:12:20] INFO:    SSHExec trying to connect root@172.16.110.200
[10:12:20] INFO:    SSH connection established
[10:12:20] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5'
[10:12:20] INFO:    Connection channel established succesfully
[10:12:20] INFO:    Start to run command
[10:12:34] INFO:    Connection channel closed
[10:12:34] INFO:    Check if exec success or not ... 
[10:12:34] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5'
[10:12:34] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:13:12 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:33 missing EOF at '-' near 'sudo'
[10:12:34] INFO:    Now wait 5 seconds to begin next task ...
[10:12:39] INFO:    Connection channel disconnect
[10:12:39] INFO:    Command is sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:12:39] INFO:    Connection channel established succesfully
[10:12:39] INFO:    Start to run command
[10:12:40] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[10:12:40] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:12:45] INFO:    Connection channel closed
[10:12:45] INFO:    Check if exec success or not ... 
[10:12:45] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:12:45] INFO:    Error message: 17/03/30 10:13:30 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:13:30 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:13:30 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:13:30 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:13:30 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:13:30 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:13:31 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ALL-TABLES` AS t LIMIT 117/03/30 10:13:31 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/30 10:13:31 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[10:12:45] INFO:    Now wait 5 seconds to begin next task ...
[10:12:50] INFO:    Connection channel disconnect
[10:12:50] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5 RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5'
[10:12:50] INFO:    Connection channel established succesfully
[10:12:50] INFO:    Start to run command
[10:13:04] INFO:    Connection channel closed
[10:13:04] INFO:    Check if exec success or not ... 
[10:13:04] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5; ALTER TABLE hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5 RENAME TO sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5'
[10:13:04] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:13:42 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesNoViableAltException(293@[184:1: tableName : (db= identifier DOT tab= identifier -> ^( TOK_TABNAME $db $tab) |tab= identifier -> ^( TOK_TABNAME $tab) );])	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)	at org.antlr.runtime.DFA.predict(DFA.java:116)	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.tableName(HiveParser_FromClauseParser.java:4692)	at org.apache.hadoop.hive.ql.parse.HiveParser.tableName(HiveParser.java:44905)	at org.apache.hadoop.hive.ql.parse.HiveParser.dropTableStatement(HiveParser.java:6929)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2572)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:26 cannot recognize input near 'sudo' '-' 'u' in table name
[10:13:04] INFO:    Now wait 5 seconds to begin next task ...
[10:13:09] INFO:    Connection channel disconnect
[10:13:09] INFO:    SSH connection shutdown

=============== [2017/03/30 10:20:33, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:20:33] INFO:    SSHExec initializing ...
[10:20:33] INFO:    Session initialized and associated with user credential 123456
[10:20:33] INFO:    SSHExec initialized successfully
[10:20:33] INFO:    SSHExec trying to connect root@172.16.110.200
[10:20:33] INFO:    SSH connection established
[10:21:09] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5;ALTER TABLE sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5 RENAME TO hl_bak.sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5'
[10:21:09] INFO:    Connection channel established succesfully
[10:21:09] INFO:    Start to run command

=============== [2017/03/30 10:26:27, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:26:27] INFO:    SSHExec initializing ...
[10:26:27] INFO:    Session initialized and associated with user credential 123456
[10:26:27] INFO:    SSHExec initialized successfully
[10:26:27] INFO:    SSHExec trying to connect root@172.16.110.200
[10:26:28] INFO:    SSH connection established
[10:26:28] INFO:    Command is sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:26:28] INFO:    Connection channel established succesfully
[10:26:28] INFO:    Start to run command
[10:26:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:26:33] INFO:    Connection channel closed
[10:26:33] INFO:    Check if exec success or not ... 
[10:26:33] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:26:33] INFO:    Error message: 17/03/30 10:26:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:26:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:26:51 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:26:51 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:26:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:26:52 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ALL-TABLES` AS t LIMIT 117/03/30 10:26:53 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/30 10:26:53 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[10:26:33] INFO:    Now wait 5 seconds to begin next task ...
[10:26:39] INFO:    Connection channel disconnect
[10:26:39] INFO:    SSH connection shutdown

=============== [2017/03/30 10:33:43, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:33:43] INFO:    SSHExec initializing ...
[10:33:43] INFO:    Session initialized and associated with user credential 123456
[10:33:43] INFO:    SSHExec initialized successfully
[10:33:43] INFO:    SSHExec trying to connect root@172.16.110.200
[10:33:43] INFO:    SSH connection established
[10:33:43] INFO:    Command is sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:33:43] INFO:    Connection channel established succesfully
[10:33:43] INFO:    Start to run command
[10:33:44] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:33:49] INFO:    Connection channel closed
[10:33:49] INFO:    Check if exec success or not ... 
[10:33:49] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table ALL-TABLES -m 4 --hive-table ALL-TABLES --hive-import  --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:33:49] INFO:    Error message: 17/03/30 10:33:52 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:33:52 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:33:52 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:33:52 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:33:53 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:33:53 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:33:54 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ALL-TABLES` AS t LIMIT 117/03/30 10:33:54 ERROR manager.SqlManager: Error executing statement: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't existcom.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'test_lp.all-tables' doesn't exist	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)	at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)	at com.mysql.jdbc.Util.getInstance(Util.java:387)	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:942)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966)	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902)	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526)	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673)	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2549)	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1861)	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1962)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:777)	at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:786)	at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:289)	at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:260)	at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:246)	at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:327)	at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1861)	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1661)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)17/03/30 10:33:54 ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter	at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1667)	at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)	at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:488)	at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:615)	at org.apache.sqoop.Sqoop.run(Sqoop.java:143)	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)	at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)	at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)	at org.apache.sqoop.Sqoop.main(Sqoop.java:236)
[10:33:49] INFO:    Now wait 5 seconds to begin next task ...
[10:33:54] INFO:    Connection channel disconnect
[10:33:54] INFO:    SSH connection shutdown

=============== [2017/03/30 10:35:47, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:35:47] INFO:    SSHExec initializing ...
[10:35:47] INFO:    Session initialized and associated with user credential 123456
[10:35:47] INFO:    SSHExec initialized successfully
[10:35:47] INFO:    SSHExec trying to connect root@172.16.110.200
[10:35:48] INFO:    SSH connection established
[10:35:48] INFO:    Command is sudo -u hdfs sqoop-import-all-tables -m 4 --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:35:48] INFO:    Connection channel established succesfully
[10:35:48] INFO:    Start to run command
[10:35:48] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[10:35:48] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:37:11] INFO:    Connection channel closed
[10:37:11] INFO:    Check if exec success or not ... 
[10:37:11] INFO:    Execution failed while executing command: sudo -u hdfs sqoop-import-all-tables -m 4 --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:37:11] INFO:    Error message: 17/03/30 10:35:55 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:35:55 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:35:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:35:57 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:35:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept` AS t LIMIT 117/03/30 10:35:57 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept` AS t LIMIT 117/03/30 10:35:57 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/dept.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:36:00 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/dept.jar17/03/30 10:36:00 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:36:00 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:36:00 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:36:00 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:36:01 INFO mapreduce.ImportJobBase: Beginning import of dept17/03/30 10:36:01 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:36:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:36:03 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:36:10 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:36:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `dept`17/03/30 10:36:10 INFO mapreduce.JobSubmitter: number of splits:117/03/30 10:36:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_005717/03/30 10:36:12 INFO impl.YarnClientImpl: Submitted application application_1490408992134_005717/03/30 10:36:12 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0057/17/03/30 10:36:12 INFO mapreduce.Job: Running job: job_1490408992134_005717/03/30 10:36:23 INFO mapreduce.Job: Job job_1490408992134_0057 running in uber mode : false17/03/30 10:36:23 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:36:32 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:36:32 INFO mapreduce.Job: Job job_1490408992134_0057 completed successfully17/03/30 10:36:33 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143273		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=105		HDFS: Number of bytes written=0		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=5549		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=5549		Total vcore-seconds taken by all map tasks=5549		Total megabyte-seconds taken by all map tasks=5682176	Map-Reduce Framework		Map input records=0		Map output records=0		Input split bytes=105		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=43		CPU time spent (ms)=590		Physical memory (bytes) snapshot=173522944		Virtual memory (bytes) snapshot=2780372992		Total committed heap usage (bytes)=190316544	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=017/03/30 10:36:33 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 30.1421 seconds (0 bytes/sec)17/03/30 10:36:33 INFO mapreduce.ImportJobBase: Retrieved 0 records.17/03/30 10:36:33 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:36:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `table` AS t LIMIT 117/03/30 10:36:33 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/table.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:36:34 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/table.jar17/03/30 10:36:34 INFO mapreduce.ImportJobBase: Beginning import of table17/03/30 10:36:34 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:36:39 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:36:39 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `table`17/03/30 10:36:39 INFO mapreduce.JobSubmitter: number of splits:117/03/30 10:36:40 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_005817/03/30 10:36:40 INFO impl.YarnClientImpl: Submitted application application_1490408992134_005817/03/30 10:36:40 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0058/17/03/30 10:36:40 INFO mapreduce.Job: Running job: job_1490408992134_005817/03/30 10:36:49 INFO mapreduce.Job: Job job_1490408992134_0058 running in uber mode : false17/03/30 10:36:49 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:36:55 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:36:55 INFO mapreduce.Job: Job job_1490408992134_0058 completed successfully17/03/30 10:36:56 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143277		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=105		HDFS: Number of bytes written=0		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=3112		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=3112		Total vcore-seconds taken by all map tasks=3112		Total megabyte-seconds taken by all map tasks=3186688	Map-Reduce Framework		Map input records=0		Map output records=0		Input split bytes=105		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=56		CPU time spent (ms)=590		Physical memory (bytes) snapshot=172195840		Virtual memory (bytes) snapshot=2780123136		Total committed heap usage (bytes)=167772160	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=017/03/30 10:36:56 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 21.8296 seconds (0 bytes/sec)17/03/30 10:36:56 INFO mapreduce.ImportJobBase: Retrieved 0 records.17/03/30 10:36:56 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:36:56 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `user` AS t LIMIT 117/03/30 10:36:56 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/user.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:36:56 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/d663366a2daf94b48588a9644910c981/user.jar17/03/30 10:36:56 INFO mapreduce.ImportJobBase: Beginning import of user17/03/30 10:36:56 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:37:02 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:37:02 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `user`17/03/30 10:37:02 INFO mapreduce.JobSubmitter: number of splits:117/03/30 10:37:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_005917/03/30 10:37:03 INFO impl.YarnClientImpl: Submitted application application_1490408992134_005917/03/30 10:37:03 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0059/17/03/30 10:37:03 INFO mapreduce.Job: Running job: job_1490408992134_005917/03/30 10:37:11 INFO mapreduce.Job: Job job_1490408992134_0059 running in uber mode : false17/03/30 10:37:11 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:37:18 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:37:18 INFO mapreduce.Job: Job job_1490408992134_0059 completed successfully17/03/30 10:37:18 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=143273		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=105		HDFS: Number of bytes written=0		HDFS: Number of read operations=4		HDFS: Number of large read operations=0		HDFS: Number of write operations=2	Job Counters 		Launched map tasks=1		Other local map tasks=1		Total time spent by all maps in occupied slots (ms)=4313		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=4313		Total vcore-seconds taken by all map tasks=4313		Total megabyte-seconds taken by all map tasks=4416512	Map-Reduce Framework		Map input records=0		Map output records=0		Input split bytes=105		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=58		CPU time spent (ms)=600		Physical memory (bytes) snapshot=172220416		Virtual memory (bytes) snapshot=2779590656		Total committed heap usage (bytes)=167247872	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=017/03/30 10:37:18 INFO mapreduce.ImportJobBase: Transferred 0 bytes in 22.0712 seconds (0 bytes/sec)17/03/30 10:37:18 INFO mapreduce.ImportJobBase: Retrieved 0 records.
[10:37:11] INFO:    Now wait 5 seconds to begin next task ...
[10:37:16] INFO:    Connection channel disconnect
[10:37:16] INFO:    SSH connection shutdown

=============== [2017/03/30 10:39:59, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:39:59] INFO:    SSHExec initializing ...
[10:39:59] INFO:    Session initialized and associated with user credential 123456
[10:39:59] INFO:    SSHExec initialized successfully
[10:39:59] INFO:    SSHExec trying to connect root@172.16.110.200
[10:40:00] INFO:    SSH connection established
[10:40:00] INFO:    Command is sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:40:00] INFO:    Connection channel established succesfully
[10:40:00] INFO:    Start to run command
[10:40:00] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[10:40:00] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:40:12] INFO:    Connection channel closed
[10:40:12] INFO:    Check if exec success or not ... 
[10:40:12] INFO:    Execution failed while executing command: sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:40:12] INFO:    Error message: 17/03/30 10:40:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:40:14 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:40:14 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:40:14 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:40:14 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:40:15 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:40:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept` AS t LIMIT 117/03/30 10:40:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept` AS t LIMIT 117/03/30 10:40:15 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/e7bf70f4a927889753d2491747bbfb82/dept.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:40:18 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/e7bf70f4a927889753d2491747bbfb82/dept.jar17/03/30 10:40:18 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:40:18 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:40:18 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:40:18 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:40:18 INFO mapreduce.ImportJobBase: Beginning import of dept17/03/30 10:40:19 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:40:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:40:21 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:40:21 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:SIMPLE) cause:org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.hadoop.demo:8020/user/hdfs/dept already exists17/03/30 10:40:21 ERROR tool.ImportAllTablesTool: Encountered IOException running import job: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://name.hadoop.demo:8020/user/hdfs/dept already exists
[10:40:12] INFO:    Now wait 5 seconds to begin next task ...
[10:40:17] INFO:    Connection channel disconnect
[10:40:17] INFO:    SSH connection shutdown

=============== [2017/03/30 10:46:24, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:46:24] INFO:    SSHExec initializing ...
[10:46:24] INFO:    Session initialized and associated with user credential 123456
[10:46:24] INFO:    SSHExec initialized successfully
[10:46:24] INFO:    SSHExec trying to connect root@172.16.110.200
[10:46:25] INFO:    SSH connection established
[10:46:25] INFO:    Command is sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:46:25] INFO:    Connection channel established succesfully
[10:46:25] INFO:    Start to run command
[10:46:25] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:48:00] INFO:    Connection channel closed
[10:48:00] INFO:    Check if exec success or not ... 
[10:48:00] INFO:    Execution failed while executing command: sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:48:00] INFO:    Error message: 17/03/30 10:46:14 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:46:14 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:46:14 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:46:14 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:46:14 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:46:15 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:46:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:46:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:46:15 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/72d5eaae6a9bf763863143d46c453cb3/dept_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:46:19 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/72d5eaae6a9bf763863143d46c453cb3/dept_t.jar17/03/30 10:46:19 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:46:19 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:46:19 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:46:19 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:46:19 INFO mapreduce.ImportJobBase: Beginning import of dept_t17/03/30 10:46:20 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:46:22 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:46:22 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:46:28 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:46:28 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `dept_t`17/03/30 10:46:28 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 10:46:28 INFO mapreduce.JobSubmitter: number of splits:217/03/30 10:46:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_006317/03/30 10:46:30 INFO impl.YarnClientImpl: Submitted application application_1490408992134_006317/03/30 10:46:30 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0063/17/03/30 10:46:30 INFO mapreduce.Job: Running job: job_1490408992134_006317/03/30 10:46:38 INFO mapreduce.Job: Job job_1490408992134_0063 running in uber mode : false17/03/30 10:46:38 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:46:45 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:46:46 INFO mapreduce.Job: Job job_1490408992134_0063 completed successfully17/03/30 10:46:46 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=286834		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=7184		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=7184		Total vcore-seconds taken by all map tasks=7184		Total megabyte-seconds taken by all map tasks=7356416	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=123		CPU time spent (ms)=1780		Physical memory (bytes) snapshot=361676800		Virtual memory (bytes) snapshot=5558607872		Total committed heap usage (bytes)=356515840	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 10:46:46 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 24.6401 seconds (0.4058 bytes/sec)17/03/30 10:46:46 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 10:46:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:46:47 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.495 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=3, totalSize=10]OKTime taken: 1.75 secondsNote: /tmp/sqoop-hdfs/compile/72d5eaae6a9bf763863143d46c453cb3/table_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.074 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=3, totalSize=8]OKTime taken: 0.86 secondsNote: /tmp/sqoop-hdfs/compile/72d5eaae6a9bf763863143d46c453cb3/user_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.043 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=5, totalSize=12]OKTime taken: 0.945 seconds
[10:48:00] INFO:    Now wait 5 seconds to begin next task ...
[10:48:06] INFO:    Connection channel disconnect
[10:48:06] INFO:    SSH connection shutdown

=============== [2017/03/30 10:48:47, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:48:47] INFO:    SSHExec initializing ...
[10:48:47] INFO:    Session initialized and associated with user credential 123456
[10:48:47] INFO:    SSHExec initialized successfully
[10:48:47] INFO:    SSHExec trying to connect root@172.16.110.200
[10:48:47] INFO:    SSH connection established
[10:48:47] INFO:    Command is sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:48:47] INFO:    Connection channel established succesfully
[10:48:47] INFO:    Start to run command
[10:48:47] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:50:26] INFO:    Connection channel closed
[10:50:26] INFO:    Check if exec success or not ... 
[10:50:26] INFO:    Execution failed while executing command: sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:50:26] INFO:    Error message: 17/03/30 10:48:41 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:48:41 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:48:41 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:48:41 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:48:41 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:48:42 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:48:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:48:42 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:48:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/ee06af9783fc02a027855294ce1de21d/dept_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:48:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/ee06af9783fc02a027855294ce1de21d/dept_t.jar17/03/30 10:48:45 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:48:45 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:48:45 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:48:45 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:48:45 INFO mapreduce.ImportJobBase: Beginning import of dept_t17/03/30 10:48:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:48:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:48:48 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:48:56 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:48:56 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `dept_t`17/03/30 10:48:56 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 10:48:56 INFO mapreduce.JobSubmitter: number of splits:217/03/30 10:48:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_006617/03/30 10:48:58 INFO impl.YarnClientImpl: Submitted application application_1490408992134_006617/03/30 10:48:58 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0066/17/03/30 10:48:58 INFO mapreduce.Job: Running job: job_1490408992134_006617/03/30 10:49:07 INFO mapreduce.Job: Job job_1490408992134_0066 running in uber mode : false17/03/30 10:49:07 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:49:14 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 10:49:15 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:49:15 INFO mapreduce.Job: Job job_1490408992134_0066 completed successfully17/03/30 10:49:16 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=286834		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8064		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8064		Total vcore-seconds taken by all map tasks=8064		Total megabyte-seconds taken by all map tasks=8257536	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=104		CPU time spent (ms)=1680		Physical memory (bytes) snapshot=362283008		Virtual memory (bytes) snapshot=5558947840		Total committed heap usage (bytes)=354942976	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 10:49:16 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 28.1238 seconds (0.3556 bytes/sec)17/03/30 10:49:16 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 10:49:16 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 10:49:16 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.278 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=5, totalSize=20]OKTime taken: 1.791 secondsNote: /tmp/sqoop-hdfs/compile/ee06af9783fc02a027855294ce1de21d/table_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.047 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=5, totalSize=16]OKTime taken: 0.849 secondsNote: /tmp/sqoop-hdfs/compile/ee06af9783fc02a027855294ce1de21d/user_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.057 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=9, totalSize=24]OKTime taken: 1.619 seconds
[10:50:27] INFO:    Now wait 5 seconds to begin next task ...
[10:50:32] INFO:    Connection channel disconnect
[10:50:32] INFO:    SSH connection shutdown

=============== [2017/03/30 10:52:07, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[10:52:07] INFO:    SSHExec initializing ...
[10:52:07] INFO:    Session initialized and associated with user credential 123456
[10:52:07] INFO:    SSHExec initialized successfully
[10:52:07] INFO:    SSHExec trying to connect root@172.16.110.200
[10:52:07] INFO:    SSH connection established
[10:52:07] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dept_t,user_t;ALTER TABLE dept_t,user_t RENAME TO hl_bak.dept_t,user_t'
[10:52:07] INFO:    Connection channel established succesfully
[10:52:07] INFO:    Start to run command
[10:52:21] INFO:    Connection channel closed
[10:52:21] INFO:    Check if exec success or not ... 
[10:52:21] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dept_t,user_t;ALTER TABLE dept_t,user_t RENAME TO hl_bak.dept_t,user_t'
[10:52:21] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:52:18 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:34 missing EOF at ',' near 'dept_t'
[10:52:21] INFO:    Now wait 5 seconds to begin next task ...
[10:52:26] INFO:    Connection channel disconnect
[10:52:26] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:52:26] INFO:    Connection channel established succesfully
[10:52:26] INFO:    Start to run command
[10:52:26] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:53:07] INFO:    Connection channel closed
[10:53:07] INFO:    Check if exec success or not ... 
[10:53:07] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:53:07] INFO:    Error message: 17/03/30 10:52:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:52:35 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:52:35 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:52:35 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:52:35 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:52:35 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:52:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 10:52:36 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 10:52:36 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/1a44fdfd8b9ec03b1fb7ac2548f68963/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:52:39 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/1a44fdfd8b9ec03b1fb7ac2548f68963/DEPT_T.jar17/03/30 10:52:39 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:52:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:52:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:52:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:52:40 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 10:52:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:52:42 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:52:42 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:52:49 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:52:49 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 10:52:49 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 10:52:49 INFO mapreduce.JobSubmitter: number of splits:217/03/30 10:52:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_006917/03/30 10:52:51 INFO impl.YarnClientImpl: Submitted application application_1490408992134_006917/03/30 10:52:51 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0069/17/03/30 10:52:51 INFO mapreduce.Job: Running job: job_1490408992134_006917/03/30 10:53:00 INFO mapreduce.Job: Job job_1490408992134_0069 running in uber mode : false17/03/30 10:53:00 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:53:06 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 10:53:07 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:53:07 INFO mapreduce.Job: Job job_1490408992134_0069 completed successfully17/03/30 10:53:08 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8614		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8614		Total vcore-seconds taken by all map tasks=8614		Total megabyte-seconds taken by all map tasks=8820736	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=121		CPU time spent (ms)=1540		Physical memory (bytes) snapshot=346611712		Virtual memory (bytes) snapshot=5560172544		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 10:53:08 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.9165 seconds (0.3859 bytes/sec)17/03/30 10:53:08 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 10:53:08 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 10:53:08 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.853 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=7, totalSize=30]OKTime taken: 1.35 seconds
[10:53:07] INFO:    Now wait 5 seconds to begin next task ...
[10:53:12] INFO:    Connection channel disconnect
[10:53:12] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dept_t,user_t;ALTER TABLE dept_t,user_t RENAME TO hl_bak.dept_t,user_t'
[10:53:12] INFO:    Connection channel established succesfully
[10:53:12] INFO:    Start to run command
[10:53:27] INFO:    Connection channel closed
[10:53:27] INFO:    Check if exec success or not ... 
[10:53:27] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.dept_t,user_t;ALTER TABLE dept_t,user_t RENAME TO hl_bak.dept_t,user_t'
[10:53:27] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 10:53:26 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:34 missing EOF at ',' near 'dept_t'
[10:53:27] INFO:    Now wait 5 seconds to begin next task ...
[10:53:32] INFO:    Connection channel disconnect
[10:53:32] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:53:32] INFO:    Connection channel established succesfully
[10:53:32] INFO:    Start to run command
[10:53:32] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[10:54:19] INFO:    Connection channel closed
[10:54:19] INFO:    Check if exec success or not ... 
[10:54:19] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[10:54:19] INFO:    Error message: 17/03/30 10:53:45 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 10:53:45 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 10:53:45 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 10:53:45 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 10:53:46 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 10:53:46 INFO tool.CodeGenTool: Beginning code generation17/03/30 10:53:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 10:53:47 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 10:53:47 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/567d6840c1bfe91744c25b7485a3b066/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 10:53:50 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/567d6840c1bfe91744c25b7485a3b066/USER_T.jar17/03/30 10:53:50 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 10:53:50 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 10:53:50 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 10:53:50 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 10:53:50 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 10:53:51 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 10:53:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 10:53:53 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 10:54:00 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 10:54:00 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 10:54:00 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 10:54:00 INFO mapreduce.JobSubmitter: number of splits:417/03/30 10:54:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007017/03/30 10:54:01 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007017/03/30 10:54:01 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0070/17/03/30 10:54:01 INFO mapreduce.Job: Running job: job_1490408992134_007017/03/30 10:54:10 INFO mapreduce.Job: Job job_1490408992134_0070 running in uber mode : false17/03/30 10:54:10 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 10:54:18 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 10:54:19 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 10:54:21 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 10:54:22 INFO mapreduce.Job: Job job_1490408992134_0070 completed successfully17/03/30 10:54:22 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=17291		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=17291		Total vcore-seconds taken by all map tasks=17291		Total megabyte-seconds taken by all map tasks=17705984	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=258		CPU time spent (ms)=3450		Physical memory (bytes) snapshot=697610240		Virtual memory (bytes) snapshot=11118940160		Total committed heap usage (bytes)=671612928	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 10:54:22 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 29.3735 seconds (0.4085 bytes/sec)17/03/30 10:54:22 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 10:54:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 10:54:22 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.365 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=13, totalSize=36]OKTime taken: 2.099 seconds
[10:54:19] INFO:    Now wait 5 seconds to begin next task ...
[10:54:24] INFO:    Connection channel disconnect
[10:54:24] INFO:    SSH connection shutdown
[10:54:24] INFO:    Session initialized and associated with user credential 123456
[10:54:24] INFO:    SSHExec initialized successfully
[10:54:24] INFO:    SSHExec trying to connect root@172.16.110.200
[10:54:24] INFO:    SSH connection established
[10:54:24] INFO:    SSH connection shutdown

=============== [2017/03/30 11:35:52, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[11:35:52] INFO:    SSHExec initializing ...
[11:35:52] INFO:    Session initialized and associated with user credential 123456
[11:35:52] INFO:    SSHExec initialized successfully
[11:35:52] INFO:    SSHExec trying to connect root@172.16.110.200
[11:36:00] INFO:    SSH connection established
[11:36:00] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.DEPT_T;ALTER TABLE DEPT_T RENAME TO hl_bak.DEPT_T'
[11:36:00] INFO:    Connection channel established succesfully
[11:36:00] INFO:    Start to run command
[11:36:17] INFO:    Connection channel closed
[11:36:17] INFO:    Check if exec success or not ... 
[11:36:17] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.DEPT_T;ALTER TABLE DEPT_T RENAME TO hl_bak.DEPT_T'
[11:36:17] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 11:35:50 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.826 secondsFAILED: SemanticException [Error 10001]: Table not found default.DEPT_T
[11:36:17] INFO:    Now wait 5 seconds to begin next task ...
[11:36:23] INFO:    Connection channel disconnect
[11:36:23] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[11:36:23] INFO:    Connection channel established succesfully
[11:36:23] INFO:    Start to run command
[11:36:23] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[11:37:10] INFO:    Connection channel closed
[11:37:10] INFO:    Check if exec success or not ... 
[11:37:10] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[11:37:10] INFO:    Error message: 17/03/30 11:36:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 11:36:09 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 11:36:09 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 11:36:09 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 11:36:10 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 11:36:10 INFO tool.CodeGenTool: Beginning code generation17/03/30 11:36:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 11:36:11 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 11:36:11 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/a4c88e47b357a1847f9cbbe987d35ada/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 11:36:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/a4c88e47b357a1847f9cbbe987d35ada/DEPT_T.jar17/03/30 11:36:14 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 11:36:14 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 11:36:14 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 11:36:14 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 11:36:14 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 11:36:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 11:36:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 11:36:16 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 11:36:23 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 11:36:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 11:36:23 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 11:36:23 INFO mapreduce.JobSubmitter: number of splits:217/03/30 11:36:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007217/03/30 11:36:24 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007217/03/30 11:36:24 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0072/17/03/30 11:36:24 INFO mapreduce.Job: Running job: job_1490408992134_007217/03/30 11:36:34 INFO mapreduce.Job: Job job_1490408992134_0072 running in uber mode : false17/03/30 11:36:34 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 11:36:40 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 11:36:41 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 11:36:42 INFO mapreduce.Job: Job job_1490408992134_0072 completed successfully17/03/30 11:36:43 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8085		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8085		Total vcore-seconds taken by all map tasks=8085		Total megabyte-seconds taken by all map tasks=8279040	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=100		CPU time spent (ms)=1570		Physical memory (bytes) snapshot=374210560		Virtual memory (bytes) snapshot=5560659968		Total committed heap usage (bytes)=379060224	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 11:36:43 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 26.6997 seconds (0.3745 bytes/sec)17/03/30 11:36:43 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 11:36:43 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 11:36:43 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.893 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=9, totalSize=40]OKTime taken: 1.681 seconds
[11:37:10] INFO:    Now wait 5 seconds to begin next task ...
[11:37:15] INFO:    Connection channel disconnect
[11:37:15] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.USER_T;ALTER TABLE USER_T RENAME TO hl_bak.USER_T'
[11:37:15] INFO:    Connection channel established succesfully
[11:37:15] INFO:    Start to run command
[11:37:31] INFO:    Connection channel closed
[11:37:31] INFO:    Check if exec success or not ... 
[11:37:31] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS hl_bak.USER_T;ALTER TABLE USER_T RENAME TO hl_bak.USER_T'
[11:37:31] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 11:37:01 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.225 secondsFAILED: SemanticException [Error 10001]: Table not found default.USER_T
[11:37:31] INFO:    Now wait 5 seconds to begin next task ...
[11:37:36] INFO:    Connection channel disconnect
[11:37:36] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[11:37:36] INFO:    Connection channel established succesfully
[11:37:36] INFO:    Start to run command
[11:37:36] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[11:37:36] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[11:38:26] INFO:    Connection channel closed
[11:38:26] INFO:    Check if exec success or not ... 
[11:38:26] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[11:38:26] INFO:    Error message: 17/03/30 11:37:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 11:37:19 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 11:37:19 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 11:37:19 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 11:37:19 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 11:37:19 INFO tool.CodeGenTool: Beginning code generation17/03/30 11:37:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 11:37:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 11:37:20 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/087ab0ced9cb0825501a00bdf247fddb/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 11:37:23 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/087ab0ced9cb0825501a00bdf247fddb/USER_T.jar17/03/30 11:37:24 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 11:37:24 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 11:37:24 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 11:37:24 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 11:37:24 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 11:37:24 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 11:37:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 11:37:26 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 11:37:35 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 11:37:35 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 11:37:35 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 11:37:35 INFO mapreduce.JobSubmitter: number of splits:417/03/30 11:37:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007317/03/30 11:37:37 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007317/03/30 11:37:37 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0073/17/03/30 11:37:37 INFO mapreduce.Job: Running job: job_1490408992134_007317/03/30 11:37:46 INFO mapreduce.Job: Job job_1490408992134_0073 running in uber mode : false17/03/30 11:37:46 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 11:37:54 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 11:37:55 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 11:37:57 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 11:37:58 INFO mapreduce.Job: Job job_1490408992134_0073 completed successfully17/03/30 11:37:58 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=17517		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=17517		Total vcore-seconds taken by all map tasks=17517		Total megabyte-seconds taken by all map tasks=17937408	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=271		CPU time spent (ms)=3870		Physical memory (bytes) snapshot=705773568		Virtual memory (bytes) snapshot=11117985792		Total committed heap usage (bytes)=690487296	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 11:37:59 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 32.6808 seconds (0.3672 bytes/sec)17/03/30 11:37:59 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 11:37:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 11:37:59 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.785 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=17, totalSize=48]OKTime taken: 1.751 seconds
[11:38:26] INFO:    Now wait 5 seconds to begin next task ...
[11:38:32] INFO:    Connection channel disconnect
[11:38:32] INFO:    SSH connection shutdown
[11:38:32] INFO:    Session initialized and associated with user credential 123456
[11:38:32] INFO:    SSHExec initialized successfully
[11:38:32] INFO:    SSHExec trying to connect root@172.16.110.200
[11:38:32] INFO:    SSH connection established
[11:38:32] INFO:    SSH connection shutdown

=============== [2017/03/30 14:16:38, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:16:38] INFO:    SSHExec initializing ...
[14:16:38] INFO:    Session initialized and associated with user credential 123456
[14:16:38] INFO:    SSHExec initialized successfully
[14:16:38] INFO:    SSHExec trying to connect root@172.16.110.200
[14:16:42] INFO:    SSH connection established
[14:16:42] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T_bak;ALTER TABLE DEPT_T RENAME TO DEPT_T_bak'
[14:16:42] INFO:    Connection channel established succesfully
[14:16:42] INFO:    Start to run command
[14:16:56] INFO:    Connection channel closed
[14:16:56] INFO:    Check if exec success or not ... 
[14:16:56] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T_bak;ALTER TABLE DEPT_T RENAME TO DEPT_T_bak'
[14:16:56] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 14:16:12 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.386 secondsFAILED: SemanticException [Error 10001]: Table not found default.DEPT_T
[14:16:56] INFO:    Now wait 5 seconds to begin next task ...
[14:17:01] INFO:    Connection channel disconnect
[14:17:01] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:17:01] INFO:    Connection channel established succesfully
[14:17:02] INFO:    Start to run command
[14:17:02] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:17:46] INFO:    Connection channel closed
[14:17:46] INFO:    Check if exec success or not ... 
[14:17:46] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:17:46] INFO:    Error message: 17/03/30 14:16:31 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:16:31 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:16:31 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:16:31 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:16:31 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:16:31 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:16:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:16:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:16:32 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/fbacb587028b849a5b37951afa55a850/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:16:36 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/fbacb587028b849a5b37951afa55a850/DEPT_T.jar17/03/30 14:16:36 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:16:36 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:16:36 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:16:36 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:16:36 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:16:37 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:16:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:16:38 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:16:46 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:16:46 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:16:46 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:16:46 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:16:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007417/03/30 14:16:47 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007417/03/30 14:16:47 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0074/17/03/30 14:16:47 INFO mapreduce.Job: Running job: job_1490408992134_007417/03/30 14:16:56 INFO mapreduce.Job: Job job_1490408992134_0074 running in uber mode : false17/03/30 14:16:56 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:17:02 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:17:04 INFO mapreduce.Job: Job job_1490408992134_0074 completed successfully17/03/30 14:17:04 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=7342		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=7342		Total vcore-seconds taken by all map tasks=7342		Total megabyte-seconds taken by all map tasks=7518208	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=106		CPU time spent (ms)=1630		Physical memory (bytes) snapshot=377655296		Virtual memory (bytes) snapshot=5560778752		Total committed heap usage (bytes)=376963072	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:17:04 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.6859 seconds (0.3893 bytes/sec)17/03/30 14:17:04 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:17:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:17:04 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.952 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.783 seconds
[14:17:46] INFO:    Now wait 5 seconds to begin next task ...
[14:17:52] INFO:    Connection channel disconnect
[14:17:52] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T_bak;ALTER TABLE USER_T RENAME TO USER_T_bak'
[14:17:52] INFO:    Connection channel established succesfully
[14:17:52] INFO:    Start to run command
[14:18:09] INFO:    Connection channel closed
[14:18:09] INFO:    Check if exec success or not ... 
[14:18:09] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T_bak;ALTER TABLE USER_T RENAME TO USER_T_bak'
[14:18:09] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 14:17:23 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.043 secondsFAILED: SemanticException [Error 10001]: Table not found default.USER_T
[14:18:09] INFO:    Now wait 5 seconds to begin next task ...
[14:18:14] INFO:    Connection channel disconnect
[14:18:14] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:18:14] INFO:    Connection channel established succesfully
[14:18:14] INFO:    Start to run command
[14:18:14] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:19:04] INFO:    Connection channel closed
[14:19:04] INFO:    Check if exec success or not ... 
[14:19:04] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:19:04] INFO:    Error message: 17/03/30 14:17:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:17:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:17:44 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:17:44 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:17:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:17:44 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:17:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:17:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:17:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/be499972573cc49a46750d4ab367374f/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:17:48 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/be499972573cc49a46750d4ab367374f/USER_T.jar17/03/30 14:17:48 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:17:48 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:17:48 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:17:48 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:17:48 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 14:17:49 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:17:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:17:51 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:17:58 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:17:58 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 14:17:58 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 14:17:58 INFO mapreduce.JobSubmitter: number of splits:417/03/30 14:17:59 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007517/03/30 14:17:59 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007517/03/30 14:17:59 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0075/17/03/30 14:17:59 INFO mapreduce.Job: Running job: job_1490408992134_007517/03/30 14:18:10 INFO mapreduce.Job: Job job_1490408992134_0075 running in uber mode : false17/03/30 14:18:10 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:18:16 INFO mapreduce.Job:  map 25% reduce 0%17/03/30 14:18:17 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:18:19 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 14:18:20 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:18:21 INFO mapreduce.Job: Job job_1490408992134_0075 completed successfully17/03/30 14:18:22 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=15851		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=15851		Total vcore-seconds taken by all map tasks=15851		Total megabyte-seconds taken by all map tasks=16231424	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=252		CPU time spent (ms)=3470		Physical memory (bytes) snapshot=692002816		Virtual memory (bytes) snapshot=11113816064		Total committed heap usage (bytes)=670040064	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 14:18:22 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 31.1072 seconds (0.3858 bytes/sec)17/03/30 14:18:22 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 14:18:22 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:18:22 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.378 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=4, totalSize=12]OKTime taken: 0.909 seconds
[14:19:04] INFO:    Now wait 5 seconds to begin next task ...
[14:19:10] INFO:    Connection channel disconnect
[14:19:10] INFO:    SSH connection shutdown
[14:19:10] INFO:    Session initialized and associated with user credential 123456
[14:19:10] INFO:    SSHExec initialized successfully
[14:19:10] INFO:    SSHExec trying to connect root@172.16.110.200
[14:19:10] INFO:    SSH connection established
[14:19:10] INFO:    SSH connection shutdown

=============== [2017/03/30 14:21:09, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:21:09] INFO:    SSHExec initializing ...
[14:21:09] INFO:    Session initialized and associated with user credential 123456
[14:21:09] INFO:    SSHExec initialized successfully
[14:21:09] INFO:    SSHExec trying to connect root@172.16.110.200
[14:21:10] INFO:    SSH connection established
[14:21:10] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T_BAK;ALTER TABLE DEPT_T RENAME TO DEPT_T_BAK'
[14:21:10] INFO:    Connection channel established succesfully
[14:21:10] INFO:    Start to run command
[14:21:24] INFO:    Connection channel closed
[14:21:24] INFO:    Check if exec success or not ... 
[14:21:24] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T_BAK;ALTER TABLE DEPT_T RENAME TO DEPT_T_BAK'
[14:21:24] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 14:20:45 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.635 secondsFAILED: SemanticException [Error 10001]: Table not found default.DEPT_T
[14:21:24] INFO:    Now wait 5 seconds to begin next task ...
[14:21:29] INFO:    Connection channel disconnect
[14:21:29] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:21:29] INFO:    Connection channel established succesfully
[14:21:29] INFO:    Start to run command
[14:21:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:22:10] INFO:    Connection channel closed
[14:22:10] INFO:    Check if exec success or not ... 
[14:22:10] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:22:10] INFO:    Error message: 17/03/30 14:21:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:21:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:21:05 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:21:05 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:21:05 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:21:05 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:21:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:21:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:21:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/91e5e11d451ef4d878c076fe2bffef46/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:21:09 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/91e5e11d451ef4d878c076fe2bffef46/DEPT_T.jar17/03/30 14:21:09 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:21:09 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:21:09 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:21:09 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:21:09 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:21:10 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:21:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:21:12 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:21:19 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:21:19 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:21:19 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:21:19 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:21:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007617/03/30 14:21:20 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007617/03/30 14:21:20 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0076/17/03/30 14:21:20 INFO mapreduce.Job: Running job: job_1490408992134_007617/03/30 14:21:31 INFO mapreduce.Job: Job job_1490408992134_0076 running in uber mode : false17/03/30 14:21:31 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:21:36 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:21:37 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:21:37 INFO mapreduce.Job: Job job_1490408992134_0076 completed successfully17/03/30 14:21:38 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=6986		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=6986		Total vcore-seconds taken by all map tasks=6986		Total megabyte-seconds taken by all map tasks=7153664	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=129		CPU time spent (ms)=1580		Physical memory (bytes) snapshot=343126016		Virtual memory (bytes) snapshot=5557641216		Total committed heap usage (bytes)=338165760	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:21:38 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.9047 seconds (0.386 bytes/sec)17/03/30 14:21:38 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:21:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:21:38 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.558 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=4, totalSize=20]OKTime taken: 1.922 seconds
[14:22:10] INFO:    Now wait 5 seconds to begin next task ...
[14:22:15] INFO:    Connection channel disconnect
[14:22:15] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T_BAK;ALTER TABLE USER_T RENAME TO USER_T_BAK'
[14:22:15] INFO:    Connection channel established succesfully
[14:22:15] INFO:    Start to run command
[14:22:30] INFO:    Connection channel closed
[14:22:30] INFO:    Check if exec success or not ... 
[14:22:30] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T_BAK;ALTER TABLE USER_T RENAME TO USER_T_BAK'
[14:22:30] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 14:21:55 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.198 secondsFAILED: SemanticException [Error 10001]: Table not found default.USER_T
[14:22:30] INFO:    Now wait 5 seconds to begin next task ...
[14:22:35] INFO:    Connection channel disconnect
[14:22:35] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:22:35] INFO:    Connection channel established succesfully
[14:22:35] INFO:    Start to run command
[14:22:35] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:23:21] INFO:    Connection channel closed
[14:23:21] INFO:    Check if exec success or not ... 
[14:23:21] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:23:21] INFO:    Error message: 17/03/30 14:22:16 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:22:16 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:22:16 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:22:16 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:22:16 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:22:16 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:22:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:22:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:22:17 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/25bf3556e405b78afa777d323e61ef45/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:22:21 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/25bf3556e405b78afa777d323e61ef45/USER_T.jar17/03/30 14:22:21 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:22:21 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:22:21 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:22:21 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:22:21 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 14:22:21 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:22:23 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:22:23 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:22:31 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:22:31 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 14:22:31 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 14:22:31 INFO mapreduce.JobSubmitter: number of splits:417/03/30 14:22:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007717/03/30 14:22:32 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007717/03/30 14:22:32 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0077/17/03/30 14:22:32 INFO mapreduce.Job: Running job: job_1490408992134_007717/03/30 14:22:43 INFO mapreduce.Job: Job job_1490408992134_0077 running in uber mode : false17/03/30 14:22:43 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:22:49 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:22:52 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:22:52 INFO mapreduce.Job: Job job_1490408992134_0077 completed successfully17/03/30 14:22:52 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=16275		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=16275		Total vcore-seconds taken by all map tasks=16275		Total megabyte-seconds taken by all map tasks=16665600	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=263		CPU time spent (ms)=3310		Physical memory (bytes) snapshot=691408896		Virtual memory (bytes) snapshot=11118137344		Total committed heap usage (bytes)=668467200	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 14:22:52 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 29.2939 seconds (0.4096 bytes/sec)17/03/30 14:22:52 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 14:22:52 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:22:52 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.291 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=8, totalSize=24]OKTime taken: 1.765 seconds
[14:23:21] INFO:    Now wait 5 seconds to begin next task ...
[14:23:26] INFO:    Connection channel disconnect
[14:23:26] INFO:    SSH connection shutdown
[14:23:26] INFO:    Session initialized and associated with user credential 123456
[14:23:26] INFO:    SSHExec initialized successfully
[14:23:26] INFO:    SSHExec trying to connect root@172.16.110.200
[14:23:26] INFO:    SSH connection established
[14:23:26] INFO:    SSH connection shutdown

=============== [2017/03/30 14:25:47, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:25:47] INFO:    SSHExec initializing ...
[14:25:47] INFO:    Session initialized and associated with user credential 123456
[14:25:47] INFO:    SSHExec initialized successfully
[14:25:47] INFO:    SSHExec trying to connect root@172.16.110.200
[14:25:48] INFO:    SSH connection established
[14:25:48] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T;
[14:25:48] INFO:    Connection channel established succesfully
[14:25:48] INFO:    Start to run command
[14:25:48] INFO:    Connection channel closed
[14:25:48] INFO:    Check if exec success or not ... 
[14:25:48] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T;
[14:25:48] INFO:    Error message: bash: -c: line 0: unexpected EOF while looking for matching `''bash: -c: line 1: syntax error: unexpected end of file
[14:25:48] INFO:    Now wait 5 seconds to begin next task ...
[14:25:53] INFO:    Connection channel disconnect
[14:25:53] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:25:53] INFO:    Connection channel established succesfully
[14:25:53] INFO:    Start to run command
[14:25:53] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:26:36] INFO:    Connection channel closed
[14:26:36] INFO:    Check if exec success or not ... 
[14:26:36] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:26:36] INFO:    Error message: 17/03/30 14:25:42 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:25:42 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:25:42 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:25:42 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:25:43 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:25:43 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:25:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:25:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:25:44 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/a6dd32d81de0b00a475b6dfd1bb01b5a/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:25:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/a6dd32d81de0b00a475b6dfd1bb01b5a/DEPT_T.jar17/03/30 14:25:47 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:25:47 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:25:47 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:25:47 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:25:47 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:25:48 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:25:49 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:25:50 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:25:56 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:25:56 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:25:56 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:25:56 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:25:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_007817/03/30 14:25:58 INFO impl.YarnClientImpl: Submitted application application_1490408992134_007817/03/30 14:25:58 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0078/17/03/30 14:25:58 INFO mapreduce.Job: Running job: job_1490408992134_007817/03/30 14:26:06 INFO mapreduce.Job: Job job_1490408992134_0078 running in uber mode : false17/03/30 14:26:06 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:26:14 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:26:15 INFO mapreduce.Job: Job job_1490408992134_0078 completed successfully17/03/30 14:26:15 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9478		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9478		Total vcore-seconds taken by all map tasks=9478		Total megabyte-seconds taken by all map tasks=9705472	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=117		CPU time spent (ms)=1630		Physical memory (bytes) snapshot=349114368		Virtual memory (bytes) snapshot=5559230464		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:26:15 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.3853 seconds (0.3939 bytes/sec)17/03/30 14:26:15 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:26:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:26:15 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.073 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=6, totalSize=30]OKTime taken: 1.805 seconds
[14:26:36] INFO:    Now wait 5 seconds to begin next task ...
[14:26:41] INFO:    Connection channel disconnect
[14:26:41] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T;
[14:26:41] INFO:    Connection channel established succesfully
[14:26:41] INFO:    Start to run command
[14:26:41] INFO:    Connection channel closed
[14:26:41] INFO:    Check if exec success or not ... 
[14:26:41] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T;
[14:26:41] INFO:    Error message: bash: -c: line 0: unexpected EOF while looking for matching `''bash: -c: line 1: syntax error: unexpected end of file
[14:26:41] INFO:    Now wait 5 seconds to begin next task ...
[14:26:46] INFO:    Connection channel disconnect
[14:26:46] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:26:46] INFO:    Connection channel established succesfully
[14:26:46] INFO:    Start to run command
[14:26:47] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[14:26:47] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.


=============== [2017/03/30 14:27:45, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:27:45] INFO:    SSHExec initializing ...
[14:27:45] INFO:    Session initialized and associated with user credential 123456
[14:27:45] INFO:    SSHExec initialized successfully
[14:27:45] INFO:    SSHExec trying to connect root@172.16.110.200
[14:27:45] INFO:    SSH connection established
[14:27:45] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T;'
[14:27:45] INFO:    Connection channel established succesfully
[14:27:45] INFO:    Start to run command
[14:28:01] INFO:    Connection channel closed
[14:28:01] INFO:    Check if exec success or not ... 
[14:28:01] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS DEPT_T;'
[14:28:01] INFO:    Now wait 5 seconds to begin next task ...
[14:28:06] INFO:    Connection channel disconnect
[14:28:06] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:28:06] INFO:    Connection channel established succesfully
[14:28:06] INFO:    Start to run command
[14:28:06] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:28:53] INFO:    Connection channel closed
[14:28:53] INFO:    Check if exec success or not ... 
[14:28:53] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:28:53] INFO:    Error message: 17/03/30 14:28:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:28:00 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:28:00 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:28:00 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:28:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:28:00 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:28:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:28:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:28:01 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/56dbcc73f29eb8b63456840f6a24575e/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:28:05 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/56dbcc73f29eb8b63456840f6a24575e/DEPT_T.jar17/03/30 14:28:05 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:28:05 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:28:05 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:28:05 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:28:05 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:28:05 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:28:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:28:07 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:28:14 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:28:14 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:28:14 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:28:14 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:28:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008017/03/30 14:28:16 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008017/03/30 14:28:16 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0080/17/03/30 14:28:16 INFO mapreduce.Job: Running job: job_1490408992134_008017/03/30 14:28:25 INFO mapreduce.Job: Job job_1490408992134_0080 running in uber mode : false17/03/30 14:28:25 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:28:31 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:28:31 INFO mapreduce.Job: Job job_1490408992134_0080 completed successfully17/03/30 14:28:32 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=6669		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=6669		Total vcore-seconds taken by all map tasks=6669		Total megabyte-seconds taken by all map tasks=6829056	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=124		CPU time spent (ms)=1660		Physical memory (bytes) snapshot=346501120		Virtual memory (bytes) snapshot=5559050240		Total committed heap usage (bytes)=335020032	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:28:32 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 24.9938 seconds (0.4001 bytes/sec)17/03/30 14:28:32 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:28:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:28:32 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.122 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=8, totalSize=40]OKTime taken: 1.661 seconds
[14:28:53] INFO:    Now wait 5 seconds to begin next task ...
[14:28:58] INFO:    Connection channel disconnect
[14:28:58] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T;'
[14:28:58] INFO:    Connection channel established succesfully
[14:28:58] INFO:    Start to run command
[14:29:15] INFO:    Connection channel closed
[14:29:15] INFO:    Check if exec success or not ... 
[14:29:15] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS USER_T;'
[14:29:15] INFO:    Now wait 5 seconds to begin next task ...
[14:29:20] INFO:    Connection channel disconnect
[14:29:20] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:29:20] INFO:    Connection channel established succesfully
[14:29:20] INFO:    Start to run command
[14:29:20] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:30:10] INFO:    Connection channel closed
[14:30:10] INFO:    Check if exec success or not ... 
[14:30:10] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:30:10] INFO:    Error message: 17/03/30 14:29:09 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:29:09 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:29:09 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:29:09 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:29:09 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:29:09 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:29:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:29:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:29:10 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/ed8712244883326c2bf7530c68e43368/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:29:14 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/ed8712244883326c2bf7530c68e43368/USER_T.jar17/03/30 14:29:14 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:29:14 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:29:14 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:29:14 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:29:14 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 14:29:14 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:29:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:29:16 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:29:23 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:29:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 14:29:23 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 14:29:24 INFO mapreduce.JobSubmitter: number of splits:417/03/30 14:29:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008117/03/30 14:29:25 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008117/03/30 14:29:25 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0081/17/03/30 14:29:25 INFO mapreduce.Job: Running job: job_1490408992134_008117/03/30 14:29:35 INFO mapreduce.Job: Job job_1490408992134_0081 running in uber mode : false17/03/30 14:29:35 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:29:42 INFO mapreduce.Job:  map 25% reduce 0%17/03/30 14:29:43 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 14:29:46 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:29:47 INFO mapreduce.Job: Job job_1490408992134_0081 completed successfully17/03/30 14:29:48 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=17617		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=17617		Total vcore-seconds taken by all map tasks=17617		Total megabyte-seconds taken by all map tasks=18039808	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=240		CPU time spent (ms)=3210		Physical memory (bytes) snapshot=705765376		Virtual memory (bytes) snapshot=11112222720		Total committed heap usage (bytes)=691011584	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 14:29:48 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 31.53 seconds (0.3806 bytes/sec)17/03/30 14:29:48 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 14:29:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:29:48 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.818 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=16, totalSize=48]OKTime taken: 1.605 seconds
[14:30:10] INFO:    Now wait 5 seconds to begin next task ...
[14:30:15] INFO:    Connection channel disconnect
[14:30:15] INFO:    SSH connection shutdown
[14:30:15] INFO:    Session initialized and associated with user credential 123456
[14:30:15] INFO:    SSHExec initialized successfully
[14:30:15] INFO:    SSHExec trying to connect root@172.16.110.200
[14:30:16] INFO:    SSH connection established
[14:30:16] INFO:    SSH connection shutdown

=============== [2017/03/30 14:34:54, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:34:54] INFO:    SSHExec initializing ...
[14:34:54] INFO:    Session initialized and associated with user credential 123456
[14:34:54] INFO:    SSHExec initialized successfully
[14:34:54] INFO:    SSHExec trying to connect root@172.16.110.200
[14:34:54] INFO:    SSH connection established
[14:34:54] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:34:54] INFO:    Connection channel established succesfully
[14:34:54] INFO:    Start to run command
[14:35:12] INFO:    Connection channel closed
[14:35:12] INFO:    Check if exec success or not ... 
[14:35:12] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:35:12] INFO:    Now wait 5 seconds to begin next task ...
[14:35:17] INFO:    Connection channel disconnect
[14:35:17] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:35:17] INFO:    Connection channel established succesfully
[14:35:17] INFO:    Start to run command
[14:35:17] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:36:01] INFO:    Connection channel closed
[14:36:01] INFO:    Check if exec success or not ... 
[14:36:01] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:36:01] INFO:    Error message: 17/03/30 14:35:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:35:00 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:35:00 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:35:00 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:35:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:35:00 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:35:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:35:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:35:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/7cadeaf5a3be10f6de377ca7a6ffdb15/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:35:05 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/7cadeaf5a3be10f6de377ca7a6ffdb15/DEPT_T.jar17/03/30 14:35:05 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:35:05 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:35:05 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:35:05 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:35:05 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:35:05 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:35:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:35:07 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:35:14 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:35:14 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:35:14 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:35:14 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:35:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008217/03/30 14:35:15 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008217/03/30 14:35:16 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0082/17/03/30 14:35:16 INFO mapreduce.Job: Running job: job_1490408992134_008217/03/30 14:35:25 INFO mapreduce.Job: Job job_1490408992134_0082 running in uber mode : false17/03/30 14:35:25 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:35:30 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:35:32 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:35:32 INFO mapreduce.Job: Job job_1490408992134_0082 completed successfully17/03/30 14:35:32 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8211		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8211		Total vcore-seconds taken by all map tasks=8211		Total megabyte-seconds taken by all map tasks=8408064	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=115		CPU time spent (ms)=1680		Physical memory (bytes) snapshot=360284160		Virtual memory (bytes) snapshot=5558980608		Total committed heap usage (bytes)=358612992	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:35:32 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.349 seconds (0.3945 bytes/sec)17/03/30 14:35:33 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:35:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:35:33 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.806 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.291 seconds
[14:36:01] INFO:    Now wait 5 seconds to begin next task ...
[14:36:06] INFO:    Connection channel disconnect
[14:36:06] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[14:36:06] INFO:    Connection channel established succesfully
[14:36:06] INFO:    Start to run command
[14:36:25] INFO:    Connection channel closed
[14:36:25] INFO:    Check if exec success or not ... 
[14:36:25] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[14:36:25] INFO:    Now wait 5 seconds to begin next task ...
[14:36:30] INFO:    Connection channel disconnect
[14:36:30] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:36:30] INFO:    Connection channel established succesfully
[14:36:30] INFO:    Start to run command
[14:36:30] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:37:23] INFO:    Connection channel closed
[14:37:23] INFO:    Check if exec success or not ... 
[14:37:23] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:37:23] INFO:    Error message: 17/03/30 14:36:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:36:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:36:12 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:36:12 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:36:13 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:36:13 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:36:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:36:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:36:14 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d737acae1746ee7cf42cd0a0c9cbe915/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:36:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/d737acae1746ee7cf42cd0a0c9cbe915/USER_T.jar17/03/30 14:36:17 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:36:17 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:36:17 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:36:17 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:36:18 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 14:36:18 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:36:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:36:20 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:36:29 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:36:29 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 14:36:29 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 14:36:29 INFO mapreduce.JobSubmitter: number of splits:417/03/30 14:36:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008317/03/30 14:36:31 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008317/03/30 14:36:31 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0083/17/03/30 14:36:31 INFO mapreduce.Job: Running job: job_1490408992134_008317/03/30 14:36:41 INFO mapreduce.Job: Job job_1490408992134_0083 running in uber mode : false17/03/30 14:36:41 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:36:47 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:36:49 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 14:36:52 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:36:53 INFO mapreduce.Job: Job job_1490408992134_0083 completed successfully17/03/30 14:36:53 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=14791		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=14791		Total vcore-seconds taken by all map tasks=14791		Total megabyte-seconds taken by all map tasks=15145984	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=240		CPU time spent (ms)=3280		Physical memory (bytes) snapshot=709464064		Virtual memory (bytes) snapshot=11115409408		Total committed heap usage (bytes)=714080256	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 14:36:53 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 33.0969 seconds (0.3626 bytes/sec)17/03/30 14:36:53 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 14:36:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:36:53 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.699 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=4, totalSize=12]OKTime taken: 2.174 seconds
[14:37:23] INFO:    Now wait 5 seconds to begin next task ...
[14:37:28] INFO:    Connection channel disconnect
[14:37:28] INFO:    SSH connection shutdown
[14:37:28] INFO:    Session initialized and associated with user credential 123456
[14:37:28] INFO:    SSHExec initialized successfully
[14:37:28] INFO:    SSHExec trying to connect root@172.16.110.200
[14:37:28] INFO:    SSH connection established
[14:37:28] INFO:    SSH connection shutdown

=============== [2017/03/30 14:38:54, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:38:54] INFO:    SSHExec initializing ...
[14:38:54] INFO:    Session initialized and associated with user credential 123456
[14:38:54] INFO:    SSHExec initialized successfully
[14:38:54] INFO:    SSHExec trying to connect root@172.16.110.200
[14:38:55] INFO:    SSH connection established
[14:38:55] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.ALL-TABLES;'
[14:38:55] INFO:    Connection channel established succesfully
[14:38:55] INFO:    Start to run command
[14:39:08] INFO:    Connection channel closed
[14:39:08] INFO:    Check if exec success or not ... 
[14:39:08] INFO:    Execution failed while executing command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.ALL-TABLES;'
[14:39:08] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 14:38:41 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: ParseException line 1:29 missing EOF at '-' near 'ALL'
[14:39:08] INFO:    Now wait 5 seconds to begin next task ...
[14:39:14] INFO:    Connection channel disconnect
[14:39:14] INFO:    Command is sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:39:14] INFO:    Connection channel established succesfully
[14:39:14] INFO:    Start to run command
[14:39:14] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:40:54] INFO:    Connection channel closed
[14:40:54] INFO:    Check if exec success or not ... 
[14:40:54] INFO:    Execution failed while executing command: sudo -u hdfs sqoop-import-all-tables -m 4 --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:40:54] INFO:    Error message: 17/03/30 14:38:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:38:59 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:38:59 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:38:59 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:38:59 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:39:01 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:39:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 14:39:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 14:39:01 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/6d1f178f1a1926964f19097eb61e9ec4/dept_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:39:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/6d1f178f1a1926964f19097eb61e9ec4/dept_t.jar17/03/30 14:39:04 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:39:04 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:39:04 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:39:04 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:39:04 INFO mapreduce.ImportJobBase: Beginning import of dept_t17/03/30 14:39:05 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:39:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:39:06 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:39:14 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:39:14 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `dept_t`17/03/30 14:39:14 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:39:14 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:39:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008417/03/30 14:39:16 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008417/03/30 14:39:16 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0084/17/03/30 14:39:16 INFO mapreduce.Job: Running job: job_1490408992134_008417/03/30 14:39:26 INFO mapreduce.Job: Job job_1490408992134_0084 running in uber mode : false17/03/30 14:39:26 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:39:33 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:39:34 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:39:35 INFO mapreduce.Job: Job job_1490408992134_0084 completed successfully17/03/30 14:39:35 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=286834		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8285		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8285		Total vcore-seconds taken by all map tasks=8285		Total megabyte-seconds taken by all map tasks=8483840	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=118		CPU time spent (ms)=1580		Physical memory (bytes) snapshot=347283456		Virtual memory (bytes) snapshot=5558157312		Total committed heap usage (bytes)=336592896	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:39:35 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 28.6691 seconds (0.3488 bytes/sec)17/03/30 14:39:35 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:39:35 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `dept_t` AS t LIMIT 117/03/30 14:39:35 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.885 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=4, totalSize=20]OKTime taken: 2.362 secondsNote: /tmp/sqoop-hdfs/compile/6d1f178f1a1926964f19097eb61e9ec4/table_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.052 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=7, totalSize=24]OKTime taken: 0.953 secondsNote: /tmp/sqoop-hdfs/compile/6d1f178f1a1926964f19097eb61e9ec4/user_t.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 0.112 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=8, totalSize=24]OKTime taken: 1.127 seconds
[14:40:54] INFO:    Now wait 5 seconds to begin next task ...
[14:40:59] INFO:    Connection channel disconnect
[14:40:59] INFO:    SSH connection shutdown

=============== [2017/03/30 14:42:48, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:42:48] INFO:    SSHExec initializing ...
[14:42:48] INFO:    Session initialized and associated with user credential 123456
[14:42:48] INFO:    SSHExec initialized successfully
[14:42:48] INFO:    SSHExec trying to connect root@172.16.110.200
[14:42:49] INFO:    SSH connection established
[14:42:49] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:42:49] INFO:    Connection channel established succesfully
[14:42:49] INFO:    Start to run command
[14:43:06] INFO:    Connection channel closed
[14:43:06] INFO:    Check if exec success or not ... 
[14:43:06] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:43:06] INFO:    Now wait 5 seconds to begin next task ...
[14:43:11] INFO:    Connection channel disconnect
[14:43:11] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:43:11] INFO:    Connection channel established succesfully
[14:43:11] INFO:    Start to run command
[14:43:11] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:43:58] INFO:    Connection channel closed
[14:43:58] INFO:    Check if exec success or not ... 
[14:43:58] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:43:58] INFO:    Error message: 17/03/30 14:42:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:42:58 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:42:58 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:42:58 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:42:58 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:42:58 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:42:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:42:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:42:59 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/764816927d1d605a687a53c523b18362/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:43:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/764816927d1d605a687a53c523b18362/DEPT_T.jar17/03/30 14:43:03 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:43:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:43:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:43:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:43:03 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:43:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:43:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:43:06 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:43:14 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:43:14 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:43:14 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:43:14 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:43:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008717/03/30 14:43:15 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008717/03/30 14:43:15 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0087/17/03/30 14:43:15 INFO mapreduce.Job: Running job: job_1490408992134_008717/03/30 14:43:24 INFO mapreduce.Job: Job job_1490408992134_0087 running in uber mode : false17/03/30 14:43:24 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:43:32 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:43:32 INFO mapreduce.Job: Job job_1490408992134_0087 completed successfully17/03/30 14:43:33 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8939		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8939		Total vcore-seconds taken by all map tasks=8939		Total megabyte-seconds taken by all map tasks=9153536	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=118		CPU time spent (ms)=1680		Physical memory (bytes) snapshot=346718208		Virtual memory (bytes) snapshot=5560143872		Total committed heap usage (bytes)=336592896	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:43:33 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 27.107 seconds (0.3689 bytes/sec)17/03/30 14:43:33 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:43:33 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:43:33 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.259 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.632 seconds
[14:43:58] INFO:    Now wait 5 seconds to begin next task ...
[14:44:03] INFO:    Connection channel disconnect
[14:44:03] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[14:44:03] INFO:    Connection channel established succesfully
[14:44:03] INFO:    Start to run command
[14:44:20] INFO:    Connection channel closed
[14:44:20] INFO:    Check if exec success or not ... 
[14:44:20] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[14:44:20] INFO:    Now wait 5 seconds to begin next task ...
[14:44:25] INFO:    Connection channel disconnect
[14:44:25] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:44:25] INFO:    Connection channel established succesfully
[14:44:25] INFO:    Start to run command
[14:44:25] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:45:12] INFO:    Connection channel closed
[14:45:12] INFO:    Check if exec success or not ... 
[14:45:12] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:45:12] INFO:    Error message: 17/03/30 14:44:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:44:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:44:12 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:44:12 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:44:12 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:44:12 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:44:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:44:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:44:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/74321569508908443510319ecf368b22/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:44:17 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/74321569508908443510319ecf368b22/USER_T.jar17/03/30 14:44:17 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:44:17 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:44:17 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:44:17 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:44:17 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 14:44:17 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:44:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:44:19 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:44:26 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:44:26 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 14:44:26 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 14:44:26 INFO mapreduce.JobSubmitter: number of splits:417/03/30 14:44:26 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008817/03/30 14:44:27 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008817/03/30 14:44:27 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0088/17/03/30 14:44:27 INFO mapreduce.Job: Running job: job_1490408992134_008817/03/30 14:44:37 INFO mapreduce.Job: Job job_1490408992134_0088 running in uber mode : false17/03/30 14:44:37 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:44:43 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 14:44:45 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 14:44:47 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:44:47 INFO mapreduce.Job: Job job_1490408992134_0088 completed successfully17/03/30 14:44:47 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=16281		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=16281		Total vcore-seconds taken by all map tasks=16281		Total megabyte-seconds taken by all map tasks=16671744	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=253		CPU time spent (ms)=3380		Physical memory (bytes) snapshot=696573952		Virtual memory (bytes) snapshot=11119800320		Total committed heap usage (bytes)=670040064	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 14:44:47 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 28.6652 seconds (0.4186 bytes/sec)17/03/30 14:44:48 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 14:44:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 14:44:48 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.4 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=4, totalSize=12]OKTime taken: 1.281 seconds
[14:45:12] INFO:    Now wait 5 seconds to begin next task ...
[14:45:17] INFO:    Connection channel disconnect
[14:45:17] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[14:45:17] INFO:    Connection channel established succesfully
[14:45:17] INFO:    Start to run command
[14:45:34] INFO:    Connection channel closed
[14:45:34] INFO:    Check if exec success or not ... 
[14:45:34] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[14:45:34] INFO:    Now wait 5 seconds to begin next task ...
[14:45:39] INFO:    Connection channel disconnect
[14:45:39] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:45:39] INFO:    Connection channel established succesfully
[14:45:39] INFO:    Start to run command
[14:45:39] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[14:45:39] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:46:26] INFO:    Connection channel closed
[14:46:26] INFO:    Check if exec success or not ... 
[14:46:26] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:46:26] INFO:    Error message: 17/03/30 14:45:25 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:45:26 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:45:26 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:45:26 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:45:26 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:45:26 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:45:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 14:45:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 14:45:27 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/6c2614c400646eca145a4853b868b746/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:45:30 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/6c2614c400646eca145a4853b868b746/TABLE_T.jar17/03/30 14:45:30 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:45:30 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:45:30 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:45:30 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:45:30 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 14:45:31 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:45:33 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:45:33 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:45:41 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:45:41 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 14:45:41 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:45:41 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:45:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_008917/03/30 14:45:43 INFO impl.YarnClientImpl: Submitted application application_1490408992134_008917/03/30 14:45:43 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0089/17/03/30 14:45:43 INFO mapreduce.Job: Running job: job_1490408992134_008917/03/30 14:45:51 INFO mapreduce.Job: Job job_1490408992134_0089 running in uber mode : false17/03/30 14:45:51 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:45:58 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:45:58 INFO mapreduce.Job: Job job_1490408992134_0089 completed successfully17/03/30 14:45:59 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287388		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=8		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8800		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8800		Total vcore-seconds taken by all map tasks=8800		Total megabyte-seconds taken by all map tasks=9011200	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=116		CPU time spent (ms)=1710		Physical memory (bytes) snapshot=360783872		Virtual memory (bytes) snapshot=5560115200		Total committed heap usage (bytes)=355991552	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=817/03/30 14:45:59 INFO mapreduce.ImportJobBase: Transferred 8 bytes in 25.6391 seconds (0.312 bytes/sec)17/03/30 14:45:59 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:45:59 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 14:45:59 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.764 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.359 seconds
[14:46:26] INFO:    Now wait 5 seconds to begin next task ...
[14:46:32] INFO:    Connection channel disconnect
[14:46:32] INFO:    SSH connection shutdown
[14:46:32] INFO:    Session initialized and associated with user credential 123456
[14:46:32] INFO:    Session initialized and associated with user credential 123456
[14:46:32] INFO:    SSHExec initialized successfully
[14:46:32] INFO:    SSHExec initialized successfully
[14:46:32] INFO:    SSHExec trying to connect root@172.16.110.200
[14:46:32] INFO:    SSHExec trying to connect root@172.16.110.200

=============== [2017/03/30 14:48:21, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:48:21] INFO:    SSHExec initializing ...
[14:48:21] INFO:    Session initialized and associated with user credential 123456
[14:48:21] INFO:    SSHExec initialized successfully
[14:48:21] INFO:    SSHExec trying to connect root@172.16.110.200
[14:48:23] INFO:    Session initialized and associated with user credential 123456
[14:48:23] INFO:    SSHExec initialized successfully
[14:48:23] INFO:    SSHExec trying to connect root@172.16.110.200
[14:48:23] INFO:    Session initialized and associated with user credential 123456
[14:48:23] INFO:    SSHExec initialized successfully
[14:48:23] INFO:    SSHExec trying to connect root@172.16.110.200
[14:48:24] INFO:    SSH connection established
[14:48:24] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[14:48:24] INFO:    SSH connection established
[14:48:24] INFO:    SSH connection established
[14:48:24] INFO:    Connection channel established succesfully
[14:48:24] INFO:    Start to run command
[14:48:41] INFO:    Connection channel closed
[14:48:41] INFO:    Check if exec success or not ... 
[14:48:41] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[14:48:41] INFO:    Now wait 5 seconds to begin next task ...
[14:48:47] INFO:    Connection channel disconnect
[14:48:47] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:48:47] INFO:    Connection channel established succesfully
[14:48:47] INFO:    Start to run command
[14:49:04] INFO:    Connection channel closed
[14:49:04] INFO:    Check if exec success or not ... 
[14:49:04] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[14:49:04] INFO:    Now wait 5 seconds to begin next task ...
[14:49:09] INFO:    Connection channel disconnect
[14:49:09] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:49:09] INFO:    Connection channel established succesfully
[14:49:09] INFO:    Start to run command
[14:49:09] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[14:49:09] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[14:49:55] INFO:    Connection channel closed
[14:49:55] INFO:    Check if exec success or not ... 
[14:49:55] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[14:49:55] INFO:    Error message: 17/03/30 14:48:43 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 14:48:43 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 14:48:43 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 14:48:43 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 14:48:43 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 14:48:43 INFO tool.CodeGenTool: Beginning code generation17/03/30 14:48:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:48:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:48:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/0c6415bed15d06fa66f1dada111b6020/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 14:48:48 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/0c6415bed15d06fa66f1dada111b6020/DEPT_T.jar17/03/30 14:48:48 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 14:48:48 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 14:48:48 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 14:48:48 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 14:48:48 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 14:48:48 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 14:48:50 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 14:48:50 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 14:48:57 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 14:48:57 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 14:48:57 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 14:48:57 INFO mapreduce.JobSubmitter: number of splits:217/03/30 14:48:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009017/03/30 14:48:58 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009017/03/30 14:48:58 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0090/17/03/30 14:48:58 INFO mapreduce.Job: Running job: job_1490408992134_009017/03/30 14:49:08 INFO mapreduce.Job: Job job_1490408992134_0090 running in uber mode : false17/03/30 14:49:08 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 14:49:14 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 14:49:15 INFO mapreduce.Job: Job job_1490408992134_0090 completed successfully17/03/30 14:49:15 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=6831		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=6831		Total vcore-seconds taken by all map tasks=6831		Total megabyte-seconds taken by all map tasks=6994944	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=104		CPU time spent (ms)=1620		Physical memory (bytes) snapshot=359489536		Virtual memory (bytes) snapshot=5558181888		Total committed heap usage (bytes)=357564416	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 14:49:15 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 25.2008 seconds (0.3968 bytes/sec)17/03/30 14:49:15 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 14:49:15 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 14:49:15 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.341 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.396 seconds
[14:49:55] INFO:    Now wait 5 seconds to begin next task ...
[14:50:00] INFO:    Connection channel disconnect
[14:50:00] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[14:50:00] ERROR:   session is down
[14:50:00] ERROR:   session is down
[14:50:00] ERROR:   session is down
[14:50:00] INFO:    SSH connection shutdown
[14:50:00] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException
[14:50:00] ERROR:   Disconnect fails with the following exception: java.lang.NullPointerException

=============== [2017/03/30 15:20:28, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================


=============== [2017/03/30 15:23:08, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:23:08] INFO:    SSHExec initializing ...
[15:23:08] INFO:    SSHExec initializing ...
[15:23:08] INFO:    SSHExec initializing ...
[15:23:08] INFO:    Session initialized and associated with user credential 123456
[15:23:08] INFO:    Session initialized and associated with user credential 123456
[15:23:08] INFO:    Session initialized and associated with user credential 123456
[15:23:08] INFO:    SSHExec initialized successfully
[15:23:08] INFO:    SSHExec initialized successfully
[15:23:08] INFO:    SSHExec trying to connect root@172.16.110.200
[15:23:08] INFO:    SSHExec initialized successfully
[15:23:08] INFO:    SSHExec trying to connect root@172.16.110.200
[15:23:08] INFO:    SSHExec trying to connect root@172.16.110.200
[15:23:13] INFO:    SSH connection established
[15:23:13] INFO:    SSH connection established
[15:23:13] INFO:    SSH connection established
[15:23:13] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:23:13] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:23:13] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:23:13] INFO:    Connection channel established succesfully
[15:23:13] INFO:    Start to run command
[15:23:13] INFO:    Connection channel established succesfully
[15:23:13] INFO:    Start to run command
[15:23:13] INFO:    Connection channel established succesfully
[15:23:13] INFO:    Start to run command
[15:23:31] INFO:    Connection channel closed
[15:23:31] INFO:    Check if exec success or not ... 
[15:23:31] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:23:31] INFO:    Now wait 5 seconds to begin next task ...
[15:23:32] INFO:    Connection channel closed
[15:23:32] INFO:    Check if exec success or not ... 
[15:23:32] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:23:32] INFO:    Now wait 5 seconds to begin next task ...
[15:23:32] INFO:    Connection channel closed
[15:23:32] INFO:    Check if exec success or not ... 
[15:23:32] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:23:32] INFO:    Now wait 5 seconds to begin next task ...
[15:23:37] INFO:    Connection channel disconnect
[15:23:37] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:23:37] INFO:    Connection channel disconnect
[15:23:37] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:23:37] INFO:    Connection channel established succesfully
[15:23:37] INFO:    Start to run command
[15:23:37] INFO:    Connection channel established succesfully
[15:23:37] INFO:    Start to run command
[15:23:37] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:23:37] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:23:37] INFO:    Connection channel disconnect
[15:23:37] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:23:37] INFO:    Connection channel established succesfully
[15:23:37] INFO:    Start to run command
[15:23:37] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[15:23:37] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:24:33] INFO:    Connection channel closed
[15:24:33] INFO:    Check if exec success or not ... 
[15:24:33] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:24:33] INFO:    Error message: 17/03/30 15:23:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:23:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:23:12 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:23:12 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:23:13 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:23:13 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:23:18 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.jar17/03/30 15:23:18 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:23:18 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:23:18 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:23:18 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:23:18 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 15:23:18 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 15:23:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:23:21 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 15:23:34 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:23:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 15:23:34 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 15:23:35 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:23:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009317/03/30 15:23:36 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009317/03/30 15:23:37 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0093/17/03/30 15:23:37 INFO mapreduce.Job: Running job: job_1490408992134_009317/03/30 15:23:46 INFO mapreduce.Job: Job job_1490408992134_0092 running in uber mode : false17/03/30 15:23:46 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:23:54 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:23:59 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:24:00 INFO mapreduce.Job: Job job_1490408992134_0092 completed successfully17/03/30 15:24:01 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9811		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9811		Total vcore-seconds taken by all map tasks=9811		Total megabyte-seconds taken by all map tasks=10046464	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=134		CPU time spent (ms)=1970		Physical memory (bytes) snapshot=348303360		Virtual memory (bytes) snapshot=5559484416		Total committed heap usage (bytes)=337641472	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:24:01 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 40.4372 seconds (0.2473 bytes/sec)17/03/30 15:24:01 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:24:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:24:01 INFO hive.HiveImport: Loading uploaded data into HiveveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.996 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.36 seconds
[15:24:33] INFO:    Now wait 5 seconds to begin next task ...
[15:24:38] INFO:    Connection channel disconnect
[15:24:38] INFO:    SSH connection shutdown
[15:24:40] INFO:    Connection channel closed
[15:24:40] INFO:    Check if exec success or not ... 
[15:24:40] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:24:40] INFO:    Error message: 17/03/30 15:23:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:23:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:23:12 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:23:12 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:23:13 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:23:13 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:23:18 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.jar17/03/30 15:23:18 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:23:18 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:23:18 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:23:18 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:23:18 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 15:23:18 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 15:23:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:23:21 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 15:23:34 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:23:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 15:23:34 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 15:23:35 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:23:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009317/03/30 15:23:36 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009317/03/30 15:23:37 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0093/17/03/30 15:23:37 INFO mapreduce.Job: Running job: job_1490408992134_009317/03/30 15:24:03 INFO mapreduce.Job: Job job_1490408992134_0093 running in uber mode : false17/03/30 15:24:03 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:23:54 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:23:59 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:24:00 INFO mapreduce.Job: Job job_1490408992134_0092 completed successfully17/03/30 15:24:01 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9811		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9811		Total vcore-seconds taken by all map tasks=9811		Total megabyte-seconds taken by all map tasks=10046464	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=134		CPU time spent (ms)=1970		Physical memory (bytes) snapshot=348303360		Virtual memory (bytes) snapshot=5559484416		Total committed heap usage (bytes)=337641472	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:24:01 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 40.4372 seconds (0.2473 bytes/sec)17/03/30 15:24:01 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:24:01 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:24:01 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.921 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.08 secondsnds
[15:24:40] INFO:    Now wait 5 seconds to begin next task ...
[15:24:46] INFO:    Connection channel disconnect
[15:24:46] INFO:    SSH connection shutdown
[15:24:56] INFO:    Connection channel closed
[15:24:56] INFO:    Check if exec success or not ... 
[15:24:56] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:24:56] INFO:    Error message: 17/03/30 15:23:12 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:23:12 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:23:12 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:23:12 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:23:13 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:23:13 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:23:14 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:23:18 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/61ab03d2002adc3a103411d518e05b3a/DEPT_T.jar17/03/30 15:23:18 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:23:18 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:23:18 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:23:18 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:23:18 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 15:23:18 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 15:23:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:23:21 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 15:23:34 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:23:34 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 15:23:34 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 417/03/30 15:23:35 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:23:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009317/03/30 15:23:36 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009317/03/30 15:23:37 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0093/17/03/30 15:23:37 INFO mapreduce.Job: Running job: job_1490408992134_009317/03/30 15:24:03 INFO mapreduce.Job: Job job_1490408992134_0093 running in uber mode : false17/03/30 15:24:03 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:24:12 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:24:15 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 15:24:17 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:24:17 INFO mapreduce.Job: Job job_1490408992134_0093 completed successfully17/03/30 15:24:18 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=25817		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=25817		Total vcore-seconds taken by all map tasks=25817		Total megabyte-seconds taken by all map tasks=26436608	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=243		CPU time spent (ms)=4780		Physical memory (bytes) snapshot=705794048		Virtual memory (bytes) snapshot=11118534656		Total committed heap usage (bytes)=710410240	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 15:24:18 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 57.0338 seconds (0.2104 bytes/sec)17/03/30 15:24:18 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 15:24:18 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:24:18 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.289 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=4, totalSize=12]OKTime taken: 0.962 seconds
[15:24:56] INFO:    Now wait 5 seconds to begin next task ...
[15:25:01] INFO:    Connection channel disconnect
[15:25:01] INFO:    SSH connection shutdown

=============== [2017/03/30 15:26:51, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:26:51] INFO:    SSHExec initializing ...
[15:26:51] INFO:    SSHExec initializing ...
[15:26:51] INFO:    SSHExec initializing ...
[15:26:51] INFO:    Session initialized and associated with user credential 123456
[15:26:51] INFO:    Session initialized and associated with user credential 123456
[15:26:51] INFO:    Session initialized and associated with user credential 123456
[15:26:51] INFO:    SSHExec initialized successfully
[15:26:51] INFO:    SSHExec initialized successfully
[15:26:51] INFO:    SSHExec trying to connect root@172.16.110.200
[15:26:51] INFO:    SSHExec initialized successfully
[15:26:51] INFO:    SSHExec trying to connect root@172.16.110.200
[15:26:51] INFO:    SSHExec trying to connect root@172.16.110.200
[15:26:51] INFO:    SSH connection established
[15:26:51] INFO:    SSH connection established
[15:26:51] INFO:    SSH connection established
[15:26:51] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:26:51] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:26:51] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:26:51] INFO:    Connection channel established succesfully
[15:26:51] INFO:    Connection channel established succesfully
[15:26:51] INFO:    Start to run command
[15:26:51] INFO:    Start to run command
[15:26:51] INFO:    Connection channel established succesfully
[15:26:51] INFO:    Start to run command
[15:27:09] INFO:    Connection channel closed
[15:27:09] INFO:    Check if exec success or not ... 
[15:27:09] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:27:09] INFO:    Now wait 5 seconds to begin next task ...
[15:27:09] INFO:    Connection channel closed
[15:27:09] INFO:    Check if exec success or not ... 
[15:27:09] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:27:09] INFO:    Now wait 5 seconds to begin next task ...
[15:27:09] INFO:    Connection channel closed
[15:27:09] INFO:    Check if exec success or not ... 
[15:27:09] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:27:09] INFO:    Now wait 5 seconds to begin next task ...
[15:27:14] INFO:    Connection channel disconnect
[15:27:14] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:27:14] INFO:    Connection channel established succesfully
[15:27:14] INFO:    Start to run command
[15:27:14] INFO:    Connection channel disconnect
[15:27:14] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:27:14] INFO:    Connection channel disconnect
[15:27:14] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:27:14] INFO:    Connection channel established succesfully
[15:27:14] INFO:    Start to run command
[15:27:14] INFO:    Connection channel established succesfully
[15:27:14] INFO:    Start to run command
[15:27:14] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:27:14] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:27:14] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:28:05] INFO:    Connection channel closed
[15:28:05] INFO:    Check if exec success or not ... 
[15:28:05] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:28:05] INFO:    Error message: 17/03/30 15:26:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:26:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:26:51 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:26:51 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:26:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:26:52 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:26:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.jar17/03/30 15:26:57 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:26:57 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:26:57 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:26:57 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:26:57 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/0317/03/30 15:26:57 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:26:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.17/0317/03/30 15:27:00 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:27:15 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:27:15 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:27:15 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 15:27:15 INFO mapreduce.JobSubmitter: number of splits:217/03/30 15:27:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_149040899213417/03/17/03/30 15:27:17 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009517/03/30 15:27:17 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0095/17/03/30 15:27:17 INFO mapreduce.Job: Running job: job_149040899213417/03/30 15:27:24 INFO mapreduce.Job: Job job_1490408992134_0096 running in uber mode : false17/03/30 15:27:24 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:27:35 INFO mapreduce.Job:  map 25% reduce 0%17/03/30 15:27:37 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:27:38 INFO mapreduce.Job:  map 75% reduce 0%134_0096 completed successfully17/03/30 15:27:32 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8659		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8659		Total vcore-seconds taken by all map tasks=8659		Total megabyte-seconds taken by all map tasks=8866816	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=131		CPU time spent (ms)=2170		Physical memory (bytes) snapshot=361902080		Virtual memory (bytes) snapshot=5558636544		Total committed heap usage (bytes)=357040128	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:27:32 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 32.4713 seconds (0.308 bytes/sec)17/03/30 15:27:32 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:27:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:27:32 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.749 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=4, numRows=0, totalSize=20, rawDataSize=0]OKTime taken: 1.504 seconds
[15:28:05] INFO:    Now wait 5 seconds to begin next task ...
[15:28:10] INFO:    Connection channel disconnect
[15:28:10] INFO:    SSH connection shutdown
[15:28:18] INFO:    Connection channel closed
[15:28:18] INFO:    Check if exec success or not ... 
[15:28:18] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:28:18] INFO:    Error message: 17/03/30 15:26:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:26:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:26:51 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:26:51 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:26:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:26:52 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:26:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.jar17/03/30 15:26:57 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:26:57 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:26:57 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:26:57 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:26:57 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/0317/03/30 15:26:57 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:26:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.17/0317/03/30 15:27:00 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:27:15 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:27:15 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:27:15 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 15:27:15 INFO mapreduce.JobSubmitter: number of splits:217/03/30 15:27:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_149040899213417/03/17/03/30 15:27:17 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009517/03/30 15:27:17 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0095/17/03/30 15:27:17 INFO mapreduce.Job: Running job: job_149040899213417/03/17/03/30 15:27:48 INFO mapreduce.Job: Job job_1490408992134_0095 running in uber mode : false17/03/30 15:27:48 INFO mapreduce.Job:  map 0% reduce 0%30 15:27:35 INFO mapreduce.Job:  map 25% reduce 0%17/03/30 15:27:37 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:27:38 INFO mapreduce.Job:  map 75% reduce 0%17/03/30 15:27:44 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:27:45 INFO mapreduce.Job: Job job_1490408992134_0094 completed successfully17/03/30 15:27:46 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=17050		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=17050		Total vcore-seconds taken by all map tasks=17050		Total megabyte-seconds taken by all map tasks=17459200	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=229		CPU time spent (ms)=3310		Physical memory (bytes) snapshot=708333568		Virtual memory (bytes) snapshot=11117228032		Total committed heap usage (bytes)=716177408	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 15:27:46 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 46.1398 seconds (0.2601 bytes/sec)17/03/30 15:27:46 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 15:27:46 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:27:46 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.059 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=8, numRows=0, totalSize=24, rawDataSize=0]OKTime taken: 1.061 seconds
[15:28:18] INFO:    Now wait 5 seconds to begin next task ...
[15:28:23] INFO:    Connection channel disconnect
[15:28:23] INFO:    SSH connection shutdown
[15:28:29] INFO:    Connection channel closed
[15:28:29] INFO:    Check if exec success or not ... 
[15:28:29] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:28:29] INFO:    Error message: 17/03/30 15:26:51 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:26:51 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:26:51 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:26:51 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:26:52 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:26:52 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:26:53 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:26:57 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/98c17fa2008eabcfc51e2ae135f9e234/DEPT_T.jar17/03/30 15:26:57 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:26:57 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:26:57 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:26:57 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:26:57 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/0317/03/30 15:26:57 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:26:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.17/0317/03/30 15:27:00 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:27:15 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:27:15 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:27:15 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 to: 217/03/30 15:27:15 INFO mapreduce.JobSubmitter: number of splits:217/03/30 15:27:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_149040899213417/03/17/03/30 15:27:17 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009517/03/30 15:27:17 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0095/17/03/30 15:27:17 INFO mapreduce.Job: Running job: job_149040899213417/03/17/03/30 15:27:48 INFO mapreduce.Job: Job job_1490408992134_0095 running in uber mode : false17/03/30 15:27:48 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:27:54 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:27:55 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:27:55 INFO mapreduce.Job: Job job_1490408992134_0095 completed successfully17/03/30 15:27:55 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287388		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=8		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8025		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8025		Total vcore-seconds taken by all map tasks=8025		Total megabyte-seconds taken by all map tasks=8217600	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=122		CPU time spent (ms)=1710		Physical memory (bytes) snapshot=351088640		Virtual memory (bytes) snapshot=5554782208		Total committed heap usage (bytes)=335544320	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=817/03/30 15:27:55 INFO mapreduce.ImportJobBase: Transferred 8 bytes in 55.7994 seconds (0.1434 bytes/sec)17/03/30 15:27:55 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:27:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:27:55 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.41 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=4, numRows=0, totalSize=16, rawDataSize=0]OKTime taken: 0.92 secondst.user_tTable test.user_t stats: [numFiles=8, numRows=0, totalSize=24, rawDataSize=0]OKTime taken: 1.061 seconds
[15:28:29] INFO:    Now wait 5 seconds to begin next task ...
[15:28:34] INFO:    Connection channel disconnect
[15:28:34] INFO:    SSH connection shutdown

=============== [2017/03/30 15:32:05, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:32:05] INFO:    SSHExec initializing ...
[15:32:05] INFO:    SSHExec initializing ...
[15:32:05] INFO:    Session initialized and associated with user credential 123456
[15:32:05] INFO:    Session initialized and associated with user credential 123456
[15:32:05] INFO:    SSHExec initialized successfully
[15:32:05] INFO:    SSHExec initialized successfully
[15:32:05] INFO:    SSHExec trying to connect root@172.16.110.200
[15:32:05] INFO:    SSHExec initializing ...
[15:32:05] INFO:    SSHExec trying to connect root@172.16.110.200
[15:32:05] INFO:    Session initialized and associated with user credential 123456
[15:32:05] INFO:    SSHExec initialized successfully
[15:32:05] INFO:    SSHExec trying to connect root@172.16.110.200
[15:32:05] INFO:    SSH connection established
[15:32:05] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:32:05] INFO:    SSH connection established
[15:32:05] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:32:05] INFO:    Connection channel established succesfully
[15:32:05] INFO:    Start to run command
[15:32:05] INFO:    Connection channel established succesfully
[15:32:05] INFO:    Start to run command
[15:32:05] INFO:    SSH connection established
[15:32:05] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:32:05] INFO:    Connection channel established succesfully
[15:32:05] INFO:    Start to run command
[15:32:21] INFO:    Connection channel closed
[15:32:21] INFO:    Check if exec success or not ... 
[15:32:21] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:32:21] INFO:    Now wait 5 seconds to begin next task ...
[15:32:22] INFO:    Connection channel closed
[15:32:22] INFO:    Check if exec success or not ... 
[15:32:22] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:32:22] INFO:    Now wait 5 seconds to begin next task ...
[15:32:23] INFO:    Connection channel closed
[15:32:23] INFO:    Check if exec success or not ... 
[15:32:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:32:23] INFO:    Now wait 5 seconds to begin next task ...
[15:32:26] INFO:    Connection channel disconnect
[15:32:26] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:32:26] INFO:    Connection channel established succesfully
[15:32:26] INFO:    Start to run command
[15:32:26] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:32:27] INFO:    Connection channel disconnect
[15:32:27] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:32:28] INFO:    Connection channel established succesfully
[15:32:28] INFO:    Start to run command
[15:32:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:32:28] INFO:    Connection channel disconnect
[15:32:28] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:32:28] INFO:    Connection channel established succesfully
[15:32:28] INFO:    Start to run command
[15:32:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:33:16] INFO:    Connection channel closed
[15:33:16] INFO:    Check if exec success or not ... 
[15:33:16] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 4 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:33:16] INFO:    Error message: 17/03/30 15:32:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:32:27 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:32:27 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:32:27 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:32:27 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:32:27 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreducNoNote: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:32:33 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.jar17/03/30 15:32:33 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:32:33 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:32:33 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:32:33 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:32:33 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 15:32:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:32:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:32:36 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:32:50 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:32:50 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:32:50 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 17/03/30 15:32:51 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:32:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009817/03/30 15:32:52 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009817/03/30 15:32:53 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0098/17/03/30 15:32:53 INFO mapreduce.Job: Running job: job_1490408992134_009817/03/30 15:33:01 INFO mapreduce.Job: Job job_1490408992134_0097 running in uber mode : false17/03/30 15:33:01 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:33:10 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:33:13 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:33:13 INFO mapreduce.Job: Job job_1490408992134_0097 completed successfully17/03/30 15:33:13 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=10496		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=10496		Total vcore-seconds taken by all map tasks=10496		Total megabyte-seconds taken by all map tasks=10747904	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=280		CPU time spent (ms)=2330		Physical memory (bytes) snapshot=346202112		Virtual memory (bytes) snapshot=5557784576		Total committed heap usage (bytes)=341311488	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:33:13 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 40.0139 seconds (0.2499 bytes/sec)17/03/30 15:33:13 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:33:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:33:13 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesKTime taken: 5.098 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.13 seconds
[15:33:16] INFO:    Now wait 5 seconds to begin next task ...
[15:33:21] INFO:    Connection channel disconnect
[15:33:21] INFO:    SSH connection shutdown
[15:33:21] INFO:    Connection channel closed
[15:33:21] INFO:    Check if exec success or not ... 
[15:33:21] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 4 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:33:21] INFO:    Error message: 17/03/30 15:32:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:32:27 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:32:27 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:32:27 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:32:27 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:32:27 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreducNoNote: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:32:33 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.jar17/03/30 15:32:33 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:32:33 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:32:33 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:32:33 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:32:33 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 15:32:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:32:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:32:36 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:32:50 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:32:50 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:32:50 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 17/03/30 15:32:51 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:32:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009817/03/30 15:32:52 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009817/03/30 15:32:53 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0098/17/03/30 15:32:53 INFO mapreduce.Job: Running job: job_1490408992134_009817/03/30 15:33:16 INFO mapreduce.Job: Job job_1490408992134_0098 running in uber mode : false17/03/30 15:33:16 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:33:10 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:33:13 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:33:13 INFO mapreduce.Job: Job job_1490408992134_0097 completed successfully17/03/30 15:33:13 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=10496		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=10496		Total vcore-seconds taken by all map tasks=10496		Total megabyte-seconds taken by all map tasks=10747904	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=280		CPU time spent (ms)=2330		Physical memory (bytes) snapshot=346202112		Virtual memory (bytes) snapshot=5557784576		Total committed heap usage (bytes)=341311488	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:33:13 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 40.0139 seconds (0.2499 bytes/sec)17/03/30 15:33:13 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:33:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:33:13 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.803 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.356 seconds
[15:33:21] INFO:    Now wait 5 seconds to begin next task ...
[15:33:26] INFO:    Connection channel disconnect
[15:33:26] INFO:    SSH connection shutdown
[15:33:36] INFO:    Connection channel closed
[15:33:36] INFO:    Check if exec success or not ... 
[15:33:36] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 4 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:33:36] INFO:    Error message: 17/03/30 15:32:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:32:27 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:32:27 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:32:27 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:32:27 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:32:27 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:32:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreducNoNote: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:32:33 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/a9bdc0d1a68752b0e0544df74b67033a/TABLE_T.jar17/03/30 15:32:33 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:32:33 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:32:33 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:32:33 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:32:33 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 15:32:34 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job17/0317/03/30 15:32:36 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:32:36 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:17/0317/03/30 15:32:50 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:32:50 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:32:50 INFO db.IntegerSplitter: Split size: 0; Num splits: 4 from: 1 17/03/30 15:32:51 INFO mapreduce.JobSubmitter: number of splits:417/03/30 15:32:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_009817/03/30 15:32:52 INFO impl.YarnClientImpl: Submitted application application_1490408992134_009817/03/30 15:32:53 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0098/17/03/30 15:32:53 INFO mapreduce.Job: Running job: job_1490408992134_009817/03/30 15:33:16 INFO mapreduce.Job: Job job_1490408992134_0098 running in uber mode : false17/03/30 15:33:16 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:33:21 INFO mapreduce.Job:  map 25% reduce 0%17/03/30 15:33:26 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:33:27 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:33:28 INFO mapreduce.Job: Job job_1490408992134_0098 completed successfully17/03/30 15:33:28 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=574752		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=393		HDFS: Number of bytes written=12		HDFS: Number of read operations=16		HDFS: Number of large read operations=0		HDFS: Number of write operations=8	Job Counters 		Launched map tasks=4		Other local map tasks=4		Total time spent by all maps in occupied slots (ms)=20556		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=20556		Total vcore-seconds taken by all map tasks=20556		Total megabyte-seconds taken by all map tasks=21049344	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=393		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=265		CPU time spent (ms)=4530		Physical memory (bytes) snapshot=719269888		Virtual memory (bytes) snapshot=11119063040		Total committed heap usage (bytes)=718274560	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 15:33:28 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 52.6794 seconds (0.2278 bytes/sec)17/03/30 15:33:28 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 15:33:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 15:33:28 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.063 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=4, totalSize=12]OKTime taken: 1.812 seconds
[15:33:36] INFO:    Now wait 5 seconds to begin next task ...
[15:33:41] INFO:    Connection channel disconnect
[15:33:41] INFO:    SSH connection shutdown

=============== [2017/03/30 15:48:34, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:48:34] INFO:    SSHExec initializing ...
[15:48:34] INFO:    SSHExec initializing ...
[15:48:34] INFO:    Session initialized and associated with user credential 123456
[15:48:34] INFO:    SSHExec initialized successfully
[15:48:34] INFO:    SSHExec trying to connect root@172.16.110.200
[15:48:34] INFO:    Session initialized and associated with user credential 123456
[15:48:34] INFO:    SSHExec initialized successfully
[15:48:34] INFO:    SSHExec trying to connect root@172.16.110.200
[15:48:35] INFO:    SSH connection established
[15:48:35] INFO:    SSH connection established
[15:48:35] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:48:35] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:48:35] INFO:    Connection channel established succesfully
[15:48:35] INFO:    Connection channel established succesfully
[15:48:35] INFO:    Start to run command
[15:48:35] INFO:    Start to run command
[15:48:51] INFO:    Connection channel closed
[15:48:51] INFO:    Check if exec success or not ... 
[15:48:51] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:48:51] INFO:    Now wait 5 seconds to begin next task ...
[15:48:51] INFO:    Connection channel closed
[15:48:51] INFO:    Check if exec success or not ... 
[15:48:51] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:48:51] INFO:    Now wait 5 seconds to begin next task ...
[15:48:56] INFO:    Connection channel disconnect
[15:48:56] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:48:56] INFO:    Connection channel established succesfully
[15:48:56] INFO:    Start to run command
[15:48:56] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[15:48:56] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:48:56] INFO:    Connection channel disconnect
[15:48:56] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:48:56] INFO:    Connection channel established succesfully
[15:48:56] INFO:    Start to run command
[15:48:56] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:49:46] INFO:    Connection channel closed
[15:49:46] INFO:    Check if exec success or not ... 
[15:49:46] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:49:46] INFO:    Error message: 17/03/30 15:49:47 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:49:47 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:49:47 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:49:47 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:49:47 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:49:47 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:49:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:49:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:49:48 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/e19d5b74c8bba96f23b413a134878ccf/DEPT_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:49:52 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/e19d5b74c8bba96f23b413a134878ccf/DEPT_T.jar17/03/30 15:49:52 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:49:52 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:49:52 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:49:52 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:49:52 INFO mapreduce.ImportJobBase: Beginning import of DEPT_T17/03/30 15:49:52 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 15:49:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:49:54 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 15:50:05 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:50:05 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 15:50:05 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 15:50:05 INFO mapreduce.JobSubmitter: number of splits:217/03/30 15:50:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010017/03/30 15:50:06 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010017/03/30 15:50:06 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0100/17/03/30 15:50:06 INFO mapreduce.Job: Running job: job_1490408992134_010017/03/30 15:50:15 INFO mapreduce.Job: Job job_1490408992134_0101 running in uber mode : false17/03/30 15:50:15 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:50:25 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:50:30 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:50:31 INFO mapreduce.Job: Job job_1490408992134_0101 completed successfully17/03/30 15:50:31 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8878		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8878		Total vcore-seconds taken by all map tasks=8878		Total megabyte-seconds taken by all map tasks=9091072	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=146		CPU time spent (ms)=3000		Physical memory (bytes) snapshot=348102656		Virtual memory (bytes) snapshot=5561139200		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 15:50:31 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 37.6366 seconds (0.2657 bytes/sec)17/03/30 15:50:31 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:50:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 15:50:32 INFO hive.HiveImport: Loading uploaded data into HiveiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.519 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=2, totalSize=12]OKTime taken: 0.908 seconds
[15:49:46] INFO:    Now wait 5 seconds to begin next task ...
[15:49:51] INFO:    Connection channel disconnect
[15:49:51] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:49:51] INFO:    Connection channel established succesfully
[15:49:51] INFO:    Start to run command
[15:49:55] INFO:    Connection channel closed
[15:49:55] INFO:    Check if exec success or not ... 
[15:49:55] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:49:55] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  OKTime taken: 4.131 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 2.899 seconds
[15:49:55] INFO:    Now wait 5 seconds to begin next task ...
[15:50:00] INFO:    Connection channel disconnect
[15:50:00] INFO:    SSH connection shutdown
[15:50:11] INFO:    Connection channel closed
[15:50:11] INFO:    Check if exec success or not ... 
[15:50:11] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[15:50:11] INFO:    Now wait 5 seconds to begin next task ...
[15:50:16] INFO:    Connection channel disconnect
[15:50:16] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:50:16] INFO:    Connection channel established succesfully
[15:50:16] INFO:    Start to run command
[15:50:16] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:51:00] INFO:    Connection channel closed
[15:51:00] INFO:    Check if exec success or not ... 
[15:51:00] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:51:00] INFO:    Error message: 17/03/30 15:51:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 15:51:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 15:51:05 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 15:51:05 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 15:51:06 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 15:51:06 INFO tool.CodeGenTool: Beginning code generation17/03/30 15:51:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:51:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:51:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/32493375b81f19fd1276426a3138d4de/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 15:51:10 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/32493375b81f19fd1276426a3138d4de/TABLE_T.jar17/03/30 15:51:10 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 15:51:10 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 15:51:10 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 15:51:10 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 15:51:10 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 15:51:11 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 15:51:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 15:51:13 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 15:51:20 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 15:51:20 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 15:51:20 INFO db.IntegerSplitter: Split size: 0; Num splits: 2 from: 1 to: 217/03/30 15:51:20 INFO mapreduce.JobSubmitter: number of splits:217/03/30 15:51:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010217/03/30 15:51:21 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010217/03/30 15:51:22 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0102/17/03/30 15:51:22 INFO mapreduce.Job: Running job: job_1490408992134_010217/03/30 15:51:31 INFO mapreduce.Job: Job job_1490408992134_0102 running in uber mode : false17/03/30 15:51:31 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 15:51:37 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 15:51:38 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 15:51:38 INFO mapreduce.Job: Job job_1490408992134_0102 completed successfully17/03/30 15:51:39 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287388		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=8		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=7813		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=7813		Total vcore-seconds taken by all map tasks=7813		Total megabyte-seconds taken by all map tasks=8000512	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=116		CPU time spent (ms)=1650		Physical memory (bytes) snapshot=347267072		Virtual memory (bytes) snapshot=5559123968		Total committed heap usage (bytes)=333971456	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=817/03/30 15:51:39 INFO mapreduce.ImportJobBase: Transferred 8 bytes in 26.1651 seconds (0.3058 bytes/sec)17/03/30 15:51:39 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 15:51:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 15:51:39 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.752 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 0.909 seconds
[15:51:00] INFO:    Now wait 5 seconds to begin next task ...
[15:51:05] INFO:    Connection channel disconnect
[15:51:05] INFO:    SSH connection shutdown

=============== [2017/03/30 15:59:27, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:59:27] INFO:    SSHExec initializing ...
[15:59:27] INFO:    SSHExec initializing ...
[15:59:28] INFO:    Session initialized and associated with user credential 123456
[15:59:28] INFO:    Session initialized and associated with user credential 123456
[15:59:28] INFO:    SSHExec initialized successfully
[15:59:28] INFO:    SSHExec initialized successfully
[15:59:28] INFO:    SSHExec trying to connect root@172.16.110.200
[15:59:28] INFO:    SSHExec trying to connect root@172.16.110.200
[15:59:28] INFO:    SSH connection established
[15:59:28] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:59:28] INFO:    Connection channel established succesfully
[15:59:28] INFO:    Start to run command
[15:59:28] INFO:    SSH connection established
[15:59:28] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:59:28] INFO:    Connection channel established succesfully
[15:59:28] INFO:    Start to run command
[15:59:48] INFO:    Connection channel closed
[15:59:48] INFO:    Check if exec success or not ... 
[15:59:48] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[15:59:48] INFO:    Now wait 5 seconds to begin next task ...
[15:59:48] INFO:    Connection channel closed
[15:59:48] INFO:    Check if exec success or not ... 
[15:59:48] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[15:59:48] INFO:    Now wait 5 seconds to begin next task ...
[15:59:53] INFO:    Connection channel disconnect
[15:59:53] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:59:53] INFO:    Connection channel disconnect
[15:59:53] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[15:59:53] INFO:    Connection channel established succesfully
[15:59:53] INFO:    Start to run command
[15:59:53] INFO:    Connection channel established succesfully
[15:59:53] INFO:    Start to run command
[15:59:53] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[15:59:53] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[15:59:53] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:00:44] INFO:    Connection channel closed
[16:00:44] INFO:    Check if exec success or not ... 
[16:00:44] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:00:44] INFO:    Error message: 17/03/30 16:01:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:01:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:01:02 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:01:02 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:01:03 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:01:03 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:01:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:01:04 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:01:04 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/d290baaa447280cb958e41431b2670a7/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:01:07 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/584666c649e9a5b92dddc08102acf197/DEPT_T.jar17/03/30 16:01:07 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:01:07 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:01:07 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:01:07 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:01:07 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:01:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:01:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:01:10 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:01:22 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:01:22 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `DEPT_T`17/03/30 16:01:22 INFO db.IntegerSplitter: Split size: 0; Num splits: 2 from: 1 to: 217/03/30 16:01:22 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:01:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010317/03/30 16:01:24 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010317/03/30 16:01:24 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0103/17/03/30 16:01:24 INFO mapreduce.Job: Running job: job_1490408992134_010317/03/30 16:01:33 INFO mapreduce.Job: Job job_1490408992134_0104 running in uber mode : false17/03/30 16:01:33 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:01:42 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:01:46 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:01:47 INFO mapreduce.Job: Job job_1490408992134_0104 completed successfully17/03/30 16:01:47 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=12		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9057		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9057		Total vcore-seconds taken by all map tasks=9057		Total megabyte-seconds taken by all map tasks=9274368	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=125		CPU time spent (ms)=1970		Physical memory (bytes) snapshot=361754624		Virtual memory (bytes) snapshot=5560672256		Total committed heap usage (bytes)=358612992	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 16:01:47 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 37.6178 seconds (0.319 bytes/sec)17/03/30 16:01:47 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 16:01:48 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:01:48 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.135 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.461 seconds
[16:00:44] INFO:    Now wait 5 seconds to begin next task ...
[16:00:49] INFO:    Connection channel disconnect
[16:00:49] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:00:49] INFO:    Connection channel established succesfully
[16:00:49] INFO:    Start to run command
[16:00:51] INFO:    Connection channel closed
[16:00:51] INFO:    Check if exec success or not ... 
[16:00:51] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:00:51] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Time taken: 3.911 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=2, totalSize=12]OKTime taken: 1.377 seconds
[16:00:51] INFO:    Now wait 5 seconds to begin next task ...
[16:00:56] INFO:    Connection channel disconnect
[16:00:56] INFO:    SSH connection shutdown
[16:01:05] INFO:    Connection channel closed
[16:01:05] INFO:    Check if exec success or not ... 
[16:01:05] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:01:05] INFO:    Now wait 5 seconds to begin next task ...
[16:01:10] INFO:    Connection channel disconnect
[16:01:10] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:01:10] INFO:    Connection channel established succesfully
[16:01:10] INFO:    Start to run command
[16:01:10] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:01:55] INFO:    Connection channel closed
[16:01:55] INFO:    Check if exec success or not ... 
[16:01:55] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:01:55] INFO:    Error message: 17/03/30 16:02:19 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:02:19 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:02:19 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:02:19 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:02:20 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:02:20 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:02:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 16:02:21 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 16:02:21 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/bd06c8bfd5fe4c6138be994b1eb5ff30/TABLE_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:02:24 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/bd06c8bfd5fe4c6138be994b1eb5ff30/TABLE_T.jar17/03/30 16:02:24 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:02:24 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:02:24 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:02:24 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:02:24 INFO mapreduce.ImportJobBase: Beginning import of TABLE_T17/03/30 16:02:25 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:02:26 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:02:26 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:02:33 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:02:33 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `TABLE_T`17/03/30 16:02:33 INFO db.IntegerSplitter: Split size: 0; Num splits: 2 from: 1 to: 217/03/30 16:02:33 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:02:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010517/03/30 16:02:35 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010517/03/30 16:02:35 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0105/17/03/30 16:02:35 INFO mapreduce.Job: Running job: job_1490408992134_010517/03/30 16:02:44 INFO mapreduce.Job: Job job_1490408992134_0105 running in uber mode : false17/03/30 16:02:44 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:02:51 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:02:52 INFO mapreduce.Job: Job job_1490408992134_0105 completed successfully17/03/30 16:02:53 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287388		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=8		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=6842		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=6842		Total vcore-seconds taken by all map tasks=6842		Total megabyte-seconds taken by all map tasks=7006208	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=99		CPU time spent (ms)=1660		Physical memory (bytes) snapshot=363536384		Virtual memory (bytes) snapshot=5559345152		Total committed heap usage (bytes)=357040128	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=817/03/30 16:02:53 INFO mapreduce.ImportJobBase: Transferred 8 bytes in 26.5003 seconds (0.3019 bytes/sec)17/03/30 16:02:53 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 16:02:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/30 16:02:53 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.591 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.331 seconds
[16:01:55] INFO:    Now wait 5 seconds to begin next task ...
[16:02:00] INFO:    Connection channel disconnect
[16:02:00] INFO:    SSH connection shutdown

=============== [2017/03/30 16:23:36, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:23:36] INFO:    SSHExec initializing ...
[16:23:36] INFO:    SSHExec initializing ...
[16:23:36] INFO:    SSHExec initializing ...
[16:23:36] INFO:    Session initialized and associated with user credential 123456
[16:23:36] INFO:    Session initialized and associated with user credential 123456
[16:23:36] INFO:    Session initialized and associated with user credential 123456
[16:23:36] INFO:    SSHExec initialized successfully
[16:23:36] INFO:    SSHExec trying to connect root@172.16.110.200
[16:23:36] INFO:    SSHExec initialized successfully
[16:23:36] INFO:    SSHExec initialized successfully
[16:23:36] INFO:    SSHExec trying to connect root@172.16.110.200
[16:23:36] INFO:    SSHExec trying to connect root@172.16.110.200
[16:23:36] INFO:    SSH connection established
[16:23:36] INFO:    SSH connection established
[16:23:37] INFO:    SSH connection established
[16:23:37] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:23:37] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:23:37] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:23:37] INFO:    Connection channel established succesfully
[16:23:37] INFO:    Start to run command
[16:23:37] INFO:    Connection channel established succesfully
[16:23:37] INFO:    Start to run command
[16:23:37] INFO:    Connection channel established succesfully
[16:23:37] INFO:    Start to run command
[16:23:54] INFO:    Connection channel closed
[16:23:54] INFO:    Check if exec success or not ... 
[16:23:54] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:23:54] INFO:    Now wait 5 seconds to begin next task ...
[16:23:54] INFO:    Connection channel closed
[16:23:54] INFO:    Check if exec success or not ... 
[16:23:54] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:23:54] INFO:    Now wait 5 seconds to begin next task ...
[16:23:55] INFO:    Connection channel closed
[16:23:55] INFO:    Check if exec success or not ... 
[16:23:55] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:23:55] INFO:    Now wait 5 seconds to begin next task ...
[16:23:59] INFO:    Connection channel disconnect
[16:23:59] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:23:59] INFO:    Connection channel established succesfully
[16:23:59] INFO:    Start to run command
[16:23:59] INFO:    Connection channel disconnect
[16:23:59] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:24:00] INFO:    Connection channel established succesfully
[16:24:00] INFO:    Start to run command
[16:24:00] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:24:00] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[16:24:00] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:24:00] INFO:    Connection channel disconnect
[16:24:00] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:24:00] INFO:    Connection channel established succesfully
[16:24:00] INFO:    Start to run command
[16:24:01] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:24:54] INFO:    Connection channel closed
[16:24:54] INFO:    Check if exec success or not ... 
[16:24:54] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:24:54] INFO:    Error message: 17/03/30 16:25:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:25:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:25:23 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:25:23 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:25:24 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:25:24 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:25:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.jar17/03/30 16:25:29 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:25:29 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:25:29 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:25:29 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:25:29 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:25:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:25:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:25:33 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:25:47 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:25:47 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 16:25:47 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 16:25:47 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:25:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010817/03/30 16:25:48 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010817/03/30 16:25:48 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0108/17/03/30 16:25:48 INFO mapreduce.Job: Running job: job_1490408992134_010817/03/30 16:25:56 INFO mapreduce.Job: Job job_1490408992134_0106 running in uber mode : false17/03/30 16:25:56 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:26:06 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:26:09 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:26:10 INFO mapreduce.Job: Job job_1490408992134_0106 completed successfully17/03/30 16:26:10 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9883		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9883		Total vcore-seconds taken by all map tasks=9883		Total megabyte-seconds taken by all map tasks=10120192	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=136		CPU time spent (ms)=1890		Physical memory (bytes) snapshot=346435584		Virtual memory (bytes) snapshot=5558587392		Total committed heap usage (bytes)=333971456	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 16:26:10 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 38.5291 seconds (0.2595 bytes/sec)17/03/30 16:26:10 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 16:26:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:26:10 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesesOKTime taken: 4.08 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.875 seconds
[16:24:54] INFO:    Now wait 5 seconds to begin next task ...
[16:24:59] INFO:    Connection channel disconnect
[16:24:59] INFO:    SSH connection shutdown
[16:24:59] INFO:    Connection channel closed
[16:24:59] INFO:    Check if exec success or not ... 
[16:24:59] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:24:59] INFO:    Error message: 17/03/30 16:25:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:25:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:25:23 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:25:23 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:25:24 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:25:24 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:25:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.jar17/03/30 16:25:29 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:25:29 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:25:29 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:25:29 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:25:29 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:25:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:25:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:25:33 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:25:47 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:25:47 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 16:25:47 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 16:25:47 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:25:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010817/03/30 16:25:48 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010817/03/30 16:25:48 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0108/17/03/30 16:25:48 INFO mapreduce.Job: Running job: job_1490408992134_010817/03/30 16:26:14 INFO mapreduce.Job: Job job_1490408992134_0108 running in uber mode : false17/03/30 16:26:14 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:26:06 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:26:09 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:26:10 INFO mapreduce.Job: Job job_1490408992134_0106 completed successfully17/03/30 16:26:10 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=10		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9883		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9883		Total vcore-seconds taken by all map tasks=9883		Total megabyte-seconds taken by all map tasks=10120192	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=136		CPU time spent (ms)=1890		Physical memory (bytes) snapshot=346435584		Virtual memory (bytes) snapshot=5558587392		Total committed heap usage (bytes)=333971456	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1017/03/30 16:26:10 INFO mapreduce.ImportJobBase: Transferred 10 bytes in 38.5291 seconds (0.2595 bytes/sec)17/03/30 16:26:10 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/30 16:26:10 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:26:10 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.351 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.495 secondsds
[16:24:59] INFO:    Now wait 5 seconds to begin next task ...
[16:25:04] INFO:    Connection channel disconnect
[16:25:04] INFO:    SSH connection shutdown
[16:25:13] INFO:    Connection channel closed
[16:25:13] INFO:    Check if exec success or not ... 
[16:25:13] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:25:13] INFO:    Error message: 17/03/30 16:25:23 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:25:23 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:25:23 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:25:23 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:25:24 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:25:24 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:25:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:25:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/dec9842895ff07c34f49f4cb3ac602af/USER_T.jar17/03/30 16:25:29 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:25:29 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:25:29 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:25:29 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:25:29 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:25:30 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:25:32 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:25:33 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:25:47 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:25:47 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 16:25:47 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 16:25:47 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:25:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_010817/03/30 16:25:48 INFO impl.YarnClientImpl: Submitted application application_1490408992134_010817/03/30 16:25:48 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0108/17/03/30 16:25:48 INFO mapreduce.Job: Running job: job_1490408992134_010817/03/30 16:26:14 INFO mapreduce.Job: Job job_1490408992134_0108 running in uber mode : false17/03/30 16:26:14 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:26:19 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:26:22 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:26:24 INFO mapreduce.Job: Job job_1490408992134_0108 completed successfully17/03/30 16:26:24 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=12		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8272		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8272		Total vcore-seconds taken by all map tasks=8272		Total megabyte-seconds taken by all map tasks=8470528	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=131		CPU time spent (ms)=1860		Physical memory (bytes) snapshot=345706496		Virtual memory (bytes) snapshot=5558636544		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 16:26:24 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 51.6521 seconds (0.2323 bytes/sec)17/03/30 16:26:24 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 16:26:24 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:26:24 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.782 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=2, totalSize=12]OKTime taken: 0.841 secondsds
[16:25:13] INFO:    Now wait 5 seconds to begin next task ...
[16:25:18] INFO:    Connection channel disconnect
[16:25:18] INFO:    SSH connection shutdown

=============== [2017/03/30 16:30:13, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:30:13] INFO:    SSHExec initializing ...
[16:30:13] INFO:    SSHExec initializing ...
[16:30:13] INFO:    SSHExec initializing ...
[16:30:13] INFO:    SSHExec initializing ...
[16:30:13] INFO:    Session initialized and associated with user credential 123456
[16:30:13] INFO:    Session initialized and associated with user credential 123456
[16:30:13] INFO:    Session initialized and associated with user credential 123456
[16:30:13] INFO:    SSHExec initialized successfully
[16:30:13] INFO:    SSHExec trying to connect root@172.16.110.200
[16:30:13] INFO:    Session initialized and associated with user credential 123456
[16:30:13] INFO:    SSHExec initialized successfully
[16:30:13] INFO:    SSHExec initialized successfully
[16:30:13] INFO:    SSHExec trying to connect root@172.16.110.200
[16:30:13] INFO:    SSHExec initialized successfully
[16:30:13] INFO:    SSHExec trying to connect root@172.16.110.200
[16:30:13] INFO:    SSHExec trying to connect root@172.16.110.200
[16:30:14] INFO:    SSH connection established
[16:30:14] INFO:    SSH connection established
[16:30:14] INFO:    SSH connection established
[16:30:14] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_AREA;'
[16:30:14] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE;'
[16:30:14] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL;'
[16:30:14] INFO:    SSH connection established
[16:30:14] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:30:14] INFO:    Connection channel established succesfully
[16:30:14] INFO:    Connection channel established succesfully
[16:30:14] INFO:    Start to run command
[16:30:14] INFO:    Connection channel established succesfully
[16:30:14] INFO:    Start to run command
[16:30:14] INFO:    Start to run command
[16:30:14] INFO:    Connection channel established succesfully
[16:30:14] INFO:    Start to run command
[16:30:38] INFO:    Connection channel closed
[16:30:38] INFO:    Check if exec success or not ... 
[16:30:38] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE;'
[16:30:38] INFO:    Now wait 5 seconds to begin next task ...
[16:30:38] INFO:    Connection channel closed
[16:30:38] INFO:    Check if exec success or not ... 
[16:30:38] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_AREA;'
[16:30:38] INFO:    Now wait 5 seconds to begin next task ...
[16:30:41] INFO:    Connection channel closed
[16:30:41] INFO:    Check if exec success or not ... 
[16:30:41] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL;'
[16:30:41] INFO:    Now wait 5 seconds to begin next task ...
[16:30:41] INFO:    Connection channel closed
[16:30:41] INFO:    Check if exec success or not ... 
[16:30:41] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:30:41] INFO:    Now wait 5 seconds to begin next task ...
[16:30:45] INFO:    Connection channel disconnect
[16:30:45] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE -m 2 --hive-table FANWE_DEAL_CATE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:30:45] INFO:    Connection channel established succesfully
[16:30:45] INFO:    Start to run command
[16:30:45] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:30:45] INFO:    Connection channel disconnect
[16:30:45] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_AREA -m 2 --hive-table FANWE_AREA --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:30:45] INFO:    Connection channel established succesfully
[16:30:45] INFO:    Start to run command
[16:30:45] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:30:47] INFO:    Connection channel disconnect
[16:30:47] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL -m 2 --hive-table FANWE_DEAL --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:30:47] INFO:    Connection channel disconnect
[16:30:47] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:30:47] INFO:    Connection channel established succesfully
[16:30:47] INFO:    Start to run command
[16:30:47] INFO:    Connection channel established succesfully
[16:30:47] INFO:    Start to run command
[16:30:47] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:30:47] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:31:57] INFO:    Connection channel closed
[16:31:57] INFO:    Check if exec success or not ... 
[16:31:57] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_AREA -m 2 --hive-table FANWE_AREA --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:31:57] INFO:    Error message: 17/03/30 16:31:59 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:31:59 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:31:59 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:31:59 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:32:00 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:32:00 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:32:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:32:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `DEPT_T` AS t LIMIT 117/03/30 16:32:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tNote: /tmp/sqoop-hdfs/compile/2085e8f5e8d675d2e49399177fc0f4ea/FANWE_DEAL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:32:11 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/2085e8f5e8d675d2e49399177fc0f4ea/FANWE_DEAL.jar17/03/30 16:32:11 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:32:11 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:32:11 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:32:11 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:32:11 INFO mapreduce.ImportJobBase: Beginning import of FANWE_DEAL17/03/30 16:32:12 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, us17/03/30 16:32:12 IN17/03/30 16:32:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:32:13 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/17/03/30 16:32:29 IN17/03/30 16:32:30 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:32:30 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `FANWE_DEAL`17/03/30 16:32:30 INFO db.IntegerSplitter: Split size: 54; Num splits: 2 from: 57 to: 16617/03/30 16:32:30 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:32:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_011217/03/30 16:32:32 INFO impl.YarnClientImpl: Submitted application application_1490408992134_011217/03/30 16:32:32 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0112/17/03/30 16:32:32 INFO mapreduce.Job: Running job: job_1490408992134_011217/03/30 16:32:34 INFO mapred17/03/30 16:32:40 INFO mapreduce.Job: Job job_1490408992134_0109 running in uber mode : false17/03/30 16:32:40 INFO mapr17/03/30 16:32:42 INFO mapred17/03/30 16:32:46 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:32:50 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:32:53 INFO mapreduce.Job: Job job_1490408992134_0109 completed successfully17/03/30 16:32:53 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287794		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=201		HDFS: Number of bytes written=726		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9304		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9304		Total vcore-seconds taken by all map tasks=9304		Total megabyte-seconds taken by all map tasks=9527296	Map-Reduce Framework		Map input records=6		Map output records=6		Input split bytes=201		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=131		CPU time spent (ms)=1800		Physical memory (bytes) snapshot=344449024		Virtual memory (bytes) snapshot=5558411264		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=72617/03/30 16:32:53 INFO mapreduce.ImportJobBase: Transferred 726 bytes in 47.9516 seconds (15.1403 bytes/sec)17/03/30 16:32:53 INFO mapreduce.ImportJobBase: Retrieved 6 records.17/03/30 16:32:53 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE` AS t LIMIT 117/03/30 16:32:53 INFO hive.HiveImport: Loading uploaded data into Hiveonfiguration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.645 secondsLoading data to table test.fanwe_areaTable test.fanwe_area stats: [numFiles=2, totalSize=341]OKTime taken: 0.966 seconds
[16:31:57] INFO:    Now wait 5 seconds to begin next task ...
[16:32:03] INFO:    Connection channel disconnect
[16:32:03] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE;'
[16:32:03] INFO:    Connection channel established succesfully
[16:32:03] INFO:    Start to run command
[16:32:10] INFO:    Connection channel closed
[16:32:10] INFO:    Check if exec success or not ... 
[16:32:10] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE -m 2 --hive-table FANWE_DEAL_CATE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:32:10] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         17/03/30 16:33:03 INFO mapreduce.Job:  map 50% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OKTime taken: 5.382 secondsLoading data to table test.fanwe_deal_cateTable test.fanwe_deal_cate stats: [numFiles=2, totalSize=726]OKTime taken: 1.411 seconds
[16:32:10] INFO:    Now wait 5 seconds to begin next task ...
[16:32:16] INFO:    Connection channel disconnect
[16:32:16] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_DEAL_LINK;'
[16:32:16] INFO:    Connection channel established succesfully
[16:32:16] INFO:    Start to run command
[16:32:23] INFO:    Connection channel closed
[16:32:23] INFO:    Check if exec success or not ... 
[16:32:23] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:32:23] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                 Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.364 seconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      17/03/30 16:33:15 INFO mapreduce.Job:  map 50% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OKTime taken: 3.369 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=2, totalSize=10]OKTime taken: 1.823 seconds
[16:32:23] INFO:    Now wait 5 seconds to begin next task ...
[16:32:24] INFO:    Connection channel closed
[16:32:24] INFO:    Check if exec success or not ... 
[16:32:24] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE;'
[16:32:24] INFO:    Now wait 5 seconds to begin next task ...
[16:32:30] INFO:    Connection channel disconnect
[16:32:30] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LINK;'
[16:32:30] INFO:    Connection channel established succesfully
[16:32:30] INFO:    Start to run command
[16:32:30] INFO:    Connection channel disconnect
[16:32:30] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE -m 2 --hive-table FANWE_DEAL_CATE_TYPE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:32:31] INFO:    Connection channel established succesfully
[16:32:31] INFO:    Start to run command
[16:32:32] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:32:42] INFO:    Connection channel closed
[16:32:42] INFO:    Check if exec success or not ... 
[16:32:42] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL -m 2 --hive-table FANWE_DEAL --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:32:42] INFO:    Error message: 17/03/30 16:33:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:33:32 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:33:32 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:33:32 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.                                                                                                 Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.properties                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                OKTime taken: 5.462 secondsLoading data to table test.fanwe_dealTable test.fanwe_deal stats: [numFiles=2, totalSize=167920]OKTime taken: 2.915 seconds
[16:32:42] INFO:    Now wait 5 seconds to begin next task ...
[16:32:44] INFO:    Connection channel closed
[16:32:44] INFO:    Check if exec success or not ... 
[16:32:44] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_DEAL_LINK;'
[16:32:44] INFO:    Now wait 5 seconds to begin next task ...
[16:32:48] INFO:    Connection channel disconnect
[16:32:48] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LOCATION_LINK;'
[16:32:48] INFO:    Connection channel established succesfully
[16:32:48] INFO:    Start to run command
[16:32:50] INFO:    Connection channel disconnect
[16:32:50] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_DEAL_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_DEAL_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:32:50] INFO:    Connection channel established succesfully
[16:32:50] INFO:    Start to run command
[16:32:51] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:33:01] INFO:    Connection channel closed
[16:33:01] INFO:    Check if exec success or not ... 
[16:33:01] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LINK;'
[16:33:01] INFO:    Now wait 5 seconds to begin next task ...
[16:33:08] INFO:    Connection channel disconnect
[16:33:08] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:33:08] INFO:    Connection channel established succesfully
[16:33:08] INFO:    Start to run command
[16:33:08] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[16:33:08] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:33:23] INFO:    Connection channel closed
[16:33:23] INFO:    Check if exec success or not ... 
[16:33:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LOCATION_LINK;'
[16:33:23] INFO:    Now wait 5 seconds to begin next task ...
[16:33:29] INFO:    Connection channel disconnect
[16:33:29] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LOCATION_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LOCATION_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:33:29] INFO:    Connection channel established succesfully
[16:33:29] INFO:    Start to run command
[16:33:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:34:01] INFO:    Connection channel closed
[16:34:01] INFO:    Check if exec success or not ... 
[16:34:01] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE -m 2 --hive-table FANWE_DEAL_CATE_TYPE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:01] INFO:    Error message: 17/03/30 16:34:18 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:34:18 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:34:18 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:34:18 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:34:19 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:34:19 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:34:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK` AS t LIMIT 117/03/30 16:34:20 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK` AS t LIMIT 117/03/30 16:34:21 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/c16ece2221d6a8d376a73a7ad4ae2f95/FANWE_DEAL_CATE_TYPE_LOCATION_LINK.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:34:24 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/c16ece2221d6a8d376a73a7ad4ae2f95/FANWE_DEAL_CATE_TYPE_LOCATION_LINK.jar17/03/30 16:34:24 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:34:24 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:34:24 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:34:24 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:34:24 WARN manager.CatalogQueryManager: The table FANWE_DEAL_CATE_TYPE_LOCATION_LINK contains a multi-column primary key. Sqoop will default to the column location_id only for this job.17/03/30 16:34:24 WARN manager.CatalogQueryManager: The table FANWE_DEAL_CATE_TYPE_LOCATION_LINK contains a multi-column primary key. Sqoop will default to the column location_id only for this job.17/03/30 16:34:24 INFO mapreduce.ImportJobBase: Beginning import of FANWE_DEAL_CATE_TYPE_LOCATION_LINK17/03/30 16:34:27 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:34:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:34:29 INFO client.RMProxy:17/03/30 16:34:33 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:34:33 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`cate_id`), MAX(`cate_id`) FROM `FANWE_DEAL_CATE_TYPE_LINK`17/03/30 16:34:33 INFO db.IntegerSplitter: Split size: 2; Num splits: 2 from: 13 to: 1817/03/30 16:34:33 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:34:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_011517/03/30 16:34:36 INFO impl.YarnClientImpl: Submitted application application_1490408992134_011517/03/30 16:34:36 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0115/17/03/30 16:34:36 INFO mapreduce.Job: Running job: job_1490408992134_0115mber of bytes written=287588		FILE: Numbe17/03/30 16:34:36 INFO mapreduce.Job: Job job_1490408992134_0114 running in uber mode : false17/03/30 16:34:36 INFO mapreduce.Job:  map 0% reduce 0%of bytes written=658		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=11231		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=11231		Total vcore-seconds taken by all map tasks=11231		Total megabyte-seconds taken by all map tasks=11500544	Map-Reduce Framework		Map input records=34		Map output records=34		Input split bytes=200		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=121		CPU time spent (ms)=1770		Physical memory (bytes) snapshot=362889216		Virtual memory (bytes) snapshot=5558362112		Total committed heap usage (bytes)=361234432	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=65817/03/30 16:34:30 INFO mapreduce.ImportJobBase: Transferred 658 bytes in 46.8108 seconds (14.0566 bytes/sec)17/03/30 16:34:30 INFO mapreduce.ImportJobBase: Retrieved 34 records.17/03/30 16:34:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE` AS t LIMIT 117/03/30 16:34:31 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 4.323 secondsLoading data to table test.fanwe_deal_cate_typeTable test.fanwe_deal_cate_type stats: [numFiles=2, totalSize=658]OKTime taken: 2.201 seconds
[16:34:01] INFO:    Now wait 5 seconds to begin next task ...
[16:34:07] INFO:    Connection channel disconnect
[16:34:07] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:34:07] INFO:    Connection channel established succesfully
[16:34:07] INFO:    Start to run command
[16:34:20] INFO:    Connection channel closed
[16:34:20] INFO:    Check if exec success or not ... 
[16:34:20] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_DEAL_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_DEAL_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:20] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/30 16:34:58 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              17/03/30 16:34:51 INFO impl.YarnClientImpl: Submitted application application_1490408992134_011617/03/30 16:34:51 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0116/17/03/30 16:34:51 INFO mapreduce.Job: Running job: job_1490408992134_0116                                                            17/03/30 16:34:52 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:34:58 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:35:00 INFO mapreduce.Job: Job job_1490408992134_0115 completed successfully17/03/30 16:35:00 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287650		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=221		HDFS: Number of bytes written=264		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=12687		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=12687		Total vcore-seconds taken by all map tasks=12687		Total megabyte-seconds taken by all map tasks=12991488	Map-Reduce Framework		Map input records=44		Map output records=44		Input split bytes=221		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=144		CPU time spent (ms)=2360		Physical memory (bytes) snapshot=349597696		Virtual memory (bytes) snapshot=5560987648		Total committed heap usage (bytes)=335544320	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=26417/03/30 16:35:00 INFO mapreduce.ImportJobBase: Transferred 264 bytes in 47.6989 seconds (5.5347 bytes/sec)17/03/30 16:35:00 INFO mapreduce.ImportJobBase: Retrieved 44 records.17/03/30 16:35:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LINK` AS t LIMIT 117/03/30 16:35:00 INFO hive.HiveImport: Loading uploaded data into Hiveve.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 5.384 secondsLoading data to table test.fanwe_deal_cate_type_deal_linkTable test.fanwe_deal_cate_type_deal_link stats: [numFiles=2, totalSize=256]OKTime taken: 1.991 seconds
[16:34:20] INFO:    Now wait 5 seconds to begin next task ...
[16:34:26] INFO:    Connection channel disconnect
[16:34:26] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:34:26] INFO:    Connection channel established succesfully
[16:34:26] INFO:    Start to run command
[16:34:32] INFO:    Connection channel closed
[16:34:32] INFO:    Check if exec success or not ... 
[16:34:32] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:32] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                                                                                                                                                                                  OKTime taken: 3.21 seconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     17/03/30 16:35:10 INFO mapreduce.Job:  map 50% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     OKTime taken: 5.242 secondsLoading data to table test.fanwe_deal_cate_type_linkTable test.fanwe_deal_cate_type_link stats: [numFiles=2, totalSize=264]OKTime taken: 1.709 seconds
[16:34:32] INFO:    Now wait 5 seconds to begin next task ...
[16:34:33] INFO:    Connection channel closed
[16:34:33] INFO:    Check if exec success or not ... 
[16:34:33] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:34:33] INFO:    Now wait 5 seconds to begin next task ...
[16:34:39] INFO:    Connection channel disconnect
[16:34:39] INFO:    SSH connection shutdown
[16:34:39] INFO:    Connection channel disconnect
[16:34:39] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:39] INFO:    Connection channel established succesfully
[16:34:39] INFO:    Start to run command
[16:34:40] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:34:46] INFO:    Connection channel closed
[16:34:46] INFO:    Check if exec success or not ... 
[16:34:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LOCATION_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LOCATION_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:47] INFO:    Error message: 17/03/30 16:35:22 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:35:22 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:35:22 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:35:22 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:35:23 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:35:23 INFO tool.CodeGenTool: Beginning code generationudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.properties                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   OKTime taken: 3.705 secondsLoading data to table test.fanwe_deal_cate_type_location_linkTable test.fanwe_deal_cate_type_location_link stats: [numFiles=2, totalSize=618]OKTime taken: 1.816 seconds
[16:34:47] INFO:    Now wait 5 seconds to begin next task ...
[16:34:49] INFO:    Connection channel closed
[16:34:49] INFO:    Check if exec success or not ... 
[16:34:49] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:34:49] INFO:    Now wait 5 seconds to begin next task ...
[16:34:53] INFO:    Connection channel disconnect
[16:34:53] INFO:    SSH connection shutdown
[16:34:56] INFO:    Connection channel disconnect
[16:34:56] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:34:56] INFO:    Connection channel established succesfully
[16:34:56] INFO:    Start to run command
[16:34:56] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:35:47] INFO:    Connection channel closed
[16:35:47] INFO:    Check if exec success or not ... 
[16:35:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:35:47] INFO:    Error message: 17/03/30 16:35:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:35:35 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:35:35 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:35:35 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:35:35 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:35:35 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:35:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:35:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:35:37 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/0bef18b4cf2d1c31881d95a94c0c6b40/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:35:40 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/0bef18b4cf2d1c31881d95a94c0c6b40/USER_T.jar17/03/30 16:35:40 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:35:40 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:35:40 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:35:40 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:35:40 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:35:41 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:35:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:35:43 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:35:54 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:35:54 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 16:35:54 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 16:35:54 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:35:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_011817/03/30 16:35:57 INFO impl.YarnClientImpl: Submitted application application_1490408992134_011817/03/30 16:35:57 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0118/17/03/30 16:35:57 INFO mapreduce.Job: Running job: job_1490408992134_011817/03/30 16:36:04 INFO mapreduce.Job: Job job_1490408992134_0118 running in uber mode : false17/03/30 16:36:04 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:36:11 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:36:16 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:36:17 INFO mapreduce.Job: Job job_1490408992134_0118 completed successfully17/03/30 16:36:17 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=12		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=10012		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=10012		Total vcore-seconds taken by all map tasks=10012		Total megabyte-seconds taken by all map tasks=10252288	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=126		CPU time spent (ms)=1870		Physical memory (bytes) snapshot=362700800		Virtual memory (bytes) snapshot=5558943744		Total committed heap usage (bytes)=359137280	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 16:36:17 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 34.2018 seconds (0.3509 bytes/sec)17/03/30 16:36:17 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 16:36:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:36:18 INFO hive.HiveImport: Loading uploaded data into HiveiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.393 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=2, totalSize=8]OKTime taken: 1.561 seconds
[16:35:47] INFO:    Now wait 5 seconds to begin next task ...
[16:35:52] INFO:    Connection channel disconnect
[16:35:52] INFO:    SSH connection shutdown
[16:35:56] INFO:    Connection channel closed
[16:35:56] INFO:    Check if exec success or not ... 
[16:35:56] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:35:56] INFO:    Error message: 17/03/30 16:35:35 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/30 16:35:35 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/30 16:35:35 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/30 16:35:35 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/30 16:35:35 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/30 16:35:35 INFO tool.CodeGenTool: Beginning code generation17/03/30 16:35:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:35:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:35:37 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/0bef18b4cf2d1c31881d95a94c0c6b40/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/30 16:35:40 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/0bef18b4cf2d1c31881d95a94c0c6b40/USER_T.jar17/03/30 16:35:40 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/30 16:35:40 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/30 16:35:40 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/30 16:35:40 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/30 16:35:40 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/30 16:35:41 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/30 16:35:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/30 16:35:43 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/30 16:35:54 INFO db.DBInputFormat: Using read commited transaction isolation17/03/30 16:35:54 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/30 16:35:54 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/30 16:35:54 INFO mapreduce.JobSubmitter: number of splits:217/03/30 16:35:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_011817/03/30 16:35:57 INFO impl.YarnClientImpl: Submitted application application_1490408992134_011817/03/30 16:35:57 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0118/17/03/30 16:35:57 INFO mapreduce.Job: Running job: job_1490408992134_011817/03/30 16:36:04 INFO mapreduce.Job: Job job_1490408992134_0118 running in uber mode : false17/03/30 16:36:04 INFO mapreduce.Job:  map 0% reduce 0%17/03/30 16:36:11 INFO mapreduce.Job:  map 50% reduce 0%17/03/30 16:36:16 INFO mapreduce.Job:  map 100% reduce 0%17/03/30 16:36:17 INFO mapreduce.Job: Job job_1490408992134_0118 completed successfully17/03/30 16:36:17 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=12		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=10012		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=10012		Total vcore-seconds taken by all map tasks=10012		Total megabyte-seconds taken by all map tasks=10252288	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=126		CPU time spent (ms)=1870		Physical memory (bytes) snapshot=362700800		Virtual memory (bytes) snapshot=5558943744		Total committed heap usage (bytes)=359137280	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/30 16:36:17 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 34.2018 seconds (0.3509 bytes/sec)17/03/30 16:36:17 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/30 16:36:17 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/30 16:36:18 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.921 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=2, totalSize=12]OKTime taken: 2.227 secondsonds
[16:35:56] INFO:    Now wait 5 seconds to begin next task ...
[16:36:01] INFO:    Connection channel disconnect
[16:36:01] INFO:    SSH connection shutdown

=============== [2017/03/31 16:39:01, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:39:02] INFO:    SSHExec initializing ...
[16:39:02] INFO:    SSHExec initializing ...
[16:39:02] INFO:    SSHExec initializing ...
[16:39:02] INFO:    SSHExec initializing ...
[16:39:02] INFO:    Session initialized and associated with user credential 123456
[16:39:02] INFO:    Session initialized and associated with user credential 123456
[16:39:02] INFO:    Session initialized and associated with user credential 123456
[16:39:02] INFO:    Session initialized and associated with user credential 123456
[16:39:02] INFO:    SSHExec initialized successfully
[16:39:02] INFO:    SSHExec initialized successfully
[16:39:02] INFO:    SSHExec initialized successfully
[16:39:02] INFO:    SSHExec trying to connect root@172.16.110.200
[16:39:02] INFO:    SSHExec trying to connect root@172.16.110.200
[16:39:02] INFO:    SSHExec initialized successfully
[16:39:02] INFO:    SSHExec trying to connect root@172.16.110.200
[16:39:02] INFO:    SSHExec trying to connect root@172.16.110.200
[16:39:02] INFO:    SSH connection established
[16:39:02] INFO:    SSH connection established
[16:39:02] INFO:    SSH connection established
[16:39:02] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:39:02] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL;'
[16:39:02] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE;'
[16:39:02] INFO:    SSH connection established
[16:39:02] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_AREA;'
[16:39:03] INFO:    Connection channel established succesfully
[16:39:03] INFO:    Connection channel established succesfully
[16:39:03] INFO:    Connection channel established succesfully
[16:39:03] INFO:    Start to run command
[16:39:03] INFO:    Start to run command
[16:39:03] INFO:    Start to run command
[16:39:03] INFO:    Connection channel established succesfully
[16:39:03] INFO:    Start to run command
[16:39:23] INFO:    Connection channel closed
[16:39:23] INFO:    Check if exec success or not ... 
[16:39:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.DEPT_T;'
[16:39:23] INFO:    Now wait 5 seconds to begin next task ...
[16:39:23] INFO:    Connection channel closed
[16:39:23] INFO:    Check if exec success or not ... 
[16:39:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL;'
[16:39:23] INFO:    Now wait 5 seconds to begin next task ...
[16:39:23] INFO:    Connection channel closed
[16:39:23] INFO:    Check if exec success or not ... 
[16:39:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE;'
[16:39:23] INFO:    Now wait 5 seconds to begin next task ...
[16:39:24] INFO:    Connection channel closed
[16:39:24] INFO:    Check if exec success or not ... 
[16:39:24] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_AREA;'
[16:39:24] INFO:    Now wait 5 seconds to begin next task ...
[16:39:28] INFO:    Connection channel disconnect
[16:39:28] INFO:    Command is sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:39:28] INFO:    Connection channel established succesfully
[16:39:28] INFO:    Start to run command
[16:39:28] INFO:    Connection channel disconnect
[16:39:28] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL -m 2 --hive-table FANWE_DEAL --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:39:28] INFO:    Connection channel established succesfully
[16:39:28] INFO:    Start to run command
[16:39:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[16:39:28] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:39:28] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:39:28] INFO:    Connection channel disconnect
[16:39:28] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE -m 2 --hive-table FANWE_DEAL_CATE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:39:28] INFO:    Connection channel established succesfully
[16:39:28] INFO:    Start to run command
[16:39:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:39:29] INFO:    Connection channel disconnect
[16:39:29] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_AREA -m 2 --hive-table FANWE_AREA --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:39:29] INFO:    Connection channel established succesfully
[16:39:29] INFO:    Start to run command
[16:39:30] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.

[16:39:30] INFO:    Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:40:25] INFO:    Connection channel closed
[16:40:25] INFO:    Check if exec success or not ... 
[16:40:25] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE -m 2 --hive-table FANWE_DEAL_CATE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:40:25] INFO:    Error message: 17/03/31 16:39:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:39:54 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:39:54 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/31 16:39:54 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/31 16:39:54 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/31 16:39:54 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:39:55 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_AREA` AS t LIMIT 117/03/31 16:39:56 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_AREA` AS t LIMIT 117/03/31 16:39:56 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-maNote: /tNote: /tmp/sqoop-hdfs/compile/e443a51228ed022b2736dacc4019e5af/FANWE_DEAL.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/31 16:40:05 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/e443a51228ed022b2736dacc4019e5af/FANWE_DEAL.jar17/03/31 16:40:05 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/31 16:40:05 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/31 16:40:05 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/31 16:40:05 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/31 16:40:05 INFO mapreduce.ImportJobBase: Beginning import of FANWE_DEAL17/03/31 16:40:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/31 16:40:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/31 16:40:08 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/17/03/31 16:40:21 IN17/03/31 16:40:23 INFO db.DBInputFormat: Using read commited transaction isolation17/03/31 16:40:23 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `FANWE_DEAL`17/03/31 16:40:23 INFO db.IntegerSplitter: Split size: 54; Num splits: 2 from: 57 to: 16617/03/31 16:40:24 INFO mapreduce.JobSubmitter: number of splits:217/03/31 16:40:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_012317/03/31 16:40:25 INFO impl.YarnClientImpl: Submitted application application_1490408992134_012317/03/31 16:40:25 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0123/17/03/31 16:40:25 INFO mapreduce.Job: Running 17/03/31 16:40:33 INFO mapreduce.Job: Job job_1490408992134_0120 running in uber mode : false17/03/31 16:40:33 INFO mapreduce.Job:  map 0% reduce 0%17/03/31 16:40:41 INFO mapreduce.Job:  map 50% reduce 0%7/03/31 16:40:36 INFO mapreduce.Job:  map 50% reduce 0%17/03/31 16:40:40 INFO mapreduce.Job:  map 100% reduce 0%17/03/31 16:40:40 INFO mapreduce.Job: Job job_1490408992134_0122 completed successfully17/03/31 16:40:40 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287794		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=201		HDFS: Number of bytes written=726		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=10595		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=10595		Total vcore-seconds taken by all map tasks=10595		Total megabyte-seconds taken by all map tasks=10849280	Map-Reduce Framework		Map input records=6		Map output records=6		Input split bytes=201		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=122		CPU time spent (ms)=1940		Physical memory (bytes) snapshot=360312832		Virtual memory (bytes) snapshot=5553082368		Total committed heap usage (bytes)=359137280	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=72617/03/31 16:40:40 INFO mapreduce.ImportJobBase: Transferred 726 bytes in 34.6711 seconds (20.9396 bytes/sec)17/03/31 16:40:40 INFO mapreduce.ImportJobBase: Retrieved 6 records.17/03/31 16:40:40 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE` AS t LIMIT 117/03/31 16:40:40 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.465 secondsLoading data to table test.fanwe_deal_cateTable test.fanwe_deal_cate stats: [numFiles=4, numRows=0, totalSize=1452, rawDataSize=0]OKTime taken: 1.281 seconds
[16:40:25] INFO:    Now wait 5 seconds to begin next task ...
[16:40:30] INFO:    Connection channel disconnect
[16:40:30] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE;'
[16:40:30] INFO:    Connection channel established succesfully
[16:40:30] INFO:    Start to run command
[16:40:34] INFO:    Connection channel closed
[16:40:34] INFO:    Check if exec success or not ... 
[16:40:34] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table DEPT_T -m 2 --hive-table DEPT_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:40:34] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              17/03/31 16:40:52 INFO mapreduce.Job: Job job_1490408992134_0121 running in uber mode : false17/03/31 16:40:52 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                OKTime taken: 4.515 secondsLoading data to table test.dept_tTable test.dept_t stats: [numFiles=4, numRows=0, totalSize=20, rawDataSize=0]OKTime taken: 0.984 seconds
[16:40:34] INFO:    Now wait 5 seconds to begin next task ...
[16:40:39] INFO:    Connection channel disconnect
[16:40:39] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_DEAL_LINK;'
[16:40:39] INFO:    Connection channel established succesfully
[16:40:39] INFO:    Start to run command
[16:40:47] INFO:    Connection channel closed
[16:40:47] INFO:    Check if exec success or not ... 
[16:40:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_AREA -m 2 --hive-table FANWE_AREA --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:40:47] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                 Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.121 seconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                17/03/31 16:41:03 INFO mapreduce.Job: Job job_1490408992134_0123 running in uber mode : false17/03/31 16:41:03 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 2.707 secondsLoading data to table test.fanwe_areaTable test.fanwe_area stats: [numFiles=4, numRows=0, totalSize=682, rawDataSize=0]OKTime taken: 1.426 seconds
[16:40:47] INFO:    Now wait 5 seconds to begin next task ...
[16:40:47] INFO:    Connection channel closed
[16:40:47] INFO:    Check if exec success or not ... 
[16:40:47] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE;'
[16:40:47] INFO:    Now wait 5 seconds to begin next task ...
[16:40:52] INFO:    Connection channel disconnect
[16:40:52] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LINK;'
[16:40:52] INFO:    Connection channel established succesfully
[16:40:52] INFO:    Start to run command
[16:40:52] INFO:    Connection channel disconnect
[16:40:52] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE -m 2 --hive-table FANWE_DEAL_CATE_TYPE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:40:52] INFO:    Connection channel established succesfully
[16:40:52] INFO:    Start to run command
[16:40:53] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:41:00] INFO:    Connection channel closed
[16:41:00] INFO:    Check if exec success or not ... 
[16:41:00] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_DEAL_LINK;'
[16:41:00] INFO:    Now wait 5 seconds to begin next task ...
[16:41:01] INFO:    Connection channel closed
[16:41:01] INFO:    Check if exec success or not ... 
[16:41:01] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL -m 2 --hive-table FANWE_DEAL --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:01] INFO:    Error message: 17/03/31 16:41:22 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:41:22 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:41:22 INFO tool.Bas17/03/31 16:41:23 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.031 16:41:23 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:41:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE` AS t LIMIT 117/03/31 16:41:23 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE` AS t LIMIT 117/03/31 16:41:24 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      17/03/31 16:41:16 INFO mapreduce.Job: Job job_1490408992134_0123 completed successfully17/03/31 16:41:16 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=289966		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=204		HDFS: Number of bytes written=167920		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=12045		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=12045		Total vcore-seconds taken by all map tasks=12045		Total megabyte-seconds taken by all map tasks=12334080	Map-Reduce Framework		Map input records=109		Map output records=109		Input split bytes=204		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=134		CPU time spent (ms)=2620		Physical memory (bytes) snapshot=351326208		Virtual memory (bytes) snapshot=5564452864		Total committed heap usage (bytes)=335544320	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=16792017/03/31 16:41:16 INFO mapreduce.ImportJobBase: Transferred 163.9844 KB in 67.7594 seconds (2.4201 KB/sec)17/03/31 16:41:16 INFO mapreduce.ImportJobBase: Retrieved 109 records.17/03/31 16:41:16 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL` AS t LIMIT 117/03/31 16:41:16 WARN hive.TableDefWriter: Column origin_price had to be cast to a less precise type in Hive17/03/31 16:41:16 WARN hive.TableDefWriter: Column current_price had to be cast to a less precise type in Hive17/03/31 16:41:16 WARN hive.TableDefWriter: Column return_money had to be cast to a less precise type in Hive17/03/31 16:41:16 WARN hive.TableDefWriter: Column weight had to be cast to a less precise type in Hive17/03/31 16:41:16 WARN hive.TableDefWriter: Column discount had to be cast to a less precise type in Hive17/03/31 16:41:16 WARN hive.TableDefWriter: Column balance_price had to be cast to a less precise type in Hive17/03/31 16:41:16 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.224 secondsLoading data to table test.fanwe_dealTable test.fanwe_deal stats: [numFiles=4, numRows=0, totalSize=335840, rawDataSize=0]OKTime taken: 1.565 seconds
[16:41:01] INFO:    Now wait 5 seconds to begin next task ...
[16:41:05] INFO:    Connection channel disconnect
[16:41:05] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_DEAL_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_DEAL_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:05] INFO:    Connection channel established succesfully
[16:41:05] INFO:    Start to run command
[16:41:05] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:41:06] INFO:    Connection channel disconnect
[16:41:06] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LOCATION_LINK;'
[16:41:07] INFO:    Connection channel established succesfully
[16:41:07] INFO:    Start to run command
[16:41:09] INFO:    Connection channel closed
[16:41:09] INFO:    Check if exec success or not ... 
[16:41:09] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LINK;'
[16:41:09] INFO:    Now wait 5 seconds to begin next task ...
[16:41:14] INFO:    Connection channel disconnect
[16:41:14] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:14] INFO:    Connection channel established succesfully
[16:41:14] INFO:    Start to run command
[16:41:15] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:41:23] INFO:    Connection channel closed
[16:41:23] INFO:    Check if exec success or not ... 
[16:41:23] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.FANWE_DEAL_CATE_TYPE_LOCATION_LINK;'
[16:41:23] INFO:    Now wait 5 seconds to begin next task ...
[16:41:28] INFO:    Connection channel disconnect
[16:41:28] INFO:    Command is sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LOCATION_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LOCATION_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:28] INFO:    Connection channel established succesfully
[16:41:28] INFO:    Start to run command
[16:41:29] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:41:47] INFO:    Connection channel closed
[16:41:47] INFO:    Check if exec success or not ... 
[16:41:47] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE -m 2 --hive-table FANWE_DEAL_CATE_TYPE --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:47] INFO:    Error message: 17/03/31 16:41:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:41:58 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:41:58 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/31 16:41:58 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/31 16:41:59 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/31 16:41:59 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:42:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK` AS t LIMIT 117/03/31 16:42:00 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK` AS t LIMIT 117/03/31 16:42:00 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/ed7de49e4324aeff75061abf12fde55a/FANWE_DEAL_CATE_TYPE_LOCATION_LINK.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/31 16:42:03 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/ed7de49e4324aeff75061abf12fde55a/FANWE_DEAL_CATE_TYPE_LOCATION_LINK.jar17/03/31 16:42:03 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/31 16:42:03 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/31 16:42:03 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/31 16:42:03 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/31 16:42:03 WARN manager.CatalogQueryManager: The table FANWE_DEAL_CATE_TYPE_LOCATION_LINK contains a multi-column primary key. Sqoop will default to the column location_id only for this job.17/03/31 16:42:03 WARN manager.CatalogQueryManager: The table FANWE_DEAL_CATE_TYPE_LOCATION_LINK contains a multi-column primary key. Sqoop will default to the column location_id only for this job.17/03/31 16:42:03 INFO mapreduce.ImportJobBase: Beginning import of FANWE_DEAL_CATE_TYPE_LOCATION_LINK17/03/31 16:42:04 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/31 16:42:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/31 16:42:07 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:8032n isolation17/03/31 16:42:01 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`cate_id`), MAX(`cate_id`) FROM `FANWE_DEAL_CATE_TYPE_LINK`17/03/31 16:42:01 INFO db.IntegerSplitter: Split size: 2; Num splits: 2 from: 13 to: 1817/03/31 16:42:01 INFO mapreduce.JobSubmitter: number of splits:217/03/31 16:42:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_012617/03/31 16:42:03 INFO impl.YarnClientImpl: Submitted application application_1490408992134_01261717/03/31 16:42:04 INFO mapreduce.Job:  map 100% reduce 0%17/03/31 16:42:05 INFO mapreduce.Job: Job job_1490408992134_0124 completed successfully17/03/31 16:42:06 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes re17/03/31 16:42:07 INFO mapreduce.Job: Job job_1490408992134_0125 running in uber mode : false17/03/31 16:42:07 INFO mapreduce.Job:  map 0% reduce 0%ations=0		HDFS: Number of bytes read=200		HDFS: Number of bytes written=658		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=8149		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=8149		Total vcore-seconds taken by all map tasks=8149		Total megabyte-seconds taken by all map tasks=8344576	Map-Reduce Framework		Map input records=34		Map output records=34		Input split bytes=200		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=122		CPU time spent (ms)=1620		Physical memory (bytes) snapshot=349130752		Virtual memory (bytes) snapshot=5560115200		Total committed heap usage (bytes)=335544320	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=65817/03/31 16:42:06 INFO mapreduce.ImportJobBase: Transferred 658 bytes in 36.3398 seconds (18.1069 bytes/sec)17/03/31 16:42:06 INFO mapreduce.ImportJobBase: Retrieved 34 records.17/03/31 16:42:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE` AS t LIMIT 117/03/31 16:42:06 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.333 secondsLoading data to table test.fanwe_deal_cate_typeTable test.fanwe_deal_cate_type stats: [numFiles=2, totalSize=658]OKTime taken: 1.311 seconds
[16:41:47] INFO:    Now wait 5 seconds to begin next task ...
[16:41:52] INFO:    Connection channel disconnect
[16:41:52] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:41:52] INFO:    Connection channel established succesfully
[16:41:52] INFO:    Start to run command
[16:41:58] INFO:    Connection channel closed
[16:41:58] INFO:    Check if exec success or not ... 
[16:41:58] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_DEAL_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_DEAL_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:41:58] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 17/03/31 16:42:21 INFO db.DBInputFormat: Using read commited transaction isolation17/03/31 16:42:21 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`location_id`), MAX(`location_id`) FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK`17/03/31 16:42:21 INFO db.IntegerSplitter: Split size: 57; Num splits: 2 from: 21 to: 13617/03/31 16:42:22 INFO mapreduce.JobSubmitter: number of splits:217/03/31 16:42:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_012717/03/31 16:42:24 INFO impl.YarnClientImpl: Submitted application application_1490408992134_012717/03/31 16:42:24 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0127/17/03/31 16:42:24 INFO mapreduce.Job: Running job: job_1490408992134_0127lse17/03/31 16:42:21 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          OKTime taken: 4.391 secondsLoading data to table test.fanwe_deal_cate_type_deal_linkTable test.fanwe_deal_cate_type_deal_link stats: [numFiles=2, totalSize=256]OKTime taken: 1.558 seconds
[16:41:58] INFO:    Now wait 5 seconds to begin next task ...
[16:42:03] INFO:    Connection channel disconnect
[16:42:03] INFO:    Command is sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:42:03] INFO:    Connection channel established succesfully
[16:42:03] INFO:    Start to run command
[16:42:08] INFO:    Connection channel closed
[16:42:08] INFO:    Check if exec success or not ... 
[16:42:08] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.TABLE_T;'
[16:42:08] INFO:    Now wait 5 seconds to begin next task ...
[16:42:09] INFO:    Connection channel closed
[16:42:09] INFO:    Check if exec success or not ... 
[16:42:09] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:42:09] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/03/31 16:42:37 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.                                                                                                                                                                                                                                                                         OKTime taken: 1.146 seconds                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              17/03/31 16:42:34 INFO mapreduce.Job: Job job_1490408992134_0127 running in uber mode : false17/03/31 16:42:34 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    17/03/31 16:42:30 INFO mapreduce.ImportJobBase: Retrieved 44 records.17/03/31 16:42:30 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LINK` AS t LIMIT 117/03/31 16:42:30 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 1.998 secondsLoading data to table test.fanwe_deal_cate_type_linkTable test.fanwe_deal_cate_type_link stats: [numFiles=4, numRows=0, totalSize=528, rawDataSize=0]OKTime taken: 1.385 seconds
[16:42:09] INFO:    Now wait 5 seconds to begin next task ...
[16:42:13] INFO:    Connection channel disconnect
[16:42:13] INFO:    Command is sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:42:13] INFO:    Connection channel established succesfully
[16:42:13] INFO:    Start to run command
[16:42:13] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:42:14] INFO:    Connection channel disconnect
[16:42:14] INFO:    SSH connection shutdown
[16:42:21] INFO:    Connection channel closed
[16:42:21] INFO:    Check if exec success or not ... 
[16:42:21] INFO:    Execute successfully for command: sudo -u hdfs hive -e 'DROP TABLE IF EXISTS test.USER_T;'
[16:42:21] INFO:    Now wait 5 seconds to begin next task ...
[16:42:24] INFO:    Connection channel closed
[16:42:24] INFO:    Check if exec success or not ... 
[16:42:24] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table FANWE_DEAL_CATE_TYPE_LOCATION_LINK -m 2 --hive-table FANWE_DEAL_CATE_TYPE_LOCATION_LINK --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:42:24] INFO:    Error message: 17/03/31 16:42:46 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:42:46 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:42:46 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/31 16:42:46 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/31 16:42:47 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/31 16:42:47 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:42:50 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/31 16:42:51 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/31 16:42:51 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           17/03/31 16:42:41 INFO mapreduce.Job:  map 50% reduce 0%17/03/31 16:42:42 INFO mapreduce.Job:  map 100% reduce 0%17/03/31 16:42:44 INFO mapreduce.Job: Job job_1490408992134_0127 completed successfully17/03/31 16:42:44 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287774		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=238		HDFS: Number of bytes written=618		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9715		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9715		Total vcore-seconds taken by all map tasks=9715		Total megabyte-seconds taken by all map tasks=9948160	Map-Reduce Framework		Map input records=97		Map output records=97		Input split bytes=238		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=161		CPU time spent (ms)=2040		Physical memory (bytes) snapshot=346869760		Virtual memory (bytes) snapshot=5559259136		Total committed heap usage (bytes)=336068608	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=61817/03/31 16:42:44 INFO mapreduce.ImportJobBase: Transferred 618 bytes in 37.6035 seconds (16.4347 bytes/sec)17/03/31 16:42:44 INFO mapreduce.ImportJobBase: Retrieved 97 records.17/03/31 16:42:44 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `FANWE_DEAL_CATE_TYPE_LOCATION_LINK` AS t LIMIT 117/03/31 16:42:44 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 5.031 secondsLoading data to table test.fanwe_deal_cate_type_location_linkTable test.fanwe_deal_cate_type_location_link stats: [numFiles=4, numRows=0, totalSize=1236, rawDataSize=0]OKTime taken: 1.8 seconds
[16:42:24] INFO:    Now wait 5 seconds to begin next task ...
[16:42:26] INFO:    Connection channel disconnect
[16:42:26] INFO:    Command is sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:42:26] INFO:    Connection channel established succesfully
[16:42:26] INFO:    Start to run command
[16:42:26] INFO:    Warning: /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.

[16:42:29] INFO:    Connection channel disconnect
[16:42:29] INFO:    SSH connection shutdown
[16:43:08] INFO:    Connection channel closed
[16:43:08] INFO:    Check if exec success or not ... 
[16:43:08] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table TABLE_T -m 2 --hive-table TABLE_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:43:08] INFO:    Error message: 17/03/31 16:43:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:43:01 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:43:01 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/31 16:43:01 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/31 16:43:01 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/31 16:43:01 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:43:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/31 16:43:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/31 16:43:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/7ce0b7d48cf6097f266e43248326b033/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/31 16:43:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/7ce0b7d48cf6097f266e43248326b033/USER_T.jar17/03/31 16:43:06 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/31 16:43:06 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/31 16:43:06 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/31 16:43:06 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/31 16:43:06 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/31 16:43:06 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/31 16:43:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/31 16:43:08 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/31 16:43:20 INFO db.DBInputFormat: Using read commited transaction isolation17/03/31 16:43:20 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/31 16:43:20 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/31 16:43:20 INFO mapreduce.JobSubmitter: number of splits:217/03/31 16:43:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_012917/03/31 16:43:21 INFO impl.YarnClientImpl: Submitted application application_1490408992134_012917/03/31 16:43:21 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0129/17/03/31 16:43:21 INFO mapreduce.Job: Running job: job_1490408992134_012917/03/31 16:43:27 INFO mapreduce.Job: Job job_1490408992134_0129 running in uber mode : false17/03/31 16:43:27 INFO mapreduce.Job:  map 0% reduce 0%17/03/31 16:43:32 INFO mapreduce.Job:  map 50% reduce 0%17/03/31 16:43:38 INFO mapreduce.Job:  map 100% reduce 0%17/03/31 16:43:39 INFO mapreduce.Job: Job job_1490408992134_0129 completed successfullyfully17/03/31 16:43:31 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287388		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=8		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9050		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9050		Total vcore-seconds taken by all map tasks=9050		Total megabyte-seconds taken by all map tasks=9267200	Map-Reduce Framework		Map input records=2		Map output records=2		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=137		CPU time spent (ms)=2390		Physical memory (bytes) snapshot=362901504		Virtual memory (bytes) snapshot=5560659968		Total committed heap usage (bytes)=357040128	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=817/03/31 16:43:31 INFO mapreduce.ImportJobBase: Transferred 8 bytes in 35.197 seconds (0.2273 bytes/sec)17/03/31 16:43:32 INFO mapreduce.ImportJobBase: Retrieved 2 records.17/03/31 16:43:32 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `TABLE_T` AS t LIMIT 117/03/31 16:43:32 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.526 secondsLoading data to table test.table_tTable test.table_t stats: [numFiles=4, numRows=0, totalSize=16, rawDataSize=0]OKTime taken: 1.291 seconds
[16:43:08] INFO:    Now wait 5 seconds to begin next task ...
[16:43:13] INFO:    Connection channel disconnect
[16:43:13] INFO:    SSH connection shutdown
[16:43:16] INFO:    Connection channel closed
[16:43:16] INFO:    Check if exec success or not ... 
[16:43:16] INFO:    Execution failed while executing command: sudo -u hdfs sqoop import --table USER_T -m 2 --hive-table USER_T --hive-import  --hive-database test --connect jdbc:mysql://10.1.20.86:3306/test_lp --username hldc_h5 --password hldc_h5
[16:43:16] INFO:    Error message: 17/03/31 16:43:00 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.8.017/03/31 16:43:01 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.17/03/31 16:43:01 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override17/03/31 16:43:01 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.17/03/31 16:43:01 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.17/03/31 16:43:01 INFO tool.CodeGenTool: Beginning code generation17/03/31 16:43:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/31 16:43:02 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/31 16:43:02 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduceNote: /tmp/sqoop-hdfs/compile/7ce0b7d48cf6097f266e43248326b033/USER_T.java uses or overrides a deprecated API.Note: Recompile with -Xlint:deprecation for details.17/03/31 16:43:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hdfs/compile/7ce0b7d48cf6097f266e43248326b033/USER_T.jar17/03/31 16:43:06 WARN manager.MySQLManager: It looks like you are importing from mysql.17/03/31 16:43:06 WARN manager.MySQLManager: This transfer can be faster! Use the --direct17/03/31 16:43:06 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.17/03/31 16:43:06 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)17/03/31 16:43:06 INFO mapreduce.ImportJobBase: Beginning import of USER_T17/03/31 16:43:06 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar17/03/31 16:43:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps17/03/31 16:43:08 INFO client.RMProxy: Connecting to ResourceManager at name.hadoop.demo/172.16.110.200:803217/03/31 16:43:20 INFO db.DBInputFormat: Using read commited transaction isolation17/03/31 16:43:20 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `USER_T`17/03/31 16:43:20 INFO db.IntegerSplitter: Split size: 1; Num splits: 2 from: 1 to: 417/03/31 16:43:20 INFO mapreduce.JobSubmitter: number of splits:217/03/31 16:43:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1490408992134_012917/03/31 16:43:21 INFO impl.YarnClientImpl: Submitted application application_1490408992134_012917/03/31 16:43:21 INFO mapreduce.Job: The url to track the job: http://name.hadoop.demo:8088/proxy/application_1490408992134_0129/17/03/31 16:43:21 INFO mapreduce.Job: Running job: job_1490408992134_012917/03/31 16:43:27 INFO mapreduce.Job: Job job_1490408992134_0129 running in uber mode : false17/03/31 16:43:27 INFO mapreduce.Job:  map 0% reduce 0%17/03/31 16:43:32 INFO mapreduce.Job:  map 50% reduce 0%17/03/31 16:43:38 INFO mapreduce.Job:  map 100% reduce 0%17/03/31 16:43:39 INFO mapreduce.Job: Job job_1490408992134_0129 completed successfully17/03/31 16:43:39 INFO mapreduce.Job: Counters: 30	File System Counters		FILE: Number of bytes read=0		FILE: Number of bytes written=287376		FILE: Number of read operations=0		FILE: Number of large read operations=0		FILE: Number of write operations=0		HDFS: Number of bytes read=197		HDFS: Number of bytes written=12		HDFS: Number of read operations=8		HDFS: Number of large read operations=0		HDFS: Number of write operations=4	Job Counters 		Launched map tasks=2		Other local map tasks=2		Total time spent by all maps in occupied slots (ms)=9646		Total time spent by all reduces in occupied slots (ms)=0		Total time spent by all map tasks (ms)=9646		Total vcore-seconds taken by all map tasks=9646		Total megabyte-seconds taken by all map tasks=9877504	Map-Reduce Framework		Map input records=3		Map output records=3		Input split bytes=197		Spilled Records=0		Failed Shuffles=0		Merged Map outputs=0		GC time elapsed (ms)=140		CPU time spent (ms)=2820		Physical memory (bytes) snapshot=346398720		Virtual memory (bytes) snapshot=5553786880		Total committed heap usage (bytes)=338690048	File Input Format Counters 		Bytes Read=0	File Output Format Counters 		Bytes Written=1217/03/31 16:43:39 INFO mapreduce.ImportJobBase: Transferred 12 bytes in 30.5324 seconds (0.393 bytes/sec)17/03/31 16:43:39 INFO mapreduce.ImportJobBase: Retrieved 3 records.17/03/31 16:43:39 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `USER_T` AS t LIMIT 117/03/31 16:43:39 INFO hive.HiveImport: Loading uploaded data into HiveLogging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesOKTime taken: 3.456 secondsLoading data to table test.user_tTable test.user_t stats: [numFiles=2, totalSize=12]OKTime taken: 1.327 seconds0]OKTime taken: 1.291 seconds
[16:43:16] INFO:    Now wait 5 seconds to begin next task ...
[16:43:21] INFO:    Connection channel disconnect
[16:43:21] INFO:    SSH connection shutdown

=============== [2017/05/17 11:03:54, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[11:03:54] INFO:    SSHExec initializing ...
[11:03:54] INFO:    Session initialized and associated with user credential inteast.com
[11:03:54] INFO:    SSHExec initialized successfully
[11:03:54] INFO:    SSHExec trying to connect root@10.1.70.200
[11:03:55] INFO:    SSH connection established
[11:03:55] INFO:    Command is hbase shell hbase.shell
[11:03:55] INFO:    Connection channel established succesfully
[11:03:55] INFO:    Start to run command
[11:04:02] INFO:    Connection channel closed
[11:04:02] INFO:    Check if exec success or not ... 
[11:04:02] INFO:    Execution failed while executing command: hbase shell hbase.shell
[11:04:02] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 11:04:07 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.availableLoadError: No such file to load -- hbase.shell    load at org/jruby/RubyKernel.java:1087  (root) at /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/hbase/bin/../bin/hirb.rb:177
[11:04:02] INFO:    Now wait 5 seconds to begin next task ...
[11:04:07] INFO:    Connection channel disconnect
[11:04:07] INFO:    SSH connection shutdown

=============== [2017/05/17 11:04:39, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[11:04:39] INFO:    SSHExec initializing ...
[11:04:39] INFO:    Session initialized and associated with user credential inteast.com
[11:04:39] INFO:    SSHExec initialized successfully
[11:04:39] INFO:    SSHExec trying to connect root@10.1.70.200
[11:04:39] INFO:    SSH connection established
[11:04:39] INFO:    Command is hbase shell /home/lp/hbase.shell
[11:04:39] INFO:    Connection channel established succesfully
[11:04:39] INFO:    Start to run command
[11:04:45] INFO:    ROW
[11:04:45] INFO:      COLUMN+CELL

[11:04:46] INFO:     
[11:04:46] INFO:    18658727110149148130790 column=cf1:time, timestamp=1494988387789, value=123456844

[11:04:46] INFO:     18658727110149148130790 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24615

[11:04:46] INFO:     
[11:04:46] INFO:    18658727110149148130791 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456290

[11:04:46] INFO:     
[11:04:46] INFO:    18658727110149148130791 column=cf1:type, timestamp=1494988387789, value=2466
[11:04:46] INFO:    

[11:04:46] INFO:     
[11:04:46] INFO:    186587271101491481307910 column=cf1:time, timestamp=1494988387789, value=123456298
[11:04:46] INFO:    

[11:04:46] INFO:     
[11:04:46] INFO:    186587271101491481307910 column=cf1:type, timestamp=1494988387789, value=24152
[11:04:46] INFO:    

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079100
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456327

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079100 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24157

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079103 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456528

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079103 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24483

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079106
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456334

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079106 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24633

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079108
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456801

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079108 column=cf1:type, timestamp=1494988387789, value=24833

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079109 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456150

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079109 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2487

[11:04:46] INFO:     
[11:04:46] INFO:    186587271101491481307911
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456954

[11:04:46] INFO:     186587271101491481307911
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2446

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079111 column=cf1:time, timestamp=1494988387789, value=12345668
[11:04:46] INFO:    

[11:04:46] INFO:     1865872711014914813079111 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24804

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079113 column=cf1:time, timestamp=1494988387789, value=1234565
[11:04:46] INFO:    

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079113 column=cf1:type, timestamp=1494988387789, value=246
[11:04:46] INFO:    

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079114 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456578

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079114 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2494
[11:04:46] INFO:    

[11:04:46] INFO:     1865872711014914813079115 column=cf1:time, timestamp=1494988387789, value=12345626
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079115 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24589
 
[11:04:46] INFO:    1865872711014914813079117 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456113
 
[11:04:46] INFO:    1865872711014914813079117 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24597
 
[11:04:46] INFO:    1865872711014914813079118
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933

[11:04:46] INFO:     1865872711014914813079118
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24118

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079119 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456572
 
[11:04:46] INFO:    1865872711014914813079119 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24464

[11:04:46] INFO:     186587271101491481307912 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456782
 
[11:04:46] INFO:    186587271101491481307912 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24375
 
[11:04:46] INFO:    1865872711014914813079120 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456256
 
[11:04:46] INFO:    1865872711014914813079120 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24246

[11:04:46] INFO:     1865872711014914813079121 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456478

[11:04:46] INFO:     1865872711014914813079121 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24492
 
[11:04:46] INFO:    1865872711014914813079122
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456900
[11:04:46] INFO:    
 1865872711014914813079122 column=cf1:type, timestamp=1494988387789, value=24190
[11:04:46] INFO:    
 1865872711014914813079123
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456138

[11:04:46] INFO:     1865872711014914813079123 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24346
 1865872711014914813079124
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456996
 
[11:04:46] INFO:    1865872711014914813079124 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24319
 
[11:04:46] INFO:    1865872711014914813079127
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456708

[11:04:46] INFO:     1865872711014914813079127 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24867
 
[11:04:46] INFO:    1865872711014914813079128 column=cf1:time, timestamp=1494988387789, value=123456326
[11:04:46] INFO:    
 1865872711014914813079128
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24541

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079129 column=cf1:time, timestamp=1494988387789, value=123456771

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079129 column=cf1:type, timestamp=1494988387789, value=24621

[11:04:46] INFO:     186587271101491481307913 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456919
 
[11:04:46] INFO:    186587271101491481307913 column=cf1:type, timestamp=1494988387789, value=24290
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079130 column=cf1:time, timestamp=1494988387789, value=123456744
[11:04:46] INFO:    
 1865872711014914813079130 column=cf1:type, timestamp=1494988387789, value=2490
 1865872711014914813079131 column=cf1:time, timestamp=1494988387789, value=123456313
 
[11:04:46] INFO:    1865872711014914813079131 column=cf1:type, timestamp=1494988387789, value=2478
 1865872711014914813079132
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456120

[11:04:46] INFO:     1865872711014914813079132 column=cf1:type, timestamp=1494988387789, value=24276
[11:04:46] INFO:    
 1865872711014914813079134 column=cf1:time, timestamp=1494988387789, value=123456521
[11:04:46] INFO:    
 1865872711014914813079134 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24292
 1865872711014914813079135 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456650
 1865872711014914813079135
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24883
 1865872711014914813079136
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456126
 
[11:04:46] INFO:    1865872711014914813079136 column=cf1:type, timestamp=1494988387789, value=2495

[11:04:46] INFO:     1865872711014914813079137 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345693
 1865872711014914813079137 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079139 column=cf1:time, timestamp=1494988387789, value=123456689
 1865872711014914813079139 column=cf1:type, timestamp=1494988387789, value=24988
 1865872711014914813079140 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456847
 1865872711014914813079140
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24131
 1865872711014914813079142
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079142
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24866
 1865872711014914813079143
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456584

[11:04:46] INFO:     1865872711014914813079143 column=cf1:type, timestamp=1494988387789, value=24761
[11:04:46] INFO:    
 1865872711014914813079144 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456546
 1865872711014914813079144
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24283

[11:04:46] INFO:     1865872711014914813079146 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345635
 1865872711014914813079146
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24888
 
[11:04:46] INFO:    1865872711014914813079147 column=cf1:time, timestamp=1494988387789, value=123456952

[11:04:46] INFO:     1865872711014914813079147 column=cf1:type, timestamp=1494988387789, value=24336
[11:04:46] INFO:    
 1865872711014914813079150 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456776
 1865872711014914813079150
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24967
 
[11:04:46] INFO:    1865872711014914813079151 column=cf1:time, timestamp=1494988387789, value=123456717

[11:04:46] INFO:     1865872711014914813079151 column=cf1:type, timestamp=1494988387789, value=24622
[11:04:46] INFO:    
 1865872711014914813079152
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456621
 1865872711014914813079152
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24586
 
[11:04:46] INFO:    1865872711014914813079154 column=cf1:time, timestamp=1494988387789, value=123456661

[11:04:46] INFO:     1865872711014914813079154 column=cf1:type, timestamp=1494988387789, value=24899
[11:04:46] INFO:    
 1865872711014914813079157 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456675
 
[11:04:46] INFO:    1865872711014914813079157 column=cf1:type, timestamp=1494988387789, value=24259

[11:04:46] INFO:     1865872711014914813079158 column=cf1:time, timestamp=1494988387789, value=123456195
[11:04:46] INFO:    
 1865872711014914813079158 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24713
 1865872711014914813079159
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456940

[11:04:46] INFO:     1865872711014914813079159 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24882
 
[11:04:46] INFO:    1865872711014914813079160 column=cf1:time, timestamp=1494988387789, value=123456502
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079160 column=cf1:type, timestamp=1494988387789, value=2433
[11:04:46] INFO:    
 1865872711014914813079161 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456738
 1865872711014914813079161
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24115
 
[11:04:46] INFO:    1865872711014914813079162 column=cf1:time, timestamp=1494988387789, value=123456191
 1865872711014914813079162 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24490
 1865872711014914813079164
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456437
 
[11:04:46] INFO:    1865872711014914813079164 column=cf1:type, timestamp=1494988387789, value=24567

[11:04:46] INFO:     1865872711014914813079165 column=cf1:time, timestamp=1494988387789, value=123456932

[11:04:46] INFO:     1865872711014914813079165 column=cf1:type, timestamp=1494988387789, value=24499
[11:04:46] INFO:    
 1865872711014914813079168 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456758
 1865872711014914813079168
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24306
 1865872711014914813079169
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456647
 1865872711014914813079169
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24123
 186587271101491481307917 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456696
 186587271101491481307917
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24978

[11:04:46] INFO:     1865872711014914813079170 column=cf1:time, timestamp=1494988387789, value=123456601
[11:04:46] INFO:    
 1865872711014914813079170 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24802
 1865872711014914813079171
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456742

[11:04:46] INFO:     1865872711014914813079171 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24269

[11:04:46] INFO:     1865872711014914813079172 column=cf1:time, timestamp=1494988387789, value=123456410
[11:04:46] INFO:    
 1865872711014914813079172 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24543
 1865872711014914813079173 column=cf1:time, timestamp=1494988387789, value=123456306
[11:04:46] INFO:    
 1865872711014914813079173 column=cf1:type, timestamp=1494988387789, value=2423
[11:04:46] INFO:    
 1865872711014914813079177 column=cf1:time, timestamp=1494988387789, value=12345649

[11:04:46] INFO:     1865872711014914813079177 column=cf1:type, timestamp=1494988387789, value=24760
[11:04:46] INFO:    
 1865872711014914813079178 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456347
 1865872711014914813079178 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24566
 1865872711014914813079182 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456756
 1865872711014914813079182
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24858
 1865872711014914813079184 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456564
 1865872711014914813079184
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24965

[11:04:46] INFO:     1865872711014914813079186 column=cf1:time, timestamp=1494988387789, value=123456178
 
[11:04:46] INFO:    1865872711014914813079186 column=cf1:type, timestamp=1494988387789, value=24763
 1865872711014914813079187
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456653
 
[11:04:46] INFO:    1865872711014914813079187 column=cf1:type, timestamp=1494988387789, value=246
 
[11:04:46] INFO:    1865872711014914813079188 column=cf1:time, timestamp=1494988387789, value=123456299
 
[11:04:46] INFO:    1865872711014914813079188 column=cf1:type, timestamp=1494988387789, value=24900
 
[11:04:46] INFO:    1865872711014914813079189 column=cf1:time, timestamp=1494988387789, value=123456636
 
[11:04:46] INFO:    1865872711014914813079189 column=cf1:type, timestamp=1494988387789, value=24350
 186587271101491481307919
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933
 186587271101491481307919 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24124
 
[11:04:46] INFO:    1865872711014914813079191 column=cf1:time, timestamp=1494988387789, value=12345637
 
[11:04:46] INFO:    1865872711014914813079191 column=cf1:type, timestamp=1494988387789, value=24745

[11:04:46] INFO:     1865872711014914813079194 column=cf1:time, timestamp=1494988387789, value=123456834

[11:04:46] INFO:     1865872711014914813079194 column=cf1:type, timestamp=1494988387789, value=24951

[11:04:46] INFO:     1865872711014914813079195
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079195
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24366
 18658727110149148130792
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456590
 
[11:04:46] INFO:    18658727110149148130792 column=cf1:type, timestamp=1494988387789, value=24463
 
[11:04:46] INFO:    1865872711014914813079200 column=cf1:time, timestamp=1494988387789, value=12345639

[11:04:46] INFO:     1865872711014914813079200 column=cf1:type, timestamp=1494988387789, value=24239
[11:04:46] INFO:    
 1865872711014914813079201 column=cf1:time, timestamp=1494988387789, value=123456232

[11:04:46] INFO:     1865872711014914813079201 column=cf1:type, timestamp=1494988387789, value=24423

[11:04:46] INFO:     1865872711014914813079202 column=cf1:time, timestamp=1494988387789, value=123456278
[11:04:46] INFO:    
 1865872711014914813079202 column=cf1:type, timestamp=1494988387789, value=24198

[11:04:46] INFO:     1865872711014914813079203 column=cf1:time, timestamp=1494988387789, value=123456358
 1865872711014914813079203 column=cf1:type, timestamp=1494988387789, value=24486
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079205 column=cf1:time, timestamp=1494988387789, value=123456247
[11:04:46] INFO:    
 1865872711014914813079205
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24374

[11:04:46] INFO:     1865872711014914813079206 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456802
 
[11:04:46] INFO:    1865872711014914813079206 column=cf1:type, timestamp=1494988387789, value=24491
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079207 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456571
 
[11:04:46] INFO:    1865872711014914813079207 column=cf1:type, timestamp=1494988387789, value=24238

[11:04:46] INFO:     1865872711014914813079208 column=cf1:time, timestamp=1494988387789, value=123456727
 1865872711014914813079208 column=cf1:type, timestamp=1494988387789, value=24937
[11:04:46] INFO:    
 1865872711014914813079210 column=cf1:time, timestamp=1494988387789, value=123456606
[11:04:46] INFO:    
 1865872711014914813079210 column=cf1:type, timestamp=1494988387789, value=24652
 1865872711014914813079211
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456973
 1865872711014914813079211 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079214 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456384
 1865872711014914813079214 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24483
 1865872711014914813079215 column=cf1:time, timestamp=1494988387789, value=12345694
[11:04:46] INFO:    
 1865872711014914813079215 column=cf1:type, timestamp=1494988387789, value=2420

[11:04:46] INFO:     1865872711014914813079216 column=cf1:time, timestamp=1494988387789, value=123456750

[11:04:46] INFO:     1865872711014914813079216 column=cf1:type, timestamp=1494988387789, value=24124
 
[11:04:46] INFO:    1865872711014914813079217 column=cf1:time, timestamp=1494988387789, value=123456934
 
[11:04:46] INFO:    1865872711014914813079217 column=cf1:type, timestamp=1494988387789, value=24361
 
[11:04:46] INFO:    1865872711014914813079218 column=cf1:time, timestamp=1494988387789, value=123456904
 
[11:04:46] INFO:    1865872711014914813079218 column=cf1:type, timestamp=1494988387789, value=24733
 
[11:04:46] INFO:    1865872711014914813079219 column=cf1:time, timestamp=1494988387789, value=123456288

[11:04:46] INFO:     1865872711014914813079219 column=cf1:type, timestamp=1494988387789, value=24901

[11:04:46] INFO:     186587271101491481307922 column=cf1:time, timestamp=1494988387789, value=123456523

[11:04:46] INFO:     186587271101491481307922 column=cf1:type, timestamp=1494988387789, value=2451

[11:04:46] INFO:     1865872711014914813079220 column=cf1:time, timestamp=1494988387789, value=123456802

[11:04:46] INFO:     1865872711014914813079220 column=cf1:type, timestamp=1494988387789, value=24935

[11:04:46] INFO:     1865872711014914813079221 column=cf1:time, timestamp=1494988387789, value=123456940

[11:04:46] INFO:     1865872711014914813079221 column=cf1:type, timestamp=1494988387789, value=24818
[11:04:46] INFO:    
 1865872711014914813079223
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456621
 
[11:04:46] INFO:    1865872711014914813079223 column=cf1:type, timestamp=1494988387789, value=24138

[11:04:46] INFO:     1865872711014914813079224 column=cf1:time, timestamp=1494988387789, value=123456513
[11:04:46] INFO:    
 1865872711014914813079224 column=cf1:type, timestamp=1494988387789, value=24474
[11:04:46] INFO:    
 1865872711014914813079225 column=cf1:time, timestamp=1494988387789, value=123456719
[11:04:46] INFO:    
 1865872711014914813079225 column=cf1:type, timestamp=1494988387789, value=24639
[11:04:46] INFO:    
 1865872711014914813079226
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456706
 1865872711014914813079226
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24595
 1865872711014914813079228
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456371
 
[11:04:46] INFO:    1865872711014914813079228 column=cf1:type, timestamp=1494988387789, value=24143
 1865872711014914813079229
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456723
 1865872711014914813079229
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24855
 186587271101491481307923 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456894
 186587271101491481307923
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24576
 1865872711014914813079230 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345675
 
[11:04:46] INFO:    1865872711014914813079230 column=cf1:type, timestamp=1494988387789, value=24548
 
[11:04:46] INFO:    1865872711014914813079231 column=cf1:time, timestamp=1494988387789, value=123456698
 1865872711014914813079231
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2420

[11:04:46] INFO:     
[11:04:46] INFO:    1865872711014914813079232 column=cf1:time, timestamp=1494988387789, value=123456548
 
[11:04:46] INFO:    1865872711014914813079232 column=cf1:type, timestamp=1494988387789, value=24804
[11:04:46] INFO:    
 1865872711014914813079233
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933
 
[11:04:46] INFO:    1865872711014914813079233 column=cf1:type, timestamp=1494988387789, value=24687
[11:04:46] INFO:    
 1865872711014914813079235 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456600
 1865872711014914813079235
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2463

[11:04:46] INFO:     1865872711014914813079236 column=cf1:time, timestamp=1494988387789, value=123456470

[11:04:46] INFO:     1865872711014914813079236 column=cf1:type, timestamp=1494988387789, value=24338
[11:04:46] INFO:    
 1865872711014914813079237
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456772

[11:04:46] INFO:     1865872711014914813079237 column=cf1:type, timestamp=1494988387789, value=24986

[11:04:46] INFO:     1865872711014914813079238 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456275
 
[11:04:46] INFO:    1865872711014914813079238 column=cf1:type, timestamp=1494988387789, value=24546
 
[11:04:46] INFO:    1865872711014914813079240 column=cf1:time, timestamp=1494988387789, value=123456959

[11:04:46] INFO:     1865872711014914813079240 column=cf1:type, timestamp=1494988387789, value=24973

[11:04:46] INFO:     1865872711014914813079241 column=cf1:time, timestamp=1494988387789, value=12345686
[11:04:46] INFO:    
 1865872711014914813079241 column=cf1:type, timestamp=1494988387789, value=24108
[11:04:46] INFO:    
 1865872711014914813079243 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456551
 1865872711014914813079243 column=cf1:type, timestamp=1494988387789, value=24579

[11:04:46] INFO:     1865872711014914813079245 column=cf1:time, timestamp=1494988387789, value=123456860

[11:04:46] INFO:     1865872711014914813079245 column=cf1:type, timestamp=1494988387789, value=24958
[11:04:46] INFO:    
 1865872711014914813079246 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456439
 1865872711014914813079246 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2433
 
[11:04:46] INFO:    1865872711014914813079247 column=cf1:time, timestamp=1494988387789, value=1234564
[11:04:46] INFO:    
 1865872711014914813079247 column=cf1:type, timestamp=1494988387789, value=24393
[11:04:46] INFO:    
 1865872711014914813079249 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456656
 1865872711014914813079249 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24922
 1865872711014914813079251 column=cf1:time, timestamp=1494988387789, value=12345643

[11:04:46] INFO:     1865872711014914813079251 column=cf1:type, timestamp=1494988387789, value=24829
[11:04:46] INFO:    
 1865872711014914813079252 column=cf1:time, timestamp=1494988387789, value=123456304
[11:04:46] INFO:    
 1865872711014914813079252 column=cf1:type, timestamp=1494988387789, value=24741
[11:04:46] INFO:    
 1865872711014914813079253 column=cf1:time, timestamp=1494988387789, value=123456127
[11:04:46] INFO:    
 1865872711014914813079253 column=cf1:type, timestamp=1494988387789, value=24854
[11:04:46] INFO:    
 1865872711014914813079254 column=cf1:time, timestamp=1494988387789, value=123456574

[11:04:46] INFO:     1865872711014914813079254 column=cf1:type, timestamp=1494988387789, value=24208

[11:04:46] INFO:     1865872711014914813079255 column=cf1:time, timestamp=1494988387789, value=123456544

[11:04:46] INFO:     1865872711014914813079255 column=cf1:type, timestamp=1494988387789, value=24840
 
[11:04:46] INFO:    1865872711014914813079256 column=cf1:time, timestamp=1494988387789, value=123456470
 
[11:04:46] INFO:    1865872711014914813079256 column=cf1:type, timestamp=1494988387789, value=24692
 1865872711014914813079257
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456656
 1865872711014914813079257
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24787
 1865872711014914813079258
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456838
 1865872711014914813079258
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24448

[11:04:46] INFO:     1865872711014914813079259 column=cf1:time, timestamp=1494988387789, value=123456839
[11:04:46] INFO:    
 1865872711014914813079259 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24321
 1865872711014914813079261 column=cf1:time, timestamp=1494988387789, value=123456338
 1865872711014914813079261
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24475
 1865872711014914813079263 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456997
 1865872711014914813079263 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079264 column=cf1:time, timestamp=1494988387789, value=12345635
[11:04:46] INFO:    
 1865872711014914813079264 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24894
 1865872711014914813079265 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456732
 1865872711014914813079265 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24586
 1865872711014914813079268 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456343
 1865872711014914813079268
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24540

[11:04:46] INFO:     1865872711014914813079269 column=cf1:time, timestamp=1494988387789, value=12345614
[11:04:46] INFO:    
 
[11:04:46] INFO:    1865872711014914813079269 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2421
 
[11:04:46] INFO:    186587271101491481307927 column=cf1:time, timestamp=1494988387789, value=12345699
[11:04:46] INFO:    
 186587271101491481307927
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2429

[11:04:46] INFO:     1865872711014914813079272 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456843
 
[11:04:46] INFO:    1865872711014914813079272 column=cf1:type, timestamp=1494988387789, value=24737
[11:04:46] INFO:    
 1865872711014914813079273
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456558

[11:04:46] INFO:     1865872711014914813079273 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24156
 1865872711014914813079275
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456795
 1865872711014914813079275
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24297
 
[11:04:46] INFO:    1865872711014914813079276 column=cf1:time, timestamp=1494988387789, value=123456404
 
[11:04:46] INFO:    1865872711014914813079276 column=cf1:type, timestamp=1494988387789, value=24893

[11:04:46] INFO:     1865872711014914813079277 column=cf1:time, timestamp=1494988387789, value=123456598
[11:04:46] INFO:    
 1865872711014914813079277
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24673
 
[11:04:46] INFO:    1865872711014914813079279 column=cf1:time, timestamp=1494988387789, value=123456879

[11:04:46] INFO:     1865872711014914813079279 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24769
 1865872711014914813079281
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079281 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24615
 1865872711014914813079284 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345631
 
[11:04:46] INFO:    1865872711014914813079284 column=cf1:type, timestamp=1494988387789, value=24509
[11:04:46] INFO:    
 1865872711014914813079285
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456454
 
[11:04:46] INFO:    1865872711014914813079285 column=cf1:type, timestamp=1494988387789, value=24456
[11:04:46] INFO:    
 1865872711014914813079287
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456150

[11:04:46] INFO:     1865872711014914813079287 column=cf1:type, timestamp=1494988387789, value=24611
[11:04:46] INFO:    
 1865872711014914813079288 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079288
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24872

[11:04:46] INFO:     1865872711014914813079289 column=cf1:time, timestamp=1494988387789, value=123456613

[11:04:46] INFO:     1865872711014914813079289 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24113
 186587271101491481307929
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456106
 
[11:04:46] INFO:    186587271101491481307929 column=cf1:type, timestamp=1494988387789, value=24319

[11:04:46] INFO:     1865872711014914813079290 column=cf1:time, timestamp=1494988387789, value=123456401

[11:04:46] INFO:     1865872711014914813079290 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24445
 1865872711014914813079292 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456505
 1865872711014914813079292
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24251
 1865872711014914813079293 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345622
 1865872711014914813079293
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24798
 1865872711014914813079296
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456524
 
[11:04:46] INFO:    1865872711014914813079296 column=cf1:type, timestamp=1494988387789, value=24188
[11:04:46] INFO:    
 1865872711014914813079297 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456905
 
[11:04:46] INFO:    1865872711014914813079297 column=cf1:type, timestamp=1494988387789, value=24927
[11:04:46] INFO:    
 1865872711014914813079298
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456756
 1865872711014914813079298
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24515
 
[11:04:46] INFO:    18658727110149148130793 column=cf1:time, timestamp=1494988387789, value=123456274
 
[11:04:46] INFO:    18658727110149148130793 column=cf1:type, timestamp=1494988387789, value=24163

[11:04:46] INFO:     1865872711014914813079300 column=cf1:time, timestamp=1494988387789, value=123456824
[11:04:46] INFO:    
 1865872711014914813079300 column=cf1:type, timestamp=1494988387789, value=24403
[11:04:46] INFO:    
 1865872711014914813079303 column=cf1:time, timestamp=1494988387789, value=123456113

[11:04:46] INFO:     1865872711014914813079303 column=cf1:type, timestamp=1494988387789, value=24635
[11:04:46] INFO:    
 1865872711014914813079304 column=cf1:time, timestamp=1494988387789, value=123456220
[11:04:46] INFO:    
 1865872711014914813079304 column=cf1:type, timestamp=1494988387789, value=24403
 1865872711014914813079309 column=cf1:time, timestamp=1494988387789, value=123456796

[11:04:46] INFO:     1865872711014914813079309 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24362
 186587271101491481307931 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456496
 
[11:04:46] INFO:    186587271101491481307931 column=cf1:type, timestamp=1494988387789, value=24751

[11:04:46] INFO:     1865872711014914813079310 column=cf1:time, timestamp=1494988387789, value=123456289
[11:04:46] INFO:    
 1865872711014914813079310 column=cf1:type, timestamp=1494988387789, value=24766
[11:04:46] INFO:    
 1865872711014914813079311 column=cf1:time, timestamp=1494988387789, value=123456981
[11:04:46] INFO:    
 1865872711014914813079311 column=cf1:type, timestamp=1494988387789, value=24179
[11:04:46] INFO:    
 1865872711014914813079316 column=cf1:time, timestamp=1494988387789, value=123456133
[11:04:46] INFO:    
 1865872711014914813079316
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2494

[11:04:46] INFO:     1865872711014914813079317 column=cf1:time, timestamp=1494988387789, value=123456663

[11:04:46] INFO:     1865872711014914813079317 column=cf1:type, timestamp=1494988387789, value=24166
[11:04:46] INFO:    
 1865872711014914813079318 column=cf1:time, timestamp=1494988387789, value=123456845
 1865872711014914813079318
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24744
 
[11:04:46] INFO:    1865872711014914813079320 column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079320
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24862
 
[11:04:46] INFO:    1865872711014914813079321 column=cf1:time, timestamp=1494988387789, value=123456934
 
[11:04:46] INFO:    1865872711014914813079321 column=cf1:type, timestamp=1494988387789, value=24792
[11:04:46] INFO:    
 1865872711014914813079322
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456596

[11:04:46] INFO:     1865872711014914813079322 column=cf1:type, timestamp=1494988387789, value=24683

[11:04:46] INFO:     1865872711014914813079323 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456401

[11:04:46] INFO:     1865872711014914813079323 column=cf1:type, timestamp=1494988387789, value=24863
[11:04:46] INFO:    
 1865872711014914813079325
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=12345623

[11:04:46] INFO:     1865872711014914813079325 column=cf1:type, timestamp=1494988387789, value=24934
[11:04:46] INFO:    
 1865872711014914813079326 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456263
 
[11:04:46] INFO:    1865872711014914813079326 column=cf1:type, timestamp=1494988387789, value=24882
 
[11:04:46] INFO:    1865872711014914813079327 column=cf1:time, timestamp=1494988387789, value=12345676

[11:04:46] INFO:     1865872711014914813079327 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24700
 1865872711014914813079329
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456246
 
[11:04:46] INFO:    1865872711014914813079329 column=cf1:type, timestamp=1494988387789, value=24784

[11:04:46] INFO:     186587271101491481307933 column=cf1:time, timestamp=1494988387789, value=123456983
[11:04:46] INFO:    
 186587271101491481307933 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24748
 1865872711014914813079330
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456589
 1865872711014914813079330
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24194
 1865872711014914813079331 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079331
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24664
 
[11:04:46] INFO:    1865872711014914813079332 column=cf1:time, timestamp=1494988387789, value=123456326
 
[11:04:46] INFO:    1865872711014914813079332 column=cf1:type, timestamp=1494988387789, value=24992
 
[11:04:46] INFO:    1865872711014914813079334 column=cf1:time, timestamp=1494988387789, value=12345654
 
[11:04:46] INFO:    1865872711014914813079334 column=cf1:type, timestamp=1494988387789, value=24687
 
[11:04:46] INFO:    1865872711014914813079335 column=cf1:time, timestamp=1494988387789, value=123456551

[11:04:46] INFO:     1865872711014914813079335 column=cf1:type, timestamp=1494988387789, value=24651

[11:04:46] INFO:     1865872711014914813079336 column=cf1:time, timestamp=1494988387789, value=123456739
[11:04:46] INFO:    
 1865872711014914813079336 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24312
 1865872711014914813079337 column=cf1:time, timestamp=1494988387789, value=123456798
[11:04:46] INFO:    
 1865872711014914813079337 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24949
 
[11:04:46] INFO:    1865872711014914813079338 column=cf1:time, timestamp=1494988387789, value=123456292

[11:04:46] INFO:     1865872711014914813079338 column=cf1:type, timestamp=1494988387789, value=24838
[11:04:46] INFO:    
 1865872711014914813079339
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456465
 
[11:04:46] INFO:    1865872711014914813079339 column=cf1:type, timestamp=1494988387789, value=24761

[11:04:46] INFO:     1865872711014914813079341 column=cf1:time, timestamp=1494988387789, value=12345661
[11:04:46] INFO:    
 1865872711014914813079341 column=cf1:type, timestamp=1494988387789, value=24118
[11:04:46] INFO:    
 1865872711014914813079342 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345689
 1865872711014914813079342 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24123
 1865872711014914813079343
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456261
 1865872711014914813079343 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24744
 1865872711014914813079344
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456148
 1865872711014914813079344
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24259
 
[11:04:46] INFO:    1865872711014914813079345 column=cf1:time, timestamp=1494988387789, value=123456874
 
[11:04:46] INFO:    1865872711014914813079345 column=cf1:type, timestamp=1494988387789, value=24324
 1865872711014914813079346
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456244
 1865872711014914813079346
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24762
 1865872711014914813079347 column=cf1:time, timestamp=1494988387789, value=123456244
[11:04:46] INFO:    
 1865872711014914813079347 column=cf1:type, timestamp=1494988387789, value=24780
 
[11:04:46] INFO:    1865872711014914813079348 column=cf1:time, timestamp=1494988387789, value=123456694
 
[11:04:46] INFO:    1865872711014914813079348 column=cf1:type, timestamp=1494988387789, value=24850
 
[11:04:46] INFO:    186587271101491481307935 column=cf1:time, timestamp=1494988387789, value=123456413
 186587271101491481307935
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24846
 
[11:04:46] INFO:    1865872711014914813079352 column=cf1:time, timestamp=1494988387789, value=12345687

[11:04:46] INFO:     1865872711014914813079352 column=cf1:type, timestamp=1494988387789, value=24979
[11:04:46] INFO:    
 1865872711014914813079353 column=cf1:time, timestamp=1494988387789, value=123456426
[11:04:46] INFO:    
 1865872711014914813079353 column=cf1:type, timestamp=1494988387789, value=2474
[11:04:46] INFO:    
 1865872711014914813079354 column=cf1:time, timestamp=1494988387789, value=123456630
 1865872711014914813079354
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24442
 
[11:04:46] INFO:    1865872711014914813079356 column=cf1:time, timestamp=1494988387789, value=123456317
 
[11:04:46] INFO:    1865872711014914813079356 column=cf1:type, timestamp=1494988387789, value=24619
 
[11:04:46] INFO:    1865872711014914813079358 column=cf1:time, timestamp=1494988387789, value=123456742
 
[11:04:46] INFO:    1865872711014914813079358 column=cf1:type, timestamp=1494988387789, value=24648

[11:04:46] INFO:     1865872711014914813079359 column=cf1:time, timestamp=1494988387789, value=12345622
[11:04:46] INFO:    
 1865872711014914813079359 column=cf1:type, timestamp=1494988387789, value=24755
[11:04:46] INFO:    
 186587271101491481307936 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456497
 186587271101491481307936
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24122
 
[11:04:46] INFO:    1865872711014914813079361 column=cf1:time, timestamp=1494988387789, value=123456726
 
[11:04:46] INFO:    1865872711014914813079361 column=cf1:type, timestamp=1494988387789, value=24463
 
[11:04:46] INFO:    1865872711014914813079363 column=cf1:time, timestamp=1494988387789, value=123456111
 1865872711014914813079363
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24233
 
[11:04:46] INFO:    1865872711014914813079364 column=cf1:time, timestamp=1494988387789, value=123456662
 1865872711014914813079364
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24597
 
[11:04:46] INFO:    1865872711014914813079367 column=cf1:time, timestamp=1494988387789, value=123456161
 
[11:04:46] INFO:    1865872711014914813079367 column=cf1:type, timestamp=1494988387789, value=24122

[11:04:46] INFO:     1865872711014914813079368 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456447
 1865872711014914813079368 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24328
 1865872711014914813079370 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456859
 1865872711014914813079370 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24262
 
[11:04:46] INFO:    1865872711014914813079372 column=cf1:time, timestamp=1494988387789, value=12345618
 
[11:04:46] INFO:    1865872711014914813079372 column=cf1:type, timestamp=1494988387789, value=24508
 
[11:04:46] INFO:    1865872711014914813079374 column=cf1:time, timestamp=1494988387789, value=123456400
 1865872711014914813079374 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24525
 1865872711014914813079375 column=cf1:time, timestamp=1494988387789, value=12345655

[11:04:46] INFO:     1865872711014914813079375 column=cf1:type, timestamp=1494988387789, value=24398
[11:04:46] INFO:    
 1865872711014914813079379
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456508

[11:04:46] INFO:     1865872711014914813079379 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24709
 186587271101491481307938 column=cf1:time, timestamp=1494988387789, value=123456378
[11:04:46] INFO:    
 186587271101491481307938 column=cf1:type, timestamp=1494988387789, value=24318
[11:04:46] INFO:    
 1865872711014914813079382 column=cf1:time, timestamp=1494988387789, value=12345696
 
[11:04:46] INFO:    1865872711014914813079382 column=cf1:type, timestamp=1494988387789, value=24287
 1865872711014914813079384 column=cf1:time, timestamp=1494988387789, value=123456878
[11:04:46] INFO:    
 1865872711014914813079384 column=cf1:type, timestamp=1494988387789, value=24897
 1865872711014914813079386 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456602
 1865872711014914813079386 column=cf1:type, timestamp=1494988387789, value=24719

[11:04:46] INFO:     1865872711014914813079389 column=cf1:time, timestamp=1494988387789, value=123456570

[11:04:46] INFO:     1865872711014914813079389 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24633
 1865872711014914813079390 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456885
 1865872711014914813079390 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24622
 1865872711014914813079391 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456378
 1865872711014914813079391 column=cf1:type, timestamp=1494988387789, value=24513
 1865872711014914813079393 column=cf1:time, timestamp=1494988387789, value=12345697
[11:04:46] INFO:    
 1865872711014914813079393
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24692
 1865872711014914813079394 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456560
 1865872711014914813079394 column=cf1:type, timestamp=1494988387789, value=24982

[11:04:46] INFO:     1865872711014914813079395 column=cf1:time, timestamp=1494988387789, value=123456952
 
[11:04:46] INFO:    1865872711014914813079395 column=cf1:type, timestamp=1494988387789, value=24708
 
[11:04:46] INFO:    186587271101491481307940 column=cf1:time, timestamp=1494988387789, value=123456826
 186587271101491481307940
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=244
 1865872711014914813079401 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345611
 
[11:04:46] INFO:    1865872711014914813079401 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24566

[11:04:46] INFO:     1865872711014914813079402 column=cf1:time, timestamp=1494988387789, value=123456366
[11:04:46] INFO:    
 1865872711014914813079402 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24472
 1865872711014914813079403 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456711
 1865872711014914813079403 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2438
 
[11:04:46] INFO:    1865872711014914813079404 column=cf1:time, timestamp=1494988387789, value=123456946
 1865872711014914813079404 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24695
 1865872711014914813079405 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456379
 
[11:04:46] INFO:    1865872711014914813079405 column=cf1:type, timestamp=1494988387789, value=24149
 1865872711014914813079406 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456403

[11:04:46] INFO:     1865872711014914813079406 column=cf1:type, timestamp=1494988387789, value=24953

[11:04:46] INFO:     1865872711014914813079408 column=cf1:time, timestamp=1494988387789, value=12345617
 
[11:04:46] INFO:    1865872711014914813079408 column=cf1:type, timestamp=1494988387789, value=24352
 
[11:04:46] INFO:    1865872711014914813079409 column=cf1:time, timestamp=1494988387789, value=123456102
[11:04:46] INFO:    
 1865872711014914813079409
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24989
 
[11:04:46] INFO:    186587271101491481307941 column=cf1:time, timestamp=1494988387789, value=123456173
[11:04:46] INFO:    
 186587271101491481307941 column=cf1:type, timestamp=1494988387789, value=24405
[11:04:46] INFO:    
 1865872711014914813079412
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=12345646
 1865872711014914813079412
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2491

[11:04:46] INFO:     1865872711014914813079413 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345647

[11:04:46] INFO:     1865872711014914813079413 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24176
 
[11:04:46] INFO:    1865872711014914813079415 column=cf1:time, timestamp=1494988387789, value=123456118
[11:04:46] INFO:    
 1865872711014914813079415
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2434

[11:04:46] INFO:     1865872711014914813079416 column=cf1:time, timestamp=1494988387789, value=123456725
[11:04:46] INFO:    
 1865872711014914813079416 column=cf1:type, timestamp=1494988387789, value=24734
[11:04:46] INFO:    
 1865872711014914813079418 column=cf1:time, timestamp=1494988387789, value=123456552

[11:04:46] INFO:     1865872711014914813079418 column=cf1:type, timestamp=1494988387789, value=24660

[11:04:46] INFO:     1865872711014914813079420 column=cf1:time, timestamp=1494988387789, value=123456193
 1865872711014914813079420 column=cf1:type, timestamp=1494988387789, value=2427
 1865872711014914813079421 column=cf1:time, timestamp=1494988387789, value=123456268
 1865872711014914813079421 column=cf1:type, timestamp=1494988387789, value=24326
 1865872711014914813079422 column=cf1:time, timestamp=1494988387789, value=123456228
 1865872711014914813079422 column=cf1:type, timestamp=1494988387789, value=24767
 1865872711014914813079424 column=cf1:time, timestamp=1494988387789, value=123456463
 1865872711014914813079424 column=cf1:type, timestamp=1494988387789, value=24463
 1865872711014914813079425 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456168
 1865872711014914813079425 column=cf1:type, timestamp=1494988387789, value=24653
[11:04:46] INFO:    
 1865872711014914813079426 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456471
 
[11:04:46] INFO:    1865872711014914813079426 column=cf1:type, timestamp=1494988387789, value=24602
[11:04:46] INFO:    
 1865872711014914813079427
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456267
 1865872711014914813079427
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24676
[11:04:46] INFO:    
 1865872711014914813079429
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456631
[11:04:46] INFO:    
 1865872711014914813079429
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24921

[11:04:46] INFO:     186587271101491481307943
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456738

[11:04:46] INFO:     186587271101491481307943 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24677

[11:04:46] INFO:     1865872711014914813079430
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456239
[11:04:46] INFO:    
 1865872711014914813079430
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24204
[11:04:46] INFO:    

[11:04:46] INFO:     1865872711014914813079433
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456291

[11:04:46] INFO:     1865872711014914813079433 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24110

[11:04:46] INFO:     1865872711014914813079434 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456707
 1865872711014914813079434
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24266
 
[11:04:46] INFO:    1865872711014914813079435 column=cf1:time, timestamp=1494988387789, value=12345692

[11:04:46] INFO:     1865872711014914813079435 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24337
 1865872711014914813079436
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456569

[11:04:46] INFO:     1865872711014914813079436 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24962
 1865872711014914813079438
[11:04:46] INFO:     
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=1234568
 
[11:04:46] INFO:    1865872711014914813079438 column=cf1:type, timestamp=1494988387789, value=24309

[11:04:46] INFO:     1865872711014914813079439
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456517

[11:04:46] INFO:     1865872711014914813079439
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24403
[11:04:46] INFO:    
 186587271101491481307944 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456509
 
[11:04:46] INFO:    186587271101491481307944 column=cf1:type, timestamp=1494988387789, value=2457
[11:04:46] INFO:    
 1865872711014914813079441 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456574

[11:04:46] INFO:     1865872711014914813079441 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24435

[11:04:46] INFO:     1865872711014914813079442 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456899
 
[11:04:46] INFO:    1865872711014914813079442 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24848
 1865872711014914813079443
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456164

[11:04:46] INFO:     1865872711014914813079443 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24622
 1865872711014914813079444
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456731
 1865872711014914813079444
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24504
[11:04:46] INFO:    
 1865872711014914813079446
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456499

[11:04:46] INFO:     1865872711014914813079446 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079447 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456998
 1865872711014914813079447 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24162
 1865872711014914813079448 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345655

[11:04:46] INFO:     1865872711014914813079448 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24870
 1865872711014914813079449
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456943
 1865872711014914813079449
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2483

[11:04:46] INFO:     186587271101491481307945 column=cf1:time, timestamp=1494988387789, value=123456385
[11:04:46] INFO:    
 186587271101491481307945
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24944

[11:04:46] INFO:     1865872711014914813079450 column=cf1:time, timestamp=1494988387789, value=12345698
[11:04:46] INFO:    
 1865872711014914813079450
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24400

[11:04:46] INFO:     1865872711014914813079452
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456416

[11:04:46] INFO:     1865872711014914813079452 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24347

[11:04:46] INFO:     1865872711014914813079453
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456158
[11:04:46] INFO:    
 1865872711014914813079453
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079454 column=cf1:time, timestamp=1494988387789, value=123456173

[11:04:46] INFO:     1865872711014914813079454 column=cf1:type, timestamp=1494988387789, value=24366
 1865872711014914813079457 column=cf1:time, timestamp=1494988387789, value=123456262
 
[11:04:46] INFO:    1865872711014914813079457 column=cf1:type, timestamp=1494988387789, value=24598
 1865872711014914813079458 column=cf1:time, timestamp=1494988387789, value=123456540
 
[11:04:46] INFO:    1865872711014914813079458 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079459 column=cf1:time, timestamp=1494988387789, value=123456724
 1865872711014914813079459 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24728
 186587271101491481307946 column=cf1:time, timestamp=1494988387789, value=123456925
 186587271101491481307946 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079460
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456117
 1865872711014914813079460 column=cf1:type, timestamp=1494988387789, value=24750
 1865872711014914813079464 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456504
 1865872711014914813079464 column=cf1:type, timestamp=1494988387789, value=24678
 1865872711014914813079466 column=cf1:time, timestamp=1494988387789, value=123456401
 
[11:04:46] INFO:    1865872711014914813079466 column=cf1:type, timestamp=1494988387789, value=24960
 1865872711014914813079468 column=cf1:time, timestamp=1494988387789, value=123456306
 1865872711014914813079468 column=cf1:type, timestamp=1494988387789, value=24536
[11:04:46] INFO:    
 1865872711014914813079469 column=cf1:time, timestamp=1494988387789, value=123456406
 1865872711014914813079469 column=cf1:type, timestamp=1494988387789, value=24980
 1865872711014914813079471
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456236
 1865872711014914813079471 column=cf1:type, timestamp=1494988387789, value=24583
 1865872711014914813079472
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079472 column=cf1:type, timestamp=1494988387789, value=2491
 1865872711014914813079473 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456150
 1865872711014914813079473 column=cf1:type, timestamp=1494988387789, value=24582
 1865872711014914813079474 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456639
 1865872711014914813079474 column=cf1:type, timestamp=1494988387789, value=24702
 1865872711014914813079475 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079475 column=cf1:type, timestamp=1494988387789, value=24461
 1865872711014914813079479 column=cf1:time, timestamp=1494988387789, value=123456427
[11:04:46] INFO:    
 1865872711014914813079479 column=cf1:type, timestamp=1494988387789, value=24527
 186587271101491481307948 column=cf1:time, timestamp=1494988387789, value=123456837
 
[11:04:46] INFO:    186587271101491481307948 column=cf1:type, timestamp=1494988387789, value=24668
 1865872711014914813079481 column=cf1:time, timestamp=1494988387789, value=123456204
 1865872711014914813079481 column=cf1:type, timestamp=1494988387789, value=24891

[11:04:46] INFO:     1865872711014914813079482 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079482 column=cf1:type, timestamp=1494988387789, value=24155
 1865872711014914813079483 column=cf1:time, timestamp=1494988387789, value=123456796
 1865872711014914813079483 column=cf1:type, timestamp=1494988387789, value=24227
 1865872711014914813079485 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456592
 1865872711014914813079485 column=cf1:type, timestamp=1494988387789, value=24690
 1865872711014914813079487 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079487 column=cf1:type, timestamp=1494988387789, value=24253
 1865872711014914813079488 column=cf1:time, timestamp=1494988387789, value=123456243

[11:04:46] INFO:     1865872711014914813079488 column=cf1:type, timestamp=1494988387789, value=24627
 186587271101491481307949 column=cf1:time, timestamp=1494988387789, value=123456179
 
[11:04:46] INFO:    186587271101491481307949 column=cf1:type, timestamp=1494988387789, value=24693
 1865872711014914813079493 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079493 column=cf1:type, timestamp=1494988387789, value=24837

[11:04:46] INFO:     1865872711014914813079495 column=cf1:time, timestamp=1494988387789, value=1234561
 1865872711014914813079495 column=cf1:type, timestamp=1494988387789, value=24597

[11:04:46] INFO:     1865872711014914813079496 column=cf1:time, timestamp=1494988387789, value=123456991
 1865872711014914813079496 column=cf1:type, timestamp=1494988387789, value=2498
 1865872711014914813079497
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456791
 1865872711014914813079497 column=cf1:type, timestamp=1494988387789, value=24540
 1865872711014914813079498 column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079498
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24514
 1865872711014914813079499 column=cf1:time, timestamp=1494988387789, value=12345647
 1865872711014914813079499 column=cf1:type, timestamp=1494988387789, value=24760
 
[11:04:46] INFO:    18658727110149148130795 column=cf1:time, timestamp=1494988387789, value=123456118
 18658727110149148130795 column=cf1:type, timestamp=1494988387789, value=24128
 186587271101491481307950 column=cf1:time, timestamp=1494988387789, value=123456613
[11:04:46] INFO:    
 186587271101491481307950 column=cf1:type, timestamp=1494988387789, value=24205
 1865872711014914813079500 column=cf1:time, timestamp=1494988387789, value=123456586
 1865872711014914813079500 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079501 column=cf1:time, timestamp=1494988387789, value=123456951
 
[11:04:46] INFO:    1865872711014914813079501 column=cf1:type, timestamp=1494988387789, value=24169
 1865872711014914813079503 column=cf1:time, timestamp=1494988387789, value=123456179

[11:04:46] INFO:     1865872711014914813079503 column=cf1:type, timestamp=1494988387789, value=24868

[11:04:46] INFO:     1865872711014914813079507 column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079507
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2426
 1865872711014914813079508 column=cf1:time, timestamp=1494988387789, value=123456660
 
[11:04:46] INFO:    1865872711014914813079508 column=cf1:type, timestamp=1494988387789, value=24363
 186587271101491481307951 column=cf1:time, timestamp=1494988387789, value=123456329
 
[11:04:46] INFO:    186587271101491481307951 column=cf1:type, timestamp=1494988387789, value=24998
 1865872711014914813079511 column=cf1:time, timestamp=1494988387789, value=123456383

[11:04:46] INFO:     1865872711014914813079511 column=cf1:type, timestamp=1494988387789, value=24907
 1865872711014914813079512 column=cf1:time, timestamp=1494988387789, value=123456594

[11:04:46] INFO:     1865872711014914813079512 column=cf1:type, timestamp=1494988387789, value=24522
 1865872711014914813079513 column=cf1:time, timestamp=1494988387789, value=123456368
 1865872711014914813079513 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24242
 1865872711014914813079515 column=cf1:time, timestamp=1494988387789, value=12345636
 1865872711014914813079515 column=cf1:type, timestamp=1494988387789, value=2495
 1865872711014914813079519 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456993
 1865872711014914813079519 column=cf1:type, timestamp=1494988387789, value=24516
 186587271101491481307952 column=cf1:time, timestamp=1494988387789, value=123456174

[11:04:46] INFO:     186587271101491481307952 column=cf1:type, timestamp=1494988387789, value=2496
 1865872711014914813079520 column=cf1:time, timestamp=1494988387789, value=123456149
 1865872711014914813079520 column=cf1:type, timestamp=1494988387789, value=24666
 
[11:04:46] INFO:    1865872711014914813079521 column=cf1:time, timestamp=1494988387789, value=123456352
 1865872711014914813079521 column=cf1:type, timestamp=1494988387789, value=24225
 1865872711014914813079523 column=cf1:time, timestamp=1494988387789, value=123456317
 
[11:04:46] INFO:    1865872711014914813079523 column=cf1:type, timestamp=1494988387789, value=24737
 1865872711014914813079524 column=cf1:time, timestamp=1494988387789, value=123456159
 1865872711014914813079524 column=cf1:type, timestamp=1494988387789, value=24287
 
[11:04:46] INFO:    1865872711014914813079525 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079525 column=cf1:type, timestamp=1494988387789, value=24458
 1865872711014914813079526
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456252
 1865872711014914813079526 column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079527 column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079527 column=cf1:type, timestamp=1494988387789, value=24788

[11:04:46] INFO:     1865872711014914813079529 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079529 column=cf1:type, timestamp=1494988387789, value=24656
 186587271101491481307953 column=cf1:time, timestamp=1494988387789, value=123456621
 186587271101491481307953 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2448
 1865872711014914813079530 column=cf1:time, timestamp=1494988387789, value=123456585
 1865872711014914813079530 column=cf1:type, timestamp=1494988387789, value=24158
 1865872711014914813079532 column=cf1:time, timestamp=1494988387789, value=12345621
 1865872711014914813079532
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24642
 1865872711014914813079534 column=cf1:time, timestamp=1494988387789, value=123456620
 1865872711014914813079534 column=cf1:type, timestamp=1494988387789, value=24809
 1865872711014914813079536 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456288
 1865872711014914813079536 column=cf1:type, timestamp=1494988387789, value=24534
 1865872711014914813079537 column=cf1:time, timestamp=1494988387789, value=123456262
 1865872711014914813079537 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24666
 1865872711014914813079538 column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079538 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079539 column=cf1:time, timestamp=1494988387789, value=12345676
 1865872711014914813079539 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24728
 186587271101491481307954 column=cf1:time, timestamp=1494988387789, value=12345687
 186587271101491481307954 column=cf1:type, timestamp=1494988387789, value=24737
 1865872711014914813079541 column=cf1:time, timestamp=1494988387789, value=123456418
 1865872711014914813079541 column=cf1:type, timestamp=1494988387789, value=24587
 1865872711014914813079542 column=cf1:time, timestamp=1494988387789, value=123456214
[11:04:46] INFO:    
 1865872711014914813079542 column=cf1:type, timestamp=1494988387789, value=24951
 1865872711014914813079545 column=cf1:time, timestamp=1494988387789, value=123456357
 1865872711014914813079545 column=cf1:type, timestamp=1494988387789, value=24990
 
[11:04:46] INFO:    1865872711014914813079546 column=cf1:time, timestamp=1494988387789, value=123456578
 1865872711014914813079546 column=cf1:type, timestamp=1494988387789, value=24602
 1865872711014914813079547 column=cf1:time, timestamp=1494988387789, value=123456170
 1865872711014914813079547 column=cf1:type, timestamp=1494988387789, value=24701
[11:04:46] INFO:    
 1865872711014914813079549 column=cf1:time, timestamp=1494988387789, value=123456930
 1865872711014914813079549 column=cf1:type, timestamp=1494988387789, value=24569
 186587271101491481307955 column=cf1:time, timestamp=1494988387789, value=123456371
 186587271101491481307955
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24199
 1865872711014914813079550 column=cf1:time, timestamp=1494988387789, value=123456665
 1865872711014914813079550 column=cf1:type, timestamp=1494988387789, value=24642
 
[11:04:46] INFO:    1865872711014914813079551 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079551 column=cf1:type, timestamp=1494988387789, value=24724
 1865872711014914813079555 column=cf1:time, timestamp=1494988387789, value=12345643
 1865872711014914813079555 column=cf1:type, timestamp=1494988387789, value=24362
[11:04:46] INFO:    
 1865872711014914813079556 column=cf1:time, timestamp=1494988387789, value=123456642
 1865872711014914813079556 column=cf1:type, timestamp=1494988387789, value=24552
 1865872711014914813079557
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456128
 1865872711014914813079557 column=cf1:type, timestamp=1494988387789, value=24762
 1865872711014914813079558 column=cf1:time, timestamp=1494988387789, value=123456949
 1865872711014914813079558 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=2481
 1865872711014914813079560 column=cf1:time, timestamp=1494988387789, value=1234566
 1865872711014914813079560 column=cf1:type, timestamp=1494988387789, value=24526
 1865872711014914813079561 column=cf1:time, timestamp=1494988387789, value=123456723
 1865872711014914813079561 column=cf1:type, timestamp=1494988387789, value=24304
 1865872711014914813079565 column=cf1:time, timestamp=1494988387789, value=123456123
 1865872711014914813079565 column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079567 column=cf1:time, timestamp=1494988387789, value=123456763
[11:04:46] INFO:    
 1865872711014914813079567 column=cf1:type, timestamp=1494988387789, value=24969
 1865872711014914813079568 column=cf1:time, timestamp=1494988387789, value=123456980
 1865872711014914813079568 column=cf1:type, timestamp=1494988387789, value=24406
 1865872711014914813079569 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=1234566
 1865872711014914813079569 column=cf1:type, timestamp=1494988387789, value=24983
 186587271101491481307957 column=cf1:time, timestamp=1494988387789, value=123456160
 186587271101491481307957 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24746
 1865872711014914813079570 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079570 column=cf1:type, timestamp=1494988387789, value=24705
 1865872711014914813079572 column=cf1:time, timestamp=1494988387789, value=123456501

[11:04:46] INFO:     1865872711014914813079572 column=cf1:type, timestamp=1494988387789, value=2410
 1865872711014914813079574 column=cf1:time, timestamp=1494988387789, value=123456271
 1865872711014914813079574 column=cf1:type, timestamp=1494988387789, value=24354
 
[11:04:46] INFO:    1865872711014914813079576 column=cf1:time, timestamp=1494988387789, value=123456836
 1865872711014914813079576 column=cf1:type, timestamp=1494988387789, value=24163
 1865872711014914813079577 column=cf1:time, timestamp=1494988387789, value=123456705

[11:04:46] INFO:     1865872711014914813079577 column=cf1:type, timestamp=1494988387789, value=24185
 1865872711014914813079578 column=cf1:time, timestamp=1494988387789, value=123456102
 1865872711014914813079578
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24302
 1865872711014914813079579 column=cf1:time, timestamp=1494988387789, value=12345690
 1865872711014914813079579 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24601
 186587271101491481307958 column=cf1:time, timestamp=1494988387789, value=123456124
 186587271101491481307958 column=cf1:type, timestamp=1494988387789, value=24958
 
[11:04:46] INFO:    1865872711014914813079580 column=cf1:time, timestamp=1494988387789, value=123456280
 1865872711014914813079580 column=cf1:type, timestamp=1494988387789, value=24320
 1865872711014914813079582
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456986
 1865872711014914813079582 column=cf1:type, timestamp=1494988387789, value=24955
 1865872711014914813079583 column=cf1:time, timestamp=1494988387789, value=123456461
 
[11:04:46] INFO:    1865872711014914813079583 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079584 column=cf1:time, timestamp=1494988387789, value=123456177
 1865872711014914813079584 column=cf1:type, timestamp=1494988387789, value=24519
[11:04:46] INFO:    
 1865872711014914813079585 column=cf1:time, timestamp=1494988387789, value=123456803
 1865872711014914813079585 column=cf1:type, timestamp=1494988387789, value=24968

[11:04:46] INFO:     1865872711014914813079587 column=cf1:time, timestamp=1494988387789, value=123456174
 1865872711014914813079587 column=cf1:type, timestamp=1494988387789, value=24431

[11:04:46] INFO:     1865872711014914813079589 column=cf1:time, timestamp=1494988387789, value=123456608
 1865872711014914813079589
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079590 column=cf1:time, timestamp=1494988387789, value=123456815
 1865872711014914813079590 column=cf1:type, timestamp=1494988387789, value=24997
 
[11:04:46] INFO:    1865872711014914813079592 column=cf1:time, timestamp=1494988387789, value=12345656
 1865872711014914813079592 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24228
 1865872711014914813079593 column=cf1:time, timestamp=1494988387789, value=123456786

[11:04:46] INFO:     1865872711014914813079593 column=cf1:type, timestamp=1494988387789, value=24307
 1865872711014914813079597 column=cf1:time, timestamp=1494988387789, value=123456317
 
[11:04:46] INFO:    1865872711014914813079597 column=cf1:type, timestamp=1494988387789, value=24207
 1865872711014914813079598 column=cf1:time, timestamp=1494988387789, value=123456850
 1865872711014914813079598 column=cf1:type, timestamp=1494988387789, value=24984

[11:04:46] INFO:     18658727110149148130796 column=cf1:time, timestamp=1494988387789, value=12345647
 18658727110149148130796 column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079603 column=cf1:time, timestamp=1494988387789, value=123456260
 1865872711014914813079603 column=cf1:type, timestamp=1494988387789, value=24618
 1865872711014914813079604 column=cf1:time, timestamp=1494988387789, value=123456217

[11:04:46] INFO:     1865872711014914813079604 column=cf1:type, timestamp=1494988387789, value=2499
 1865872711014914813079605 column=cf1:time, timestamp=1494988387789, value=123456347
 1865872711014914813079605 column=cf1:type, timestamp=1494988387789, value=24225
 1865872711014914813079608 column=cf1:time, timestamp=1494988387789, value=123456919
 1865872711014914813079608 column=cf1:type, timestamp=1494988387789, value=2432
 1865872711014914813079609 column=cf1:time, timestamp=1494988387789, value=123456216
 1865872711014914813079609 column=cf1:type, timestamp=1494988387789, value=24525
 1865872711014914813079610 column=cf1:time, timestamp=1494988387789, value=123456100
 1865872711014914813079610 column=cf1:type, timestamp=1494988387789, value=24675
 1865872711014914813079611 column=cf1:time, timestamp=1494988387789, value=123456653
[11:04:46] INFO:    
 1865872711014914813079611 column=cf1:type, timestamp=1494988387789, value=24975
 1865872711014914813079613 column=cf1:time, timestamp=1494988387789, value=123456482
 1865872711014914813079613 column=cf1:type, timestamp=1494988387789, value=24697
 
[11:04:46] INFO:    1865872711014914813079616 column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079616 column=cf1:type, timestamp=1494988387789, value=24253
 1865872711014914813079617 column=cf1:time, timestamp=1494988387789, value=123456289
[11:04:46] INFO:    
 1865872711014914813079617 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079618 column=cf1:time, timestamp=1494988387789, value=123456285

[11:04:46] INFO:     1865872711014914813079618 column=cf1:type, timestamp=1494988387789, value=24519
 1865872711014914813079619 column=cf1:time, timestamp=1494988387789, value=123456165
 
[11:04:46] INFO:    1865872711014914813079619 column=cf1:type, timestamp=1494988387789, value=24277
 186587271101491481307962 column=cf1:time, timestamp=1494988387789, value=12345625
 
[11:04:46] INFO:    186587271101491481307962 column=cf1:type, timestamp=1494988387789, value=24547
 1865872711014914813079620 column=cf1:time, timestamp=1494988387789, value=123456784
 1865872711014914813079620 column=cf1:type, timestamp=1494988387789, value=24496
 1865872711014914813079621 column=cf1:time, timestamp=1494988387789, value=123456257
 1865872711014914813079621 column=cf1:type, timestamp=1494988387789, value=24806
 1865872711014914813079622 column=cf1:time, timestamp=1494988387789, value=123456229

[11:04:46] INFO:     1865872711014914813079622 column=cf1:type, timestamp=1494988387789, value=24119
 1865872711014914813079623 column=cf1:time, timestamp=1494988387789, value=123456936
 1865872711014914813079623 column=cf1:type, timestamp=1494988387789, value=24489
 
[11:04:46] INFO:    1865872711014914813079625 column=cf1:time, timestamp=1494988387789, value=123456113
 1865872711014914813079625 column=cf1:type, timestamp=1494988387789, value=24517
 1865872711014914813079626 column=cf1:time, timestamp=1494988387789, value=123456391
 1865872711014914813079626 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24579
 1865872711014914813079627 column=cf1:time, timestamp=1494988387789, value=123456109
 1865872711014914813079627 column=cf1:type, timestamp=1494988387789, value=24593
 1865872711014914813079628 column=cf1:time, timestamp=1494988387789, value=123456861

[11:04:46] INFO:     1865872711014914813079628 column=cf1:type, timestamp=1494988387789, value=24716
 1865872711014914813079629 column=cf1:time, timestamp=1494988387789, value=123456737
 1865872711014914813079629 column=cf1:type, timestamp=1494988387789, value=24309
 1865872711014914813079630 column=cf1:time, timestamp=1494988387789, value=123456863
 1865872711014914813079630 column=cf1:type, timestamp=1494988387789, value=24850
 1865872711014914813079631 column=cf1:time, timestamp=1494988387789, value=123456115

[11:04:46] INFO:     1865872711014914813079631 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079633 column=cf1:time, timestamp=1494988387789, value=12345634
 1865872711014914813079633 column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079634 column=cf1:time, timestamp=1494988387789, value=123456658
 1865872711014914813079634 column=cf1:type, timestamp=1494988387789, value=24371
 1865872711014914813079636 column=cf1:time, timestamp=1494988387789, value=123456425
 1865872711014914813079636 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079637 column=cf1:time, timestamp=1494988387789, value=123456420
 1865872711014914813079637 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24451
 1865872711014914813079638 column=cf1:time, timestamp=1494988387789, value=123456712
 1865872711014914813079638 column=cf1:type, timestamp=1494988387789, value=24110
 1865872711014914813079639 column=cf1:time, timestamp=1494988387789, value=123456964
 
[11:04:46] INFO:    1865872711014914813079639 column=cf1:type, timestamp=1494988387789, value=2417
 186587271101491481307964 column=cf1:time, timestamp=1494988387789, value=123456706
 186587271101491481307964 column=cf1:type, timestamp=1494988387789, value=24200
 1865872711014914813079641 column=cf1:time, timestamp=1494988387789, value=123456658

[11:04:46] INFO:     1865872711014914813079641 column=cf1:type, timestamp=1494988387789, value=24436
 1865872711014914813079642 column=cf1:time, timestamp=1494988387789, value=123456171
 1865872711014914813079642 column=cf1:type, timestamp=1494988387789, value=24539
 
[11:04:46] INFO:    1865872711014914813079644 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079644 column=cf1:type, timestamp=1494988387789, value=24610
 1865872711014914813079646 column=cf1:time, timestamp=1494988387789, value=123456259

[11:04:46] INFO:     1865872711014914813079646 column=cf1:type, timestamp=1494988387789, value=24532
 1865872711014914813079647 column=cf1:time, timestamp=1494988387789, value=123456442
 1865872711014914813079647 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24780
 1865872711014914813079648 column=cf1:time, timestamp=1494988387789, value=123456467
 1865872711014914813079648 column=cf1:type, timestamp=1494988387789, value=2416
 186587271101491481307965 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456448
 186587271101491481307965 column=cf1:type, timestamp=1494988387789, value=24560
 1865872711014914813079652 column=cf1:time, timestamp=1494988387789, value=123456196
 1865872711014914813079652 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24371
 1865872711014914813079655 column=cf1:time, timestamp=1494988387789, value=123456111
 1865872711014914813079655 column=cf1:type, timestamp=1494988387789, value=24585
 
[11:04:46] INFO:    1865872711014914813079659 column=cf1:time, timestamp=1494988387789, value=123456305
 1865872711014914813079659 column=cf1:type, timestamp=1494988387789, value=24946
 186587271101491481307966 column=cf1:time, timestamp=1494988387789, value=123456829
 186587271101491481307966 column=cf1:type, timestamp=1494988387789, value=24917
[11:04:46] INFO:    
 1865872711014914813079662 column=cf1:time, timestamp=1494988387789, value=123456964
 1865872711014914813079662 column=cf1:type, timestamp=1494988387789, value=24437
 1865872711014914813079663 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456297
 1865872711014914813079663 column=cf1:type, timestamp=1494988387789, value=2423
 1865872711014914813079665 column=cf1:time, timestamp=1494988387789, value=123456843
 1865872711014914813079665 column=cf1:type, timestamp=1494988387789, value=24593
[11:04:46] INFO:    
 1865872711014914813079667 column=cf1:time, timestamp=1494988387789, value=123456416
 1865872711014914813079667 column=cf1:type, timestamp=1494988387789, value=24593

[11:04:46] INFO:     1865872711014914813079669 column=cf1:time, timestamp=1494988387789, value=123456522
 1865872711014914813079669 column=cf1:type, timestamp=1494988387789, value=24329
 186587271101491481307967 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456269
 186587271101491481307967 column=cf1:type, timestamp=1494988387789, value=24688
 1865872711014914813079671 column=cf1:time, timestamp=1494988387789, value=123456579

[11:04:46] INFO:     1865872711014914813079671 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079672 column=cf1:time, timestamp=1494988387789, value=123456662
 1865872711014914813079672 column=cf1:type, timestamp=1494988387789, value=24686
 1865872711014914813079674 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456276
 1865872711014914813079674 column=cf1:type, timestamp=1494988387789, value=24387
 1865872711014914813079675 column=cf1:time, timestamp=1494988387789, value=123456330
 1865872711014914813079675
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24950
 1865872711014914813079680 column=cf1:time, timestamp=1494988387789, value=123456203
 1865872711014914813079680 column=cf1:type, timestamp=1494988387789, value=24657
 1865872711014914813079681
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456623
 1865872711014914813079681 column=cf1:type, timestamp=1494988387789, value=24408
 1865872711014914813079684 column=cf1:time, timestamp=1494988387789, value=123456900
 1865872711014914813079684 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24760
 1865872711014914813079685 column=cf1:time, timestamp=1494988387789, value=123456488
 1865872711014914813079685 column=cf1:type, timestamp=1494988387789, value=24761
 
[11:04:46] INFO:    1865872711014914813079686 column=cf1:time, timestamp=1494988387789, value=123456237
 1865872711014914813079686 column=cf1:type, timestamp=1494988387789, value=24135
 1865872711014914813079687 column=cf1:time, timestamp=1494988387789, value=123456940
[11:04:46] INFO:    
 1865872711014914813079687 column=cf1:type, timestamp=1494988387789, value=24254
 1865872711014914813079688 column=cf1:time, timestamp=1494988387789, value=123456970
 1865872711014914813079688
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24197
 186587271101491481307969 column=cf1:time, timestamp=1494988387789, value=12345629
 186587271101491481307969 column=cf1:type, timestamp=1494988387789, value=24647
[11:04:46] INFO:    
 1865872711014914813079690 column=cf1:time, timestamp=1494988387789, value=123456761
 1865872711014914813079690 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079692 column=cf1:time, timestamp=1494988387789, value=123456610

[11:04:46] INFO:     1865872711014914813079692 column=cf1:type, timestamp=1494988387789, value=24174
 1865872711014914813079694 column=cf1:time, timestamp=1494988387789, value=123456777
 1865872711014914813079694 column=cf1:type, timestamp=1494988387789, value=24874

[11:04:46] INFO:     1865872711014914813079695 column=cf1:time, timestamp=1494988387789, value=123456111
 1865872711014914813079695 column=cf1:type, timestamp=1494988387789, value=24943
 1865872711014914813079698 column=cf1:time, timestamp=1494988387789, value=123456526

[11:04:46] INFO:     1865872711014914813079698 column=cf1:type, timestamp=1494988387789, value=24361
 1865872711014914813079699 column=cf1:time, timestamp=1494988387789, value=12345618
 1865872711014914813079699 column=cf1:type, timestamp=1494988387789, value=24590
 
[11:04:46] INFO:    186587271101491481307970 column=cf1:time, timestamp=1494988387789, value=123456834
 186587271101491481307970 column=cf1:type, timestamp=1494988387789, value=24880
 1865872711014914813079700 column=cf1:time, timestamp=1494988387789, value=123456689
 1865872711014914813079700 column=cf1:type, timestamp=1494988387789, value=24883

[11:04:46] INFO:     1865872711014914813079701 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079701 column=cf1:type, timestamp=1494988387789, value=24912
 1865872711014914813079703 column=cf1:time, timestamp=1494988387789, value=123456519

[11:04:46] INFO:     1865872711014914813079703 column=cf1:type, timestamp=1494988387789, value=24220
 1865872711014914813079704 column=cf1:time, timestamp=1494988387789, value=123456539
 1865872711014914813079704 column=cf1:type, timestamp=1494988387789, value=24198
 
[11:04:46] INFO:    1865872711014914813079705 column=cf1:time, timestamp=1494988387789, value=123456467
 1865872711014914813079705 column=cf1:type, timestamp=1494988387789, value=24921
 
[11:04:46] INFO:    1865872711014914813079707 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079707 column=cf1:type, timestamp=1494988387789, value=24128
 1865872711014914813079709 column=cf1:time, timestamp=1494988387789, value=123456420
[11:04:46] INFO:    
 1865872711014914813079709 column=cf1:type, timestamp=1494988387789, value=24394
 1865872711014914813079710 column=cf1:time, timestamp=1494988387789, value=123456877

[11:04:46] INFO:     1865872711014914813079710 column=cf1:type, timestamp=1494988387789, value=24720
 1865872711014914813079712 column=cf1:time, timestamp=1494988387789, value=123456880
 1865872711014914813079712 column=cf1:type, timestamp=1494988387789, value=24238

[11:04:46] INFO:     1865872711014914813079714 column=cf1:time, timestamp=1494988387789, value=123456341
 1865872711014914813079714 column=cf1:type, timestamp=1494988387789, value=24743
 1865872711014914813079715 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456903
 1865872711014914813079715 column=cf1:type, timestamp=1494988387789, value=24284
 1865872711014914813079716 column=cf1:time, timestamp=1494988387789, value=123456683
 1865872711014914813079716 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=249
 1865872711014914813079720 column=cf1:time, timestamp=1494988387789, value=123456261
 1865872711014914813079720 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079722 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456238
 1865872711014914813079722 column=cf1:type, timestamp=1494988387789, value=24549
 1865872711014914813079723 column=cf1:time, timestamp=1494988387789, value=123456677

[11:04:46] INFO:     1865872711014914813079723 column=cf1:type, timestamp=1494988387789, value=24136
 1865872711014914813079725 column=cf1:time, timestamp=1494988387789, value=12345623
 1865872711014914813079725 column=cf1:type, timestamp=1494988387789, value=24350
[11:04:46] INFO:    
 1865872711014914813079727 column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079727 column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079728 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456481
 1865872711014914813079728 column=cf1:type, timestamp=1494988387789, value=24207
 1865872711014914813079729 column=cf1:time, timestamp=1494988387789, value=123456627
 1865872711014914813079729
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2422
 186587271101491481307973 column=cf1:time, timestamp=1494988387789, value=123456109
 186587271101491481307973 column=cf1:type, timestamp=1494988387789, value=24889
 1865872711014914813079731 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456841
 1865872711014914813079731 column=cf1:type, timestamp=1494988387789, value=24771
 1865872711014914813079732 column=cf1:time, timestamp=1494988387789, value=123456787

[11:04:46] INFO:     1865872711014914813079732 column=cf1:type, timestamp=1494988387789, value=24853
 1865872711014914813079734 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079734 column=cf1:type, timestamp=1494988387789, value=24516
 1865872711014914813079735 column=cf1:time, timestamp=1494988387789, value=123456781

[11:04:46] INFO:     1865872711014914813079735 column=cf1:type, timestamp=1494988387789, value=24356
 1865872711014914813079736 column=cf1:time, timestamp=1494988387789, value=123456391
 
[11:04:46] INFO:    1865872711014914813079736 column=cf1:type, timestamp=1494988387789, value=24125
 1865872711014914813079739 column=cf1:time, timestamp=1494988387789, value=123456438
 1865872711014914813079739 column=cf1:type, timestamp=1494988387789, value=24241
 186587271101491481307974 column=cf1:time, timestamp=1494988387789, value=123456590

[11:04:46] INFO:     186587271101491481307974 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079741 column=cf1:time, timestamp=1494988387789, value=12345691
 1865872711014914813079741 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=249
 1865872711014914813079742 column=cf1:time, timestamp=1494988387789, value=123456358
 1865872711014914813079742 column=cf1:type, timestamp=1494988387789, value=24785
 1865872711014914813079743 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079743 column=cf1:type, timestamp=1494988387789, value=24109
 1865872711014914813079744 column=cf1:time, timestamp=1494988387789, value=123456600
 
[11:04:46] INFO:    1865872711014914813079744 column=cf1:type, timestamp=1494988387789, value=24182
 1865872711014914813079745 column=cf1:time, timestamp=1494988387789, value=123456399
 1865872711014914813079745 column=cf1:type, timestamp=1494988387789, value=24766
 1865872711014914813079746 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345683
 1865872711014914813079746 column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079747 column=cf1:time, timestamp=1494988387789, value=123456228
 
[11:04:46] INFO:    1865872711014914813079747 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079749 column=cf1:time, timestamp=1494988387789, value=123456591
 1865872711014914813079749 column=cf1:type, timestamp=1494988387789, value=24209
 
[11:04:46] INFO:    1865872711014914813079751 column=cf1:time, timestamp=1494988387789, value=123456461
 1865872711014914813079751 column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079752 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456320
 1865872711014914813079752 column=cf1:type, timestamp=1494988387789, value=24715
 1865872711014914813079756 column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079756
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24388
 186587271101491481307976 column=cf1:time, timestamp=1494988387789, value=12345618
 186587271101491481307976 column=cf1:type, timestamp=1494988387789, value=24547
[11:04:46] INFO:    
 1865872711014914813079761 column=cf1:time, timestamp=1494988387789, value=123456777
 1865872711014914813079761 column=cf1:type, timestamp=1494988387789, value=24505
 1865872711014914813079764 column=cf1:time, timestamp=1494988387789, value=123456233
[11:04:46] INFO:    
 1865872711014914813079764 column=cf1:type, timestamp=1494988387789, value=24713
 1865872711014914813079765 column=cf1:time, timestamp=1494988387789, value=123456359
 1865872711014914813079765 column=cf1:type, timestamp=1494988387789, value=24285
 1865872711014914813079766 column=cf1:time, timestamp=1494988387789, value=123456417
 1865872711014914813079766
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=242
 1865872711014914813079767 column=cf1:time, timestamp=1494988387789, value=123456876
 1865872711014914813079767 column=cf1:type, timestamp=1494988387789, value=2493
[11:04:46] INFO:    
 1865872711014914813079769 column=cf1:time, timestamp=1494988387789, value=123456309
 1865872711014914813079769 column=cf1:type, timestamp=1494988387789, value=24823
 1865872711014914813079770 column=cf1:time, timestamp=1494988387789, value=123456791
 1865872711014914813079770 column=cf1:type, timestamp=1494988387789, value=24220
 1865872711014914813079771 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079771 column=cf1:type, timestamp=1494988387789, value=24701
 1865872711014914813079772 column=cf1:time, timestamp=1494988387789, value=123456920
 1865872711014914813079772 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079774 column=cf1:time, timestamp=1494988387789, value=123456275
 1865872711014914813079774 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079776 column=cf1:time, timestamp=1494988387789, value=123456227
 1865872711014914813079776 column=cf1:type, timestamp=1494988387789, value=24129
 1865872711014914813079777 column=cf1:time, timestamp=1494988387789, value=123456766
 1865872711014914813079777 column=cf1:type, timestamp=1494988387789, value=24123
[11:04:46] INFO:    
 1865872711014914813079778 column=cf1:time, timestamp=1494988387789, value=123456152
 1865872711014914813079778 column=cf1:type, timestamp=1494988387789, value=2490
 1865872711014914813079779 column=cf1:time, timestamp=1494988387789, value=123456222
[11:04:46] INFO:    
 1865872711014914813079779 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079780 column=cf1:time, timestamp=1494988387789, value=123456635
 1865872711014914813079780
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24132
 1865872711014914813079782 column=cf1:time, timestamp=1494988387789, value=123456173
 1865872711014914813079782 column=cf1:type, timestamp=1494988387789, value=24282
[11:04:46] INFO:    
 1865872711014914813079783 column=cf1:time, timestamp=1494988387789, value=123456813
 1865872711014914813079783 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079786 column=cf1:time, timestamp=1494988387789, value=12345670
 
[11:04:46] INFO:    1865872711014914813079786 column=cf1:type, timestamp=1494988387789, value=2426
 1865872711014914813079787 column=cf1:time, timestamp=1494988387789, value=123456634
 1865872711014914813079787 column=cf1:type, timestamp=1494988387789, value=24945
 1865872711014914813079788 column=cf1:time, timestamp=1494988387789, value=123456554
 1865872711014914813079788
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24428
 186587271101491481307979 column=cf1:time, timestamp=1494988387789, value=123456105
 186587271101491481307979 column=cf1:type, timestamp=1494988387789, value=24162
 1865872711014914813079790
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456641
 1865872711014914813079790 column=cf1:type, timestamp=1494988387789, value=24103
 1865872711014914813079791 column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079791 column=cf1:type, timestamp=1494988387789, value=24403
 1865872711014914813079792
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456106
 1865872711014914813079792 column=cf1:type, timestamp=1494988387789, value=24561
 1865872711014914813079793 column=cf1:time, timestamp=1494988387789, value=123456435
 1865872711014914813079793 column=cf1:type, timestamp=1494988387789, value=24514
 1865872711014914813079794
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456576
 1865872711014914813079794 column=cf1:type, timestamp=1494988387789, value=24865
 1865872711014914813079795 column=cf1:time, timestamp=1494988387789, value=123456726
 1865872711014914813079795 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24608
 1865872711014914813079796 column=cf1:time, timestamp=1494988387789, value=123456473
 1865872711014914813079796 column=cf1:type, timestamp=1494988387789, value=24242
 1865872711014914813079798 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456172
 1865872711014914813079798 column=cf1:type, timestamp=1494988387789, value=24226
 1865872711014914813079799 column=cf1:time, timestamp=1494988387789, value=123456830
 1865872711014914813079799 column=cf1:type, timestamp=1494988387789, value=24722
 18658727110149148130798 column=cf1:time, timestamp=1494988387789, value=123456779
 18658727110149148130798 column=cf1:type, timestamp=1494988387789, value=24498
 1865872711014914813079800 column=cf1:time, timestamp=1494988387789, value=123456914
 1865872711014914813079800 column=cf1:type, timestamp=1494988387789, value=24390
 1865872711014914813079801 column=cf1:time, timestamp=1494988387789, value=123456900
 1865872711014914813079801 column=cf1:type, timestamp=1494988387789, value=2473
 1865872711014914813079803 column=cf1:time, timestamp=1494988387789, value=123456731
 1865872711014914813079803 column=cf1:type, timestamp=1494988387789, value=24103
 1865872711014914813079804 column=cf1:time, timestamp=149498
[11:04:46] INFO:    8387789, value=123456815
 1865872711014914813079804 column=cf1:type, timestamp=1494988387789, value=24250
 1865872711014914813079805 column=cf1:time, timestamp=1494988387789, value=123456316
 1865872711014914813079805 column=cf1:type, timestamp=1494988387789, value=24673
 1865872711014914813079806 column=cf1:time, timestamp=1494988387789, value=123456501
 1865872711014914813079806 column=cf1:type, timestamp=1494988387789, value=24263
 1865872711014914813079807 column=cf1:time, timestamp=1494988387789, value=123456229
 1865872711014914813079807 column=cf1:type, timestamp=1494988387789, value=2455
 1865872711014914813079808 column=cf1:time, timestamp=1494988387789, value=123456472
 1865872711014914813079808 column=cf1:type, timestamp=1494988387789, value=24238
 1865872711014914813079809 column=cf1:time, timestamp=1494988387789, value=123456790
 1865872711014914813079809 column=cf1:type, timestamp=1494988387789, value=24604
 1865872711014914813079811 column=cf1:time, timestamp=1494988387789, value=123456490
 186
[11:04:46] INFO:    5872711014914813079811 column=cf1:type, timestamp=1494988387789, value=24441
 1865872711014914813079814 column=cf1:time, timestamp=1494988387789, value=123456872
 1865872711014914813079814 column=cf1:type, timestamp=1494988387789, value=24900
 1865872711014914813079815 column=cf1:time, timestamp=1494988387789, value=123456936
 1865872711014914813079815 column=cf1:type, timestamp=1494988387789, value=24456
 1865872711014914813079817 column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079817 column=cf1:type, timestamp=1494988387789, value=2431
 1865872711014914813079819 column=cf1:time, timestamp=1494988387789, value=123456625
 1865872711014914813079819 column=cf1:type, timestamp=1494988387789, value=24618
 1865872711014914813079821 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079821 column=cf1:type, timestamp=1494988387789, value=24129
 1865872711014914813079822 column=cf1:time, timestamp=1494988387789, value=123456372
 1865872711014914813079822 column
[11:04:46] INFO:    =cf1:type, timestamp=1494988387789, value=24296
 1865872711014914813079823 column=cf1:time, timestamp=1494988387789, value=123456123
 1865872711014914813079823 column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079824 column=cf1:time, timestamp=1494988387789, value=123456285
 1865872711014914813079824 column=cf1:type, timestamp=1494988387789, value=2461
 1865872711014914813079825 column=cf1:time, timestamp=1494988387789, value=123456783
 1865872711014914813079825 column=cf1:type, timestamp=1494988387789, value=24884
 1865872711014914813079827 column=cf1:time, timestamp=1494988387789, value=123456251
 1865872711014914813079827 column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079829 column=cf1:time, timestamp=1494988387789, value=123456608
 1865872711014914813079829 column=cf1:type, timestamp=1494988387789, value=24148
 186587271101491481307983 column=cf1:time, timestamp=1494988387789, value=123456589
 186587271101491481307983 column=cf1:type, timestamp=1494988387
[11:04:46] INFO:    789, value=24971
 1865872711014914813079831 column=cf1:time, timestamp=1494988387789, value=123456528
 1865872711014914813079831 column=cf1:type, timestamp=1494988387789, value=24550
 1865872711014914813079832 column=cf1:time, timestamp=1494988387789, value=123456360
 1865872711014914813079832 column=cf1:type, timestamp=1494988387789, value=24646
 1865872711014914813079833 column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079833 column=cf1:type, timestamp=1494988387789, value=24823
 1865872711014914813079837 column=cf1:time, timestamp=1494988387789, value=123456596
 1865872711014914813079837 column=cf1:type, timestamp=1494988387789, value=24971
 1865872711014914813079838 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079838 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079839 column=cf1:time, timestamp=1494988387789, value=123456335
 1865872711014914813079839 column=cf1:type, timestamp=1494988387789, value=24155
 1865872711
[11:04:46] INFO:    014914813079840 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079840 column=cf1:type, timestamp=1494988387789, value=24846
 1865872711014914813079841 column=cf1:time, timestamp=1494988387789, value=123456414
 1865872711014914813079841 column=cf1:type, timestamp=1494988387789, value=24121
 1865872711014914813079842 column=cf1:time, timestamp=1494988387789, value=123456328
 1865872711014914813079842 column=cf1:type, timestamp=1494988387789, value=24559
 1865872711014914813079843 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079843 column=cf1:type, timestamp=1494988387789, value=2432
 1865872711014914813079844 column=cf1:time, timestamp=1494988387789, value=123456971
 1865872711014914813079844 column=cf1:type, timestamp=1494988387789, value=24932
 1865872711014914813079849 column=cf1:time, timestamp=1494988387789, value=123456955
 1865872711014914813079849 column=cf1:type, timestamp=1494988387789, value=24773
 186587271101491481307985 column=cf1:tim
[11:04:46] INFO:    e, timestamp=1494988387789, value=123456733
 186587271101491481307985 column=cf1:type, timestamp=1494988387789, value=24754
 1865872711014914813079850 column=cf1:time, timestamp=1494988387789, value=123456738
 1865872711014914813079850 column=cf1:type, timestamp=1494988387789, value=24781
 1865872711014914813079852 column=cf1:time, timestamp=1494988387789, value=123456618
 1865872711014914813079852 column=cf1:type, timestamp=1494988387789, value=24856
 1865872711014914813079853 column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079853 column=cf1:type, timestamp=1494988387789, value=24779
 1865872711014914813079854 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079854 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079855 column=cf1:time, timestamp=1494988387789, value=123456774
 1865872711014914813079855 column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079856 column=cf1:time, timestamp=1494988387789, v
[11:04:46] INFO:    alue=123456962
 1865872711014914813079856 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079857 column=cf1:time, timestamp=1494988387789, value=123456475
 1865872711014914813079857 column=cf1:type, timestamp=1494988387789, value=24352
 1865872711014914813079858 column=cf1:time, timestamp=1494988387789, value=123456609
 1865872711014914813079858 column=cf1:type, timestamp=1494988387789, value=24636
 1865872711014914813079860 column=cf1:time, timestamp=1494988387789, value=123456528
 1865872711014914813079860 column=cf1:type, timestamp=1494988387789, value=24664
 1865872711014914813079861 column=cf1:time, timestamp=1494988387789, value=123456275
 1865872711014914813079861 column=cf1:type, timestamp=1494988387789, value=24423
 1865872711014914813079862 column=cf1:time, timestamp=1494988387789, value=123456154
 1865872711014914813079862 column=cf1:type, timestamp=1494988387789, value=24952
 1865872711014914813079863 column=cf1:time, timestamp=1494988387789, value=123456780
 186587271101
[11:04:46] INFO:    4914813079863 column=cf1:type, timestamp=1494988387789, value=2467
 1865872711014914813079865 column=cf1:time, timestamp=1494988387789, value=123456551
 1865872711014914813079865 column=cf1:type, timestamp=1494988387789, value=24341
 186587271101491481307987 column=cf1:time, timestamp=1494988387789, value=123456123
 186587271101491481307987 column=cf1:type, timestamp=1494988387789, value=24841
 1865872711014914813079870 column=cf1:time, timestamp=1494988387789, value=123456564
 1865872711014914813079870 column=cf1:type, timestamp=1494988387789, value=24425
 1865872711014914813079873 column=cf1:time, timestamp=1494988387789, value=123456258
 1865872711014914813079873 column=cf1:type, timestamp=1494988387789, value=2410
 1865872711014914813079874 column=cf1:time, timestamp=1494988387789, value=123456695
 1865872711014914813079874 column=cf1:type, timestamp=1494988387789, value=24257
 1865872711014914813079875 column=cf1:time, timestamp=1494988387789, value=123456294
 1865872711014914813079875 column=cf1:type, t
[11:04:46] INFO:    imestamp=1494988387789, value=24272
 1865872711014914813079876 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079876 column=cf1:type, timestamp=1494988387789, value=24513
 1865872711014914813079878 column=cf1:time, timestamp=1494988387789, value=123456211
 1865872711014914813079878 column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079879 column=cf1:time, timestamp=1494988387789, value=123456961
 1865872711014914813079879 column=cf1:type, timestamp=1494988387789, value=24137
 1865872711014914813079880 column=cf1:time, timestamp=1494988387789, value=12345633
 1865872711014914813079880 column=cf1:type, timestamp=1494988387789, value=24266
 1865872711014914813079881 column=cf1:time, timestamp=1494988387789, value=123456577
 1865872711014914813079881 column=cf1:type, timestamp=1494988387789, value=2480
 1865872711014914813079882 column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079882 column=cf1:type, timestamp=1494988387789, value=2
[11:04:46] INFO:    4438
 1865872711014914813079883 column=cf1:time, timestamp=1494988387789, value=12345663
 1865872711014914813079883 column=cf1:type, timestamp=1494988387789, value=24236
 1865872711014914813079884 column=cf1:time, timestamp=1494988387789, value=123456384
 1865872711014914813079884 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079885 column=cf1:time, timestamp=1494988387789, value=123456198
 1865872711014914813079885 column=cf1:type, timestamp=1494988387789, value=24334
 1865872711014914813079887 column=cf1:time, timestamp=1494988387789, value=123456659
 1865872711014914813079887 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079888 column=cf1:time, timestamp=1494988387789, value=123456218
 1865872711014914813079888 column=cf1:type, timestamp=1494988387789, value=24334
 1865872711014914813079889 column=cf1:time, timestamp=1494988387789, value=123456463
 1865872711014914813079889 column=cf1:type, timestamp=1494988387789, value=24893
 18658727110149148130798
[11:04:46] INFO:    90 column=cf1:time, timestamp=1494988387789, value=123456942
 1865872711014914813079890 column=cf1:type, timestamp=1494988387789, value=24946
 1865872711014914813079893 column=cf1:time, timestamp=1494988387789, value=123456656
 1865872711014914813079893 column=cf1:type, timestamp=1494988387789, value=2419
 1865872711014914813079894 column=cf1:time, timestamp=1494988387789, value=123456708
 1865872711014914813079894 column=cf1:type, timestamp=1494988387789, value=24752
 1865872711014914813079895 column=cf1:time, timestamp=1494988387789, value=123456769
 1865872711014914813079895 column=cf1:type, timestamp=1494988387789, value=24324
 1865872711014914813079896 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079896 column=cf1:type, timestamp=1494988387789, value=24896
 1865872711014914813079897 column=cf1:time, timestamp=1494988387789, value=123456469
 1865872711014914813079897 column=cf1:type, timestamp=1494988387789, value=241
 1865872711014914813079898 column=cf1:time, timestamp=1
[11:04:46] INFO:    494988387789, value=123456813
 1865872711014914813079898 column=cf1:type, timestamp=1494988387789, value=247
 1865872711014914813079899 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079899 column=cf1:type, timestamp=1494988387789, value=24371
 18658727110149148130799 column=cf1:time, timestamp=1494988387789, value=123456381
 18658727110149148130799 column=cf1:type, timestamp=1494988387789, value=24947
 186587271101491481307990 column=cf1:time, timestamp=1494988387789, value=123456469
 186587271101491481307990 column=cf1:type, timestamp=1494988387789, value=24776
 1865872711014914813079901 column=cf1:time, timestamp=1494988387789, value=123456602
 1865872711014914813079901 column=cf1:type, timestamp=1494988387789, value=24694
 1865872711014914813079903 column=cf1:time, timestamp=1494988387789, value=12345649
 1865872711014914813079903 column=cf1:type, timestamp=1494988387789, value=2485
 1865872711014914813079904 column=cf1:time, timestamp=1494988387789, value=1234563
 186587271
[11:04:46] INFO:    1014914813079904 column=cf1:type, timestamp=1494988387789, value=24556
 1865872711014914813079905 column=cf1:time, timestamp=1494988387789, value=123456494
 1865872711014914813079905 column=cf1:type, timestamp=1494988387789, value=24603
 1865872711014914813079907 column=cf1:time, timestamp=1494988387789, value=123456563
 1865872711014914813079907 column=cf1:type, timestamp=1494988387789, value=24488
 1865872711014914813079908 column=cf1:time, timestamp=1494988387789, value=12345669
 1865872711014914813079908 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079909 column=cf1:time, timestamp=1494988387789, value=123456455
 1865872711014914813079909 column=cf1:type, timestamp=1494988387789, value=24443
 186587271101491481307991 column=cf1:time, timestamp=1494988387789, value=12345683
 186587271101491481307991 column=cf1:type, timestamp=1494988387789, value=24473
 1865872711014914813079910 column=cf1:time, timestamp=1494988387789, value=12345662
 1865872711014914813079910 column=cf1:type,
[11:04:46] INFO:     timestamp=1494988387789, value=24793
 1865872711014914813079913 column=cf1:time, timestamp=1494988387789, value=123456493
 1865872711014914813079913 column=cf1:type, timestamp=1494988387789, value=24415
 1865872711014914813079915 column=cf1:time, timestamp=1494988387789, value=123456845
 1865872711014914813079915 column=cf1:type, timestamp=1494988387789, value=24257
 1865872711014914813079917 column=cf1:time, timestamp=1494988387789, value=123456153
 1865872711014914813079917 column=cf1:type, timestamp=1494988387789, value=24703
 1865872711014914813079918 column=cf1:time, timestamp=1494988387789, value=12345627
 1865872711014914813079918 column=cf1:type, timestamp=1494988387789, value=24459
 1865872711014914813079919 column=cf1:time, timestamp=1494988387789, value=123456313
 1865872711014914813079919 column=cf1:type, timestamp=1494988387789, value=24373
 1865872711014914813079920 column=cf1:time, timestamp=1494988387789, value=123456496
 1865872711014914813079920 column=cf1:type, timestamp=1494988387789, val
[11:04:46] INFO:    ue=24545
 1865872711014914813079925 column=cf1:time, timestamp=1494988387789, value=123456186
 1865872711014914813079925 column=cf1:type, timestamp=1494988387789, value=24150
 1865872711014914813079927 column=cf1:time, timestamp=1494988387789, value=123456557
 1865872711014914813079927 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079928 column=cf1:time, timestamp=1494988387789, value=123456683
 1865872711014914813079928 column=cf1:type, timestamp=1494988387789, value=24543
 186587271101491481307993 column=cf1:time, timestamp=1494988387789, value=123456867
 186587271101491481307993 column=cf1:type, timestamp=1494988387789, value=24332
 1865872711014914813079930 column=cf1:time, timestamp=1494988387789, value=123456677
 1865872711014914813079930 column=cf1:type, timestamp=1494988387789, value=24775
 1865872711014914813079932 column=cf1:time, timestamp=1494988387789, value=123456222
 1865872711014914813079932 column=cf1:type, timestamp=1494988387789, value=24711
 18658727110149148130
[11:04:46] INFO:    79935 column=cf1:time, timestamp=1494988387789, value=123456692
 1865872711014914813079935 column=cf1:type, timestamp=1494988387789, value=2471
 1865872711014914813079936 column=cf1:time, timestamp=1494988387789, value=123456248
 1865872711014914813079936 column=cf1:type, timestamp=1494988387789, value=24530
 186587271101491481307994 column=cf1:time, timestamp=1494988387789, value=123456235
 186587271101491481307994 column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079940 column=cf1:time, timestamp=1494988387789, value=123456131
 1865872711014914813079940 column=cf1:type, timestamp=1494988387789, value=24983
 1865872711014914813079943 column=cf1:time, timestamp=1494988387789, value=123456854
 1865872711014914813079943 column=cf1:type, timestamp=1494988387789, value=24553
 1865872711014914813079945 column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079945 column=cf1:type, timestamp=1494988387789, value=24925
 1865872711014914813079946 column=cf1:time, timestamp=
[11:04:46] INFO:    1494988387789, value=123456188
 1865872711014914813079946 column=cf1:type, timestamp=1494988387789, value=24307
 1865872711014914813079947 column=cf1:time, timestamp=1494988387789, value=12345697
 1865872711014914813079947 column=cf1:type, timestamp=1494988387789, value=24272
 1865872711014914813079948 column=cf1:time, timestamp=1494988387789, value=123456913
 1865872711014914813079948 column=cf1:type, timestamp=1494988387789, value=24258
 1865872711014914813079949 column=cf1:time, timestamp=1494988387789, value=123456686
 1865872711014914813079949 column=cf1:type, timestamp=1494988387789, value=24304
 186587271101491481307995 column=cf1:time, timestamp=1494988387789, value=123456953
 186587271101491481307995 column=cf1:type, timestamp=1494988387789, value=24184
 1865872711014914813079950 column=cf1:time, timestamp=1494988387789, value=123456428
 1865872711014914813079950 column=cf1:type, timestamp=1494988387789, value=246
 1865872711014914813079951 column=cf1:time, timestamp=1494988387789, value=123456624
 1
[11:04:46] INFO:    865872711014914813079951 column=cf1:type, timestamp=1494988387789, value=24478
 1865872711014914813079952 column=cf1:time, timestamp=1494988387789, value=123456282
 1865872711014914813079952 column=cf1:type, timestamp=1494988387789, value=2496
 1865872711014914813079953 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079953 column=cf1:type, timestamp=1494988387789, value=2475
 1865872711014914813079954 column=cf1:time, timestamp=1494988387789, value=123456959
 1865872711014914813079954 column=cf1:type, timestamp=1494988387789, value=24785
 1865872711014914813079956 column=cf1:time, timestamp=1494988387789, value=123456194
 1865872711014914813079956 column=cf1:type, timestamp=1494988387789, value=24195
 1865872711014914813079957 column=cf1:time, timestamp=1494988387789, value=123456169
 1865872711014914813079957 column=cf1:type, timestamp=1494988387789, value=24312
 1865872711014914813079959 column=cf1:time, timestamp=1494988387789, value=123456346
 1865872711014914813079959 colum
[11:04:46] INFO:    n=cf1:type, timestamp=1494988387789, value=24493
 186587271101491481307996 column=cf1:time, timestamp=1494988387789, value=123456686
 186587271101491481307996 column=cf1:type, timestamp=1494988387789, value=24556
 1865872711014914813079962 column=cf1:time, timestamp=1494988387789, value=123456997
 1865872711014914813079962 column=cf1:type, timestamp=1494988387789, value=24480
 1865872711014914813079963 column=cf1:time, timestamp=1494988387789, value=12345661
 1865872711014914813079963 column=cf1:type, timestamp=1494988387789, value=24641
 1865872711014914813079964 column=cf1:time, timestamp=1494988387789, value=123456778
 1865872711014914813079964
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24188
 1865872711014914813079965 column=cf1:time, timestamp=1494988387789, value=123456785
 1865872711014914813079965 column=cf1:type, timestamp=1494988387789, value=24752
 1865872711014914813079967 column=cf1:time, timestamp=1494988387789, value=123456220
 1865872711014914813079967 column=cf1:type, timestamp=1494988387789, value=24168
 186587271101491481307997 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=123456785
 186587271101491481307997 column=cf1:type, timestamp=1494988387789, value=24188
 1865872711014914813079970 column=cf1:time, timestamp=1494988387789, value=123456880
 1865872711014914813079970 column=cf1:type, timestamp=1494988387789, value=24355
 1865872711014914813079971 column=cf1:time, timestamp=1494988387789, value=123456894
 1865872711014914813079971 column=cf1:type, timestamp=1494988387789, value=2428
[11:04:46] INFO:    
 1865872711014914813079972 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079972 column=cf1:type, timestamp=1494988387789, value=24536
 1865872711014914813079974
[11:04:46] INFO:     column=cf1:time, timestamp=1494988387789, value=123456193
 1865872711014914813079974 column=cf1:type, timestamp=1494988387789, value=24184
 1865872711014914813079975 column=cf1:time, timestamp=1494988387789, value=12345679
[11:04:46] INFO:    
 1865872711014914813079975 column=cf1:type, timestamp=1494988387789, value=24240
 1865872711014914813079976 column=cf1:time, timestamp=1494988387789, value=123456961
 1865872711014914813079976
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=24830
 1865872711014914813079977 column=cf1:time, timestamp=1494988387789, value=123456290
 1865872711014914813079977 column=cf1:type, timestamp=1494988387789, value=24439
[11:04:46] INFO:    
 1865872711014914813079978 column=cf1:time, timestamp=1494988387789, value=123456581
 1865872711014914813079978 column=cf1:type, timestamp=1494988387789, value=24149
[11:04:46] INFO:    
 1865872711014914813079979 column=cf1:time, timestamp=1494988387789, value=123456550
 1865872711014914813079979 column=cf1:type, timestamp=1494988387789, value=24418
 
[11:04:46] INFO:    186587271101491481307998 column=cf1:time, timestamp=1494988387789, value=123456265
 186587271101491481307998 column=cf1:type, timestamp=1494988387789, value=24589
 1865872711014914813079982 column=cf1:time, timestamp=1494988387789, value=123456147

[11:04:46] INFO:     1865872711014914813079982 column=cf1:type, timestamp=1494988387789, value=24457
 1865872711014914813079983 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079983 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24451
 1865872711014914813079984 column=cf1:time, timestamp=1494988387789, value=123456221
 1865872711014914813079984 column=cf1:type, timestamp=1494988387789, value=24222
 
[11:04:46] INFO:    1865872711014914813079985 column=cf1:time, timestamp=1494988387789, value=123456375
 1865872711014914813079985 column=cf1:type, timestamp=1494988387789, value=24774
 1865872711014914813079986 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079986 column=cf1:type, timestamp=1494988387789, value=24196
 1865872711014914813079987 column=cf1:time, timestamp=1494988387789, value=123456893
 1865872711014914813079987 column=cf1:type, timestamp=1494988387789, value=24961
 1865872711014914813079988 column=cf1:time, timestamp=1494988387789, value=123456402
 1865872711014914813079988 column=cf1:type, timestamp=1494988387789, value=24583
 1865872711014914813079990 column=cf1:time, timestamp=1494988387789, value=123456586
 1865872711014914813079990 column=cf1:type, timestamp=1494988387789, value=24454
 1865872711014914813079991 
[11:04:46] INFO:    column=cf1:time, timestamp=1494988387789, value=12345670
 1865872711014914813079991 column=cf1:type, timestamp=1494988387789, value=24342
 1865872711014914813079992 column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079992 
[11:04:46] INFO:    column=cf1:type, timestamp=1494988387789, value=24948
 1865872711014914813079994 column=cf1:time, timestamp=1494988387789, value=123456693
 1865872711014914813079994 column=cf1:type, timestamp=1494988387789, value=24932
 1865872711014914813079995 column=cf1:time, timestamp=1494988387789, value=123456183

[11:04:46] INFO:     1865872711014914813079995 column=cf1:type, timestamp=1494988387789, value=24958
 1865872711014914813079996 column=cf1:time, timestamp=1494988387789, value=12345624
 1865872711014914813079996 column=cf1:type, timestamp=1494988387789, value=2415
 
[11:04:46] INFO:    1865872711014914813079997 column=cf1:time, timestamp=1494988387789, value=123456215
 1865872711014914813079997 column=cf1:type, timestamp=1494988387789, value=24594
 1865872711014914813079999 column=cf1:time, timestamp=1494988387789, value=123456303
 1865872711014914813079999
[11:04:46] INFO:     column=cf1:type, timestamp=1494988387789, value=2464
 2 column=cf1:q, timestamp=1494919193176, value=qq
 t column=cf1:q, timestamp=1494917634220, value=tv

[11:04:46] INFO:    645 row(s) in 0.7910 seconds


[11:04:47] INFO:    Connection channel closed
[11:04:47] INFO:    Check if exec success or not ... 
[11:04:47] INFO:    Execute successfully for command: hbase shell /home/lp/hbase.shell
[11:04:47] INFO:    Now wait 5 seconds to begin next task ...
[11:04:52] INFO:    Connection channel disconnect
[11:04:52] INFO:    SSH connection shutdown

=============== [2017/05/17 11:07:15, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[11:07:15] INFO:    SSHExec initializing ...
[11:07:15] INFO:    Session initialized and associated with user credential inteast.com
[11:07:15] INFO:    SSHExec initialized successfully
[11:07:15] INFO:    SSHExec trying to connect root@10.1.70.200
[11:07:16] INFO:    SSH connection established
[11:07:16] INFO:    Command is hbase shell /home/lp/hbase.shell
[11:07:16] INFO:    Connection channel established succesfully
[11:07:16] INFO:    Start to run command
[11:07:22] INFO:    ROW
[11:07:22] INFO:      COLUMN+CELL

[11:07:22] INFO:     
[11:07:22] INFO:    18658727110149148130790 column=cf1:time, timestamp=1494988387789, value=123456844
 18658727110149148130790 column=cf1:type, timestamp=1494988387789, value=24615
[11:07:22] INFO:    

[11:07:22] INFO:     
[11:07:22] INFO:    18658727110149148130791 column=cf1:time, timestamp=1494988387789, value=123456290
 
[11:07:22] INFO:    18658727110149148130791 column=cf1:type, timestamp=1494988387789, value=2466
[11:07:22] INFO:    
 186587271101491481307910
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456298
 
[11:07:22] INFO:    186587271101491481307910 column=cf1:type, timestamp=1494988387789, value=24152

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079100 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456327

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079100 column=cf1:type, timestamp=1494988387789, value=24157
[11:07:22] INFO:    

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079103 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456528
 
[11:07:22] INFO:    1865872711014914813079103 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24483

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079106 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079106 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24633
 1865872711014914813079108 column=cf1:time, timestamp=1494988387789, value=123456801
[11:07:22] INFO:    
 1865872711014914813079108 column=cf1:type, timestamp=1494988387789, value=24833
[11:07:22] INFO:    
 1865872711014914813079109 column=cf1:time, timestamp=1494988387789, value=123456150

[11:07:22] INFO:     1865872711014914813079109 column=cf1:type, timestamp=1494988387789, value=2487
 
[11:07:22] INFO:    186587271101491481307911 column=cf1:time, timestamp=1494988387789, value=123456954
 186587271101491481307911 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=2446
 
[11:07:22] INFO:    1865872711014914813079111 column=cf1:time, timestamp=1494988387789, value=12345668

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079111 column=cf1:type, timestamp=1494988387789, value=24804

[11:07:22] INFO:     1865872711014914813079113 column=cf1:time, timestamp=1494988387789, value=1234565
[11:07:22] INFO:    
 1865872711014914813079113 column=cf1:type, timestamp=1494988387789, value=246
[11:07:22] INFO:    

[11:07:22] INFO:     1865872711014914813079114
[11:07:22] INFO:     
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456578

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079114 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=2494
[11:07:22] INFO:    

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079115 column=cf1:time, timestamp=1494988387789, value=12345626
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079115 column=cf1:type, timestamp=1494988387789, value=24589

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079117 column=cf1:time, timestamp=1494988387789, value=123456113
[11:07:22] INFO:    
 1865872711014914813079117
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24597

[11:07:22] INFO:     1865872711014914813079118
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933
 
[11:07:22] INFO:    1865872711014914813079118 column=cf1:type, timestamp=1494988387789, value=24118
[11:07:22] INFO:    

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079119 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456572
 
[11:07:22] INFO:    1865872711014914813079119 column=cf1:type, timestamp=1494988387789, value=24464
[11:07:22] INFO:    
 186587271101491481307912
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456782

[11:07:22] INFO:     186587271101491481307912 column=cf1:type, timestamp=1494988387789, value=24375
[11:07:22] INFO:    
 1865872711014914813079120
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456256

[11:07:22] INFO:     1865872711014914813079120 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24246
 1865872711014914813079121
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079121 column=cf1:type, timestamp=1494988387789, value=24492

[11:07:22] INFO:     1865872711014914813079122 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456900
 
[11:07:22] INFO:    1865872711014914813079122 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24190
 1865872711014914813079123
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456138

[11:07:22] INFO:     1865872711014914813079123 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24346
 
[11:07:22] INFO:    1865872711014914813079124 column=cf1:time, timestamp=1494988387789, value=123456996
[11:07:22] INFO:    
 1865872711014914813079124
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24319
 
[11:07:22] INFO:    1865872711014914813079127 column=cf1:time, timestamp=1494988387789, value=123456708
[11:07:22] INFO:    
 1865872711014914813079127
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24867
 1865872711014914813079128 column=cf1:time, timestamp=1494988387789, value=123456326
 1865872711014914813079128 column=cf1:type, timestamp=1494988387789, value=24541

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079129 column=cf1:time, timestamp=1494988387789, value=123456771

[11:07:22] INFO:     
[11:07:22] INFO:    1865872711014914813079129 column=cf1:type, timestamp=1494988387789, value=24621
[11:07:22] INFO:    
 
[11:07:22] INFO:    186587271101491481307913 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456919

[11:07:22] INFO:     186587271101491481307913 column=cf1:type, timestamp=1494988387789, value=24290
[11:07:22] INFO:    
 1865872711014914813079130 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079130
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=2490

[11:07:22] INFO:     1865872711014914813079131 column=cf1:time, timestamp=1494988387789, value=123456313
[11:07:22] INFO:    
 1865872711014914813079131 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=2478
 
[11:07:22] INFO:    1865872711014914813079132 column=cf1:time, timestamp=1494988387789, value=123456120
[11:07:22] INFO:    
 1865872711014914813079132 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24276
 1865872711014914813079134
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456521

[11:07:22] INFO:     1865872711014914813079134
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24292
 
[11:07:22] INFO:    1865872711014914813079135 column=cf1:time, timestamp=1494988387789, value=123456650
 1865872711014914813079135
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24883

[11:07:22] INFO:     1865872711014914813079136 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456126

[11:07:22] INFO:     1865872711014914813079136 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=2495

[11:07:22] INFO:     1865872711014914813079137
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=12345693
 1865872711014914813079137
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079139
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456689
 1865872711014914813079139
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24988
 1865872711014914813079140 column=cf1:time, timestamp=1494988387789, value=123456847
[11:07:22] INFO:    
 1865872711014914813079140
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24131
 
[11:07:22] INFO:    1865872711014914813079142 column=cf1:time, timestamp=1494988387789, value=123456165
[11:07:22] INFO:    
 1865872711014914813079142 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24866
 1865872711014914813079143 column=cf1:time, timestamp=1494988387789, value=123456584
 1865872711014914813079143
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24761
 1865872711014914813079144
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456546
 1865872711014914813079144 column=cf1:type, timestamp=1494988387789, value=24283
[11:07:22] INFO:    
 1865872711014914813079146 column=cf1:time, timestamp=1494988387789, value=12345635
 
[11:07:22] INFO:    1865872711014914813079146 column=cf1:type, timestamp=1494988387789, value=24888

[11:07:22] INFO:     1865872711014914813079147 column=cf1:time, timestamp=1494988387789, value=123456952
 
[11:07:22] INFO:    1865872711014914813079147 column=cf1:type, timestamp=1494988387789, value=24336
 1865872711014914813079150
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456776
 1865872711014914813079150 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24967

[11:07:22] INFO:     1865872711014914813079151 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456717
 1865872711014914813079151
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24622
 1865872711014914813079152 column=cf1:time, timestamp=1494988387789, value=123456621
 1865872711014914813079152 column=cf1:type, timestamp=1494988387789, value=24586
 1865872711014914813079154 column=cf1:time, timestamp=1494988387789, value=123456661
 1865872711014914813079154 column=cf1:type, timestamp=1494988387789, value=24899
 1865872711014914813079157 column=cf1:time, timestamp=1494988387789, value=123456675
 1865872711014914813079157 column=cf1:type, timestamp=1494988387789, value=24259

[11:07:22] INFO:     1865872711014914813079158 column=cf1:time, timestamp=1494988387789, value=123456195
[11:07:22] INFO:    
 1865872711014914813079158 column=cf1:type, timestamp=1494988387789, value=24713
[11:07:22] INFO:    
 1865872711014914813079159
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456940
 1865872711014914813079159 column=cf1:type, timestamp=1494988387789, value=24882
 1865872711014914813079160 column=cf1:time, timestamp=1494988387789, value=123456502
 1865872711014914813079160 column=cf1:type, timestamp=1494988387789, value=2433

[11:07:22] INFO:     1865872711014914813079161 column=cf1:time, timestamp=1494988387789, value=123456738
 1865872711014914813079161 column=cf1:type, timestamp=1494988387789, value=24115
 1865872711014914813079162 column=cf1:time, timestamp=1494988387789, value=123456191
 1865872711014914813079162 column=cf1:type, timestamp=1494988387789, value=24490
 
[11:07:22] INFO:    1865872711014914813079164 column=cf1:time, timestamp=1494988387789, value=123456437
 1865872711014914813079164
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24567
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079165 column=cf1:time, timestamp=1494988387789, value=123456932
 
[11:07:22] INFO:    1865872711014914813079165 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24499
 
[11:07:22] INFO:    1865872711014914813079168 column=cf1:time, timestamp=1494988387789, value=123456758

[11:07:22] INFO:     1865872711014914813079168 column=cf1:type, timestamp=1494988387789, value=24306
[11:07:22] INFO:    
 1865872711014914813079169 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456647
 
[11:07:22] INFO:    1865872711014914813079169 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24123
 186587271101491481307917
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456696

[11:07:22] INFO:     186587271101491481307917 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24978
 1865872711014914813079170 column=cf1:time, timestamp=1494988387789, value=123456601
 1865872711014914813079170 column=cf1:type, timestamp=1494988387789, value=24802
 1865872711014914813079171 column=cf1:time, timestamp=1494988387789, value=123456742
[11:07:22] INFO:    
 1865872711014914813079171 column=cf1:type, timestamp=1494988387789, value=24269

[11:07:22] INFO:     1865872711014914813079172 column=cf1:time, timestamp=1494988387789, value=123456410

[11:07:22] INFO:     1865872711014914813079172 column=cf1:type, timestamp=1494988387789, value=24543

[11:07:22] INFO:     1865872711014914813079173 column=cf1:time, timestamp=1494988387789, value=123456306
 
[11:07:22] INFO:    1865872711014914813079173 column=cf1:type, timestamp=1494988387789, value=2423
 
[11:07:22] INFO:    1865872711014914813079177 column=cf1:time, timestamp=1494988387789, value=12345649
[11:07:22] INFO:    
 1865872711014914813079177 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24760
 1865872711014914813079178
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456347
 1865872711014914813079178 column=cf1:type, timestamp=1494988387789, value=24566
 
[11:07:22] INFO:    1865872711014914813079182 column=cf1:time, timestamp=1494988387789, value=123456756
 
[11:07:22] INFO:    1865872711014914813079182 column=cf1:type, timestamp=1494988387789, value=24858
 
[11:07:22] INFO:    1865872711014914813079184 column=cf1:time, timestamp=1494988387789, value=123456564

[11:07:22] INFO:     1865872711014914813079184 column=cf1:type, timestamp=1494988387789, value=24965
 
[11:07:22] INFO:    1865872711014914813079186 column=cf1:time, timestamp=1494988387789, value=123456178
 
[11:07:22] INFO:    1865872711014914813079186 column=cf1:type, timestamp=1494988387789, value=24763
[11:07:22] INFO:    
 1865872711014914813079187 column=cf1:time, timestamp=1494988387789, value=123456653
 1865872711014914813079187 column=cf1:type, timestamp=1494988387789, value=246

[11:07:22] INFO:     1865872711014914813079188 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456299
 1865872711014914813079188 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24900
 1865872711014914813079189 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456636
 1865872711014914813079189
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24350
 
[11:07:22] INFO:    186587271101491481307919 column=cf1:time, timestamp=1494988387789, value=123456933

[11:07:22] INFO:     186587271101491481307919 column=cf1:type, timestamp=1494988387789, value=24124
[11:07:22] INFO:    
 1865872711014914813079191 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345637
 1865872711014914813079191
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24745
 
[11:07:22] INFO:    1865872711014914813079194 column=cf1:time, timestamp=1494988387789, value=123456834

[11:07:22] INFO:     1865872711014914813079194 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24951
 1865872711014914813079195
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456958
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079195 column=cf1:type, timestamp=1494988387789, value=24366

[11:07:22] INFO:     18658727110149148130792
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456590

[11:07:22] INFO:     18658727110149148130792 column=cf1:type, timestamp=1494988387789, value=24463
 1865872711014914813079200 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345639
 1865872711014914813079200 column=cf1:type, timestamp=1494988387789, value=24239
 1865872711014914813079201 column=cf1:time, timestamp=1494988387789, value=123456232
 1865872711014914813079201 column=cf1:type, timestamp=1494988387789, value=24423
 1865872711014914813079202 column=cf1:time, timestamp=1494988387789, value=123456278
 1865872711014914813079202 column=cf1:type, timestamp=1494988387789, value=24198
 1865872711014914813079203 column=cf1:time, timestamp=1494988387789, value=123456358
[11:07:22] INFO:    
 1865872711014914813079203 column=cf1:type, timestamp=1494988387789, value=24486

[11:07:22] INFO:     1865872711014914813079205 column=cf1:time, timestamp=1494988387789, value=123456247
[11:07:22] INFO:    
 1865872711014914813079205
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24374

[11:07:22] INFO:     1865872711014914813079206
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456802

[11:07:22] INFO:     1865872711014914813079206 column=cf1:type, timestamp=1494988387789, value=24491
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079207 column=cf1:time, timestamp=1494988387789, value=123456571
[11:07:22] INFO:    
 1865872711014914813079207 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24238

[11:07:22] INFO:     1865872711014914813079208
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456727
 1865872711014914813079208
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24937

[11:07:22] INFO:     1865872711014914813079210 column=cf1:time, timestamp=1494988387789, value=123456606
[11:07:22] INFO:    
 1865872711014914813079210 column=cf1:type, timestamp=1494988387789, value=24652
[11:07:22] INFO:    
 1865872711014914813079211 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456973
 
[11:07:22] INFO:    1865872711014914813079211 column=cf1:type, timestamp=1494988387789, value=24375
[11:07:22] INFO:    
 1865872711014914813079214 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456384

[11:07:22] INFO:     1865872711014914813079214 column=cf1:type, timestamp=1494988387789, value=24483

[11:07:22] INFO:     1865872711014914813079215 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345694
 
[11:07:22] INFO:    1865872711014914813079215 column=cf1:type, timestamp=1494988387789, value=2420

[11:07:22] INFO:     1865872711014914813079216 column=cf1:time, timestamp=1494988387789, value=123456750
[11:07:22] INFO:    
 1865872711014914813079216 column=cf1:type, timestamp=1494988387789, value=24124

[11:07:22] INFO:     1865872711014914813079217 column=cf1:time, timestamp=1494988387789, value=123456934

[11:07:22] INFO:     1865872711014914813079217 column=cf1:type, timestamp=1494988387789, value=24361
[11:07:22] INFO:    
 1865872711014914813079218 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456904
 1865872711014914813079218 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24733
 1865872711014914813079219
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456288
 1865872711014914813079219
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24901

[11:07:22] INFO:     186587271101491481307922 column=cf1:time, timestamp=1494988387789, value=123456523
[11:07:22] INFO:    
 186587271101491481307922
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=2451
 1865872711014914813079220
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456802
 
[11:07:22] INFO:    1865872711014914813079220 column=cf1:type, timestamp=1494988387789, value=24935
[11:07:22] INFO:    
 1865872711014914813079221 column=cf1:time, timestamp=1494988387789, value=123456940
[11:07:22] INFO:    
 1865872711014914813079221 column=cf1:type, timestamp=1494988387789, value=24818
[11:07:22] INFO:    
 1865872711014914813079223 column=cf1:time, timestamp=1494988387789, value=123456621
[11:07:22] INFO:    
 1865872711014914813079223 column=cf1:type, timestamp=1494988387789, value=24138
 1865872711014914813079224 column=cf1:time, timestamp=1494988387789, value=123456513
 1865872711014914813079224 column=cf1:type, timestamp=1494988387789, value=24474
 1865872711014914813079225 column=cf1:time, timestamp=1494988387789, value=123456719
 1865872711014914813079225 column=cf1:type, timestamp=1494988387789, value=24639
 1865872711014914813079226 column=cf1:time, timestamp=1494988387789, value=123456706
 1865872711014914813079226 column=cf1:type, timestamp=1494988387789, value=24595
 1865872711014914813079228 column=cf1:time, timestamp=1494988387789, value=123456371
[11:07:22] INFO:    
 1865872711014914813079228 column=cf1:type, timestamp=1494988387789, value=24143
[11:07:22] INFO:    
 1865872711014914813079229 column=cf1:time, timestamp=1494988387789, value=123456723
[11:07:22] INFO:    
 1865872711014914813079229
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24855
 
[11:07:22] INFO:    186587271101491481307923 column=cf1:time, timestamp=1494988387789, value=123456894

[11:07:22] INFO:     186587271101491481307923 column=cf1:type, timestamp=1494988387789, value=24576
[11:07:22] INFO:    
 1865872711014914813079230
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=12345675
 1865872711014914813079230
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24548
 
[11:07:22] INFO:    1865872711014914813079231 column=cf1:time, timestamp=1494988387789, value=123456698

[11:07:22] INFO:     1865872711014914813079231 column=cf1:type, timestamp=1494988387789, value=2420

[11:07:22] INFO:     1865872711014914813079232 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456548
 1865872711014914813079232 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24804
 1865872711014914813079233
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933

[11:07:22] INFO:     1865872711014914813079233 column=cf1:type, timestamp=1494988387789, value=24687
[11:07:22] INFO:    
 1865872711014914813079235 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456600
 
[11:07:22] INFO:    1865872711014914813079235 column=cf1:type, timestamp=1494988387789, value=2463

[11:07:22] INFO:     1865872711014914813079236 column=cf1:time, timestamp=1494988387789, value=123456470

[11:07:22] INFO:     1865872711014914813079236 column=cf1:type, timestamp=1494988387789, value=24338
[11:07:22] INFO:    
 1865872711014914813079237 column=cf1:time, timestamp=1494988387789, value=123456772
[11:07:22] INFO:    
 1865872711014914813079237 column=cf1:type, timestamp=1494988387789, value=24986

[11:07:22] INFO:     1865872711014914813079238 column=cf1:time, timestamp=1494988387789, value=123456275
 
[11:07:22] INFO:    1865872711014914813079238 column=cf1:type, timestamp=1494988387789, value=24546
 1865872711014914813079240
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456959
 1865872711014914813079240
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24973
 1865872711014914813079241
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=12345686
 1865872711014914813079241
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24108
 1865872711014914813079243
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456551
 
[11:07:22] INFO:    1865872711014914813079243 column=cf1:type, timestamp=1494988387789, value=24579
 
[11:07:22] INFO:    1865872711014914813079245 column=cf1:time, timestamp=1494988387789, value=123456860

[11:07:22] INFO:     1865872711014914813079245 column=cf1:type, timestamp=1494988387789, value=24958
[11:07:22] INFO:    
 1865872711014914813079246 column=cf1:time, timestamp=1494988387789, value=123456439
 1865872711014914813079246 column=cf1:type, timestamp=1494988387789, value=2433
 1865872711014914813079247 column=cf1:time, timestamp=1494988387789, value=1234564
 
[11:07:22] INFO:    1865872711014914813079247 column=cf1:type, timestamp=1494988387789, value=24393
 1865872711014914813079249
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456656
 
[11:07:22] INFO:    1865872711014914813079249 column=cf1:type, timestamp=1494988387789, value=24922
 
[11:07:22] INFO:    1865872711014914813079251 column=cf1:time, timestamp=1494988387789, value=12345643

[11:07:22] INFO:     1865872711014914813079251 column=cf1:type, timestamp=1494988387789, value=24829

[11:07:22] INFO:     1865872711014914813079252 column=cf1:time, timestamp=1494988387789, value=123456304
[11:07:22] INFO:    
 1865872711014914813079252 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24741
 1865872711014914813079253 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456127
 1865872711014914813079253 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24854
 1865872711014914813079254 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079254
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24208
 1865872711014914813079255
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456544
 1865872711014914813079255
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24840
 1865872711014914813079256
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456470
 1865872711014914813079256
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24692
 1865872711014914813079257
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456656
 
[11:07:22] INFO:    1865872711014914813079257 column=cf1:type, timestamp=1494988387789, value=24787

[11:07:22] INFO:     1865872711014914813079258 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456838
 1865872711014914813079258
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24448
 
[11:07:22] INFO:    1865872711014914813079259 column=cf1:time, timestamp=1494988387789, value=123456839

[11:07:22] INFO:     1865872711014914813079259 column=cf1:type, timestamp=1494988387789, value=24321
[11:07:22] INFO:    
 1865872711014914813079261
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456338
 
[11:07:22] INFO:    1865872711014914813079261 column=cf1:type, timestamp=1494988387789, value=24475

[11:07:22] INFO:     1865872711014914813079263 column=cf1:time, timestamp=1494988387789, value=123456997
 
[11:07:22] INFO:    1865872711014914813079263 column=cf1:type, timestamp=1494988387789, value=24368

[11:07:22] INFO:     1865872711014914813079264 column=cf1:time, timestamp=1494988387789, value=12345635
 1865872711014914813079264 column=cf1:type, timestamp=1494988387789, value=24894
 1865872711014914813079265 column=cf1:time, timestamp=1494988387789, value=123456732
 1865872711014914813079265 column=cf1:type, timestamp=1494988387789, value=24586
 1865872711014914813079268 column=cf1:time, timestamp=1494988387789, value=123456343
 1865872711014914813079268 column=cf1:type, timestamp=1494988387789, value=24540
 1865872711014914813079269 column=cf1:time, timestamp=1494988387789, value=12345614
 1865872711014914813079269 column=cf1:type, timestamp=1494988387789, value=2421
 186587271101491481307927 column=cf1:time, timestamp=1494988387789, value=12345699
 
[11:07:22] INFO:    186587271101491481307927 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079272
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456843

[11:07:22] INFO:     1865872711014914813079272
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24737
 
[11:07:22] INFO:    1865872711014914813079273 column=cf1:time, timestamp=1494988387789, value=123456558
[11:07:22] INFO:    
 1865872711014914813079273
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24156
 1865872711014914813079275
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456795
 
[11:07:22] INFO:    1865872711014914813079275 column=cf1:type, timestamp=1494988387789, value=24297
[11:07:22] INFO:    
 1865872711014914813079276 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456404
 1865872711014914813079276
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24893
 1865872711014914813079277 column=cf1:time, timestamp=1494988387789, value=123456598
 
[11:07:22] INFO:    1865872711014914813079277 column=cf1:type, timestamp=1494988387789, value=24673

[11:07:22] INFO:     1865872711014914813079279 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456879
 
[11:07:22] INFO:    1865872711014914813079279 column=cf1:type, timestamp=1494988387789, value=24769

[11:07:22] INFO:     1865872711014914813079281 column=cf1:time, timestamp=1494988387789, value=123456525

[11:07:22] INFO:     1865872711014914813079281 column=cf1:type, timestamp=1494988387789, value=24615
[11:07:22] INFO:    
 1865872711014914813079284 column=cf1:time, timestamp=1494988387789, value=12345631
[11:07:22] INFO:    
 1865872711014914813079284 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24509
 
[11:07:22] INFO:    1865872711014914813079285 column=cf1:time, timestamp=1494988387789, value=123456454
[11:07:22] INFO:    
 1865872711014914813079285
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24456

[11:07:22] INFO:     1865872711014914813079287 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456150
 1865872711014914813079287 column=cf1:type, timestamp=1494988387789, value=24611
 1865872711014914813079288 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079288 column=cf1:type, timestamp=1494988387789, value=24872
[11:07:22] INFO:    
 1865872711014914813079289 column=cf1:time, timestamp=1494988387789, value=123456613
[11:07:22] INFO:    
 1865872711014914813079289
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24113

[11:07:22] INFO:     186587271101491481307929 column=cf1:time, timestamp=1494988387789, value=123456106
[11:07:22] INFO:    
 186587271101491481307929 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24319
 
[11:07:22] INFO:    1865872711014914813079290 column=cf1:time, timestamp=1494988387789, value=123456401

[11:07:22] INFO:     1865872711014914813079290 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24445
 1865872711014914813079292 column=cf1:time, timestamp=1494988387789, value=123456505
[11:07:22] INFO:    
 1865872711014914813079292 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24251
 
[11:07:22] INFO:    1865872711014914813079293 column=cf1:time, timestamp=1494988387789, value=12345622
[11:07:22] INFO:    
 1865872711014914813079293 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24798

[11:07:22] INFO:     1865872711014914813079296 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456524
 
[11:07:22] INFO:    1865872711014914813079296 column=cf1:type, timestamp=1494988387789, value=24188
[11:07:22] INFO:    
 1865872711014914813079297 column=cf1:time, timestamp=1494988387789, value=123456905
 1865872711014914813079297
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24927

[11:07:22] INFO:     1865872711014914813079298 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456756
 
[11:07:22] INFO:    1865872711014914813079298 column=cf1:type, timestamp=1494988387789, value=24515
[11:07:22] INFO:    
 18658727110149148130793 column=cf1:time, timestamp=1494988387789, value=123456274
[11:07:22] INFO:    
 18658727110149148130793 column=cf1:type, timestamp=1494988387789, value=24163
 1865872711014914813079300 column=cf1:time, timestamp=1494988387789, value=123456824
 
[11:07:22] INFO:    1865872711014914813079300 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24403
 
[11:07:22] INFO:    1865872711014914813079303 column=cf1:time, timestamp=1494988387789, value=123456113
[11:07:22] INFO:    
 1865872711014914813079303
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24635

[11:07:22] INFO:     1865872711014914813079304 column=cf1:time, timestamp=1494988387789, value=123456220
[11:07:22] INFO:    
 1865872711014914813079304 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24403
 1865872711014914813079309
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456796
 1865872711014914813079309 column=cf1:type, timestamp=1494988387789, value=24362
 186587271101491481307931 column=cf1:time, timestamp=1494988387789, value=123456496
 186587271101491481307931 column=cf1:type, timestamp=1494988387789, value=24751
 1865872711014914813079310 column=cf1:time, timestamp=1494988387789, value=123456289
 
[11:07:22] INFO:    1865872711014914813079310 column=cf1:type, timestamp=1494988387789, value=24766
 
[11:07:22] INFO:    1865872711014914813079311 column=cf1:time, timestamp=1494988387789, value=123456981

[11:07:22] INFO:     1865872711014914813079311 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24179
 
[11:07:22] INFO:    1865872711014914813079316 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456133
 
[11:07:22] INFO:    1865872711014914813079316 column=cf1:type, timestamp=1494988387789, value=2494
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079317 column=cf1:time, timestamp=1494988387789, value=123456663
[11:07:22] INFO:    
 1865872711014914813079317
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24166
[11:07:22] INFO:    
 1865872711014914813079318
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456845
[11:07:22] INFO:    
 1865872711014914813079318 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24744

[11:07:22] INFO:     1865872711014914813079320 column=cf1:time, timestamp=1494988387789, value=1234568
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079320 column=cf1:type, timestamp=1494988387789, value=24862
[11:07:22] INFO:    
 1865872711014914813079321
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456934

[11:07:22] INFO:     1865872711014914813079321
[11:07:22] INFO:     
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24792

[11:07:22] INFO:     1865872711014914813079322 column=cf1:time, timestamp=1494988387789, value=123456596
[11:07:22] INFO:    
 1865872711014914813079322 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24683

[11:07:22] INFO:     1865872711014914813079323 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456401
 
[11:07:22] INFO:    1865872711014914813079323 column=cf1:type, timestamp=1494988387789, value=24863
 
[11:07:22] INFO:    1865872711014914813079325 column=cf1:time, timestamp=1494988387789, value=12345623

[11:07:22] INFO:     1865872711014914813079325 column=cf1:type, timestamp=1494988387789, value=24934
[11:07:22] INFO:    
 1865872711014914813079326
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456263

[11:07:22] INFO:     1865872711014914813079326 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24882
 
[11:07:22] INFO:    1865872711014914813079327 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345676
 1865872711014914813079327 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24700
 
[11:07:22] INFO:    1865872711014914813079329 column=cf1:time, timestamp=1494988387789, value=123456246
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079329 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24784
 
[11:07:22] INFO:    186587271101491481307933 column=cf1:time, timestamp=1494988387789, value=123456983
[11:07:22] INFO:    
 186587271101491481307933
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24748
[11:07:22] INFO:    
 1865872711014914813079330 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456589
 
[11:07:22] INFO:    1865872711014914813079330 column=cf1:type, timestamp=1494988387789, value=24194
[11:07:22] INFO:    
 1865872711014914813079331 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456574
 
[11:07:22] INFO:    1865872711014914813079331 column=cf1:type, timestamp=1494988387789, value=24664
 1865872711014914813079332
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456326
 
[11:07:22] INFO:    1865872711014914813079332 column=cf1:type, timestamp=1494988387789, value=24992

[11:07:22] INFO:     1865872711014914813079334 column=cf1:time, timestamp=1494988387789, value=12345654
[11:07:22] INFO:    
 1865872711014914813079334 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24687
 1865872711014914813079335
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456551
 1865872711014914813079335 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24651
 1865872711014914813079336
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456739
 1865872711014914813079336 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24312
 1865872711014914813079337 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456798
 1865872711014914813079337
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24949
 1865872711014914813079338
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456292
 
[11:07:22] INFO:    1865872711014914813079338 column=cf1:type, timestamp=1494988387789, value=24838

[11:07:22] INFO:     1865872711014914813079339 column=cf1:time, timestamp=1494988387789, value=123456465

[11:07:22] INFO:     1865872711014914813079339 column=cf1:type, timestamp=1494988387789, value=24761
[11:07:22] INFO:    
 1865872711014914813079341 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345661
 1865872711014914813079341
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24118
 
[11:07:22] INFO:    1865872711014914813079342 column=cf1:time, timestamp=1494988387789, value=12345689
 
[11:07:22] INFO:    1865872711014914813079342 column=cf1:type, timestamp=1494988387789, value=24123
 
[11:07:22] INFO:    1865872711014914813079343 column=cf1:time, timestamp=1494988387789, value=123456261
 
[11:07:22] INFO:    1865872711014914813079343 column=cf1:type, timestamp=1494988387789, value=24744
 1865872711014914813079344
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456148
 
[11:07:22] INFO:    1865872711014914813079344 column=cf1:type, timestamp=1494988387789, value=24259

[11:07:22] INFO:     1865872711014914813079345 column=cf1:time, timestamp=1494988387789, value=123456874
 1865872711014914813079345
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24324

[11:07:22] INFO:     1865872711014914813079346 column=cf1:time, timestamp=1494988387789, value=123456244
[11:07:22] INFO:    
 1865872711014914813079346 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24762
 1865872711014914813079347
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456244
 
[11:07:22] INFO:    1865872711014914813079347 column=cf1:type, timestamp=1494988387789, value=24780
[11:07:22] INFO:    
 1865872711014914813079348 column=cf1:time, timestamp=1494988387789, value=123456694
 1865872711014914813079348
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24850
 
[11:07:22] INFO:    186587271101491481307935 column=cf1:time, timestamp=1494988387789, value=123456413

[11:07:22] INFO:     186587271101491481307935 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24846
 
[11:07:22] INFO:    1865872711014914813079352 column=cf1:time, timestamp=1494988387789, value=12345687
[11:07:22] INFO:    
 1865872711014914813079352 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24979
 
[11:07:22] INFO:    1865872711014914813079353 column=cf1:time, timestamp=1494988387789, value=123456426
[11:07:22] INFO:    
 1865872711014914813079353 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=2474
 
[11:07:22] INFO:    1865872711014914813079354 column=cf1:time, timestamp=1494988387789, value=123456630
[11:07:22] INFO:    
 1865872711014914813079354 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24442
 
[11:07:22] INFO:    1865872711014914813079356 column=cf1:time, timestamp=1494988387789, value=123456317
[11:07:22] INFO:    
 1865872711014914813079356 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24619
 1865872711014914813079358
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=123456742
 
[11:07:22] INFO:    1865872711014914813079358 column=cf1:type, timestamp=1494988387789, value=24648
 1865872711014914813079359
[11:07:22] INFO:     column=cf1:time, timestamp=1494988387789, value=12345622
 
[11:07:22] INFO:    1865872711014914813079359 column=cf1:type, timestamp=1494988387789, value=24755
[11:07:22] INFO:    
 186587271101491481307936 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456497
 
[11:07:22] INFO:    186587271101491481307936 column=cf1:type, timestamp=1494988387789, value=24122
[11:07:22] INFO:    
 1865872711014914813079361 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456726
 
[11:07:22] INFO:    1865872711014914813079361 column=cf1:type, timestamp=1494988387789, value=24463
[11:07:22] INFO:    
 1865872711014914813079363 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456111
 
[11:07:22] INFO:    1865872711014914813079363 column=cf1:type, timestamp=1494988387789, value=24233

[11:07:22] INFO:     1865872711014914813079364 column=cf1:time, timestamp=1494988387789, value=123456662
[11:07:22] INFO:    
 1865872711014914813079364
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24597

[11:07:22] INFO:     1865872711014914813079367 column=cf1:time, timestamp=1494988387789, value=123456161

[11:07:22] INFO:     1865872711014914813079367 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24122

[11:07:22] INFO:     1865872711014914813079368 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456447

[11:07:22] INFO:     1865872711014914813079368 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24328
 1865872711014914813079370 column=cf1:time, timestamp=1494988387789, value=123456859
 1865872711014914813079370 column=cf1:type, timestamp=1494988387789, value=24262
 1865872711014914813079372 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=12345618
 
[11:07:22] INFO:    1865872711014914813079372 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24508
 1865872711014914813079374 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456400
 1865872711014914813079374
[11:07:22] INFO:     column=cf1:type, timestamp=1494988387789, value=24525
 
[11:07:22] INFO:    1865872711014914813079375 column=cf1:time, timestamp=1494988387789, value=12345655
 
[11:07:22] INFO:    1865872711014914813079375 column=cf1:type, timestamp=1494988387789, value=24398
[11:07:22] INFO:    
 
[11:07:22] INFO:    1865872711014914813079379 
[11:07:22] INFO:    column=cf1:time, timestamp=1494988387789, value=123456508

[11:07:22] INFO:     1865872711014914813079379 
[11:07:22] INFO:    column=cf1:type, timestamp=1494988387789, value=24709
 
[11:07:22] INFO:    186587271101491481307938 column=cf1:time, timestamp=1494988387789, value=123456378
 
[11:07:22] INFO:    186587271101491481307938 column=cf1:type, timestamp=1494988387789, value=24318
 
[11:07:23] INFO:    1865872711014914813079382 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=12345696

[11:07:23] INFO:     1865872711014914813079382 column=cf1:type, timestamp=1494988387789, value=24287

[11:07:23] INFO:     1865872711014914813079384 column=cf1:time, timestamp=1494988387789, value=123456878
[11:07:23] INFO:    
 1865872711014914813079384 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24897
 1865872711014914813079386 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456602
 
[11:07:23] INFO:    1865872711014914813079386 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24719

[11:07:23] INFO:     1865872711014914813079389
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456570
[11:07:23] INFO:    
 1865872711014914813079389 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24633
 
[11:07:23] INFO:    1865872711014914813079390 column=cf1:time, timestamp=1494988387789, value=123456885
 
[11:07:23] INFO:    1865872711014914813079390 column=cf1:type, timestamp=1494988387789, value=24622
 
[11:07:23] INFO:    1865872711014914813079391 column=cf1:time, timestamp=1494988387789, value=123456378
 
[11:07:23] INFO:    1865872711014914813079391 column=cf1:type, timestamp=1494988387789, value=24513
 
[11:07:23] INFO:    1865872711014914813079393 column=cf1:time, timestamp=1494988387789, value=12345697
[11:07:23] INFO:    
 1865872711014914813079393 column=cf1:type, timestamp=1494988387789, value=24692
[11:07:23] INFO:    
 1865872711014914813079394 column=cf1:time, timestamp=1494988387789, value=123456560

[11:07:23] INFO:     1865872711014914813079394 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24982
 1865872711014914813079395
[11:07:23] INFO:     
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456952

[11:07:23] INFO:     1865872711014914813079395 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24708
 186587271101491481307940
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456826
 
[11:07:23] INFO:    186587271101491481307940 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=244
 
[11:07:23] INFO:    1865872711014914813079401 column=cf1:time, timestamp=1494988387789, value=12345611
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079401 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24566

[11:07:23] INFO:     1865872711014914813079402 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456366
 1865872711014914813079402 column=cf1:type, timestamp=1494988387789, value=24472
[11:07:23] INFO:    
 1865872711014914813079403 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456711
 1865872711014914813079403
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=2438

[11:07:23] INFO:     1865872711014914813079404 column=cf1:time, timestamp=1494988387789, value=123456946

[11:07:23] INFO:     1865872711014914813079404 column=cf1:type, timestamp=1494988387789, value=24695
[11:07:23] INFO:    
 1865872711014914813079405
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456379
 
[11:07:23] INFO:    1865872711014914813079405 column=cf1:type, timestamp=1494988387789, value=24149
 
[11:07:23] INFO:    1865872711014914813079406 column=cf1:time, timestamp=1494988387789, value=123456403
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079406 column=cf1:type, timestamp=1494988387789, value=24953

[11:07:23] INFO:     1865872711014914813079408 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=12345617

[11:07:23] INFO:     1865872711014914813079408 column=cf1:type, timestamp=1494988387789, value=24352
[11:07:23] INFO:    

[11:07:23] INFO:     1865872711014914813079409 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456102

[11:07:23] INFO:     1865872711014914813079409 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24989

[11:07:23] INFO:     186587271101491481307941
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456173
[11:07:23] INFO:    
 
[11:07:23] INFO:    186587271101491481307941 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24405
 
[11:07:23] INFO:    1865872711014914813079412 column=cf1:time, timestamp=1494988387789, value=12345646
 1865872711014914813079412 column=cf1:type, timestamp=1494988387789, value=2491
 1865872711014914813079413 column=cf1:time, timestamp=1494988387789, value=12345647
 1865872711014914813079413 column=cf1:type, timestamp=1494988387789, value=24176
 1865872711014914813079415 column=cf1:time, timestamp=1494988387789, value=123456118
 1865872711014914813079415 column=cf1:type, timestamp=1494988387789, value=2434
 1865872711014914813079416 column=cf1:time, timestamp=1494988387789, value=123456725
 1865872711014914813079416 column=cf1:type, timestamp=1494988387789, value=24734
 
[11:07:23] INFO:    1865872711014914813079418 column=cf1:time, timestamp=1494988387789, value=123456552
[11:07:23] INFO:    
 1865872711014914813079418 column=cf1:type, timestamp=1494988387789, value=24660
 
[11:07:23] INFO:    1865872711014914813079420 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456193

[11:07:23] INFO:     
[11:07:23] INFO:    1865872711014914813079420 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=2427
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079421 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456268

[11:07:23] INFO:     1865872711014914813079421
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24326
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079422 column=cf1:time, timestamp=1494988387789, value=123456228

[11:07:23] INFO:     1865872711014914813079422
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24767
 
[11:07:23] INFO:    1865872711014914813079424 column=cf1:time, timestamp=1494988387789, value=123456463

[11:07:23] INFO:     1865872711014914813079424 column=cf1:type, timestamp=1494988387789, value=24463
[11:07:23] INFO:    
 1865872711014914813079425 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456168
 1865872711014914813079425
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24653
 
[11:07:23] INFO:    1865872711014914813079426 column=cf1:time, timestamp=1494988387789, value=123456471

[11:07:23] INFO:     1865872711014914813079426 column=cf1:type, timestamp=1494988387789, value=24602
[11:07:23] INFO:    
 1865872711014914813079427 column=cf1:time, timestamp=1494988387789, value=123456267
[11:07:23] INFO:    
 1865872711014914813079427
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24676

[11:07:23] INFO:     1865872711014914813079429 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456631

[11:07:23] INFO:     1865872711014914813079429
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24921
[11:07:23] INFO:    
 
[11:07:23] INFO:    186587271101491481307943 column=cf1:time, timestamp=1494988387789, value=123456738
[11:07:23] INFO:    
 
[11:07:23] INFO:    186587271101491481307943
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24677

[11:07:23] INFO:     1865872711014914813079430
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456239

[11:07:23] INFO:     1865872711014914813079430 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24204
 
[11:07:23] INFO:    1865872711014914813079433 column=cf1:time, timestamp=1494988387789, value=123456291
[11:07:23] INFO:    
 1865872711014914813079433 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24110

[11:07:23] INFO:     1865872711014914813079434
[11:07:23] INFO:     
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456707
 
[11:07:23] INFO:    1865872711014914813079434 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24266
 
[11:07:23] INFO:    1865872711014914813079435 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=12345692

[11:07:23] INFO:     
[11:07:23] INFO:    1865872711014914813079435
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24337

[11:07:23] INFO:     1865872711014914813079436
[11:07:23] INFO:     
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456569
 
[11:07:23] INFO:    1865872711014914813079436 column=cf1:type, timestamp=1494988387789, value=24962

[11:07:23] INFO:     
[11:07:23] INFO:    1865872711014914813079438 column=cf1:time, timestamp=1494988387789, value=1234568
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079438 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24309
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079439 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456517
 1865872711014914813079439
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24403
[11:07:23] INFO:    
 186587271101491481307944
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456509

[11:07:23] INFO:     186587271101491481307944
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=2457

[11:07:23] INFO:     1865872711014914813079441
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456574

[11:07:23] INFO:     1865872711014914813079441
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24435

[11:07:23] INFO:     1865872711014914813079442 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456899
 
[11:07:23] INFO:    1865872711014914813079442 column=cf1:type, timestamp=1494988387789, value=24848
[11:07:23] INFO:    
 
[11:07:23] INFO:    1865872711014914813079443 column=cf1:time, timestamp=1494988387789, value=123456164
[11:07:23] INFO:    
 1865872711014914813079443
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24622

[11:07:23] INFO:     1865872711014914813079444
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456731

[11:07:23] INFO:     1865872711014914813079444 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24504
 
[11:07:23] INFO:    1865872711014914813079446 column=cf1:time, timestamp=1494988387789, value=123456499

[11:07:23] INFO:     1865872711014914813079446 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079447 column=cf1:time, timestamp=1494988387789, value=123456998
 
[11:07:23] INFO:    1865872711014914813079447 column=cf1:type, timestamp=1494988387789, value=24162
 1865872711014914813079448 column=cf1:time, timestamp=1494988387789, value=12345655
 1865872711014914813079448 column=cf1:type, timestamp=1494988387789, value=24870
 1865872711014914813079449 column=cf1:time, timestamp=1494988387789, value=123456943
 1865872711014914813079449 column=cf1:type, timestamp=1494988387789, value=2483
 186587271101491481307945 column=cf1:time, timestamp=1494988387789, value=123456385
 186587271101491481307945 column=cf1:type, timestamp=1494988387789, value=24944
 1865872711014914813079450 column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079450 column=cf1:type, timestamp=1494988387789, value=24400
[11:07:23] INFO:    
 1865872711014914813079452 column=cf1:time, timestamp=1494988387789, value=123456416
[11:07:23] INFO:    
 1865872711014914813079452 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24347
 1865872711014914813079453 column=cf1:time, timestamp=1494988387789, value=123456158
 1865872711014914813079453 column=cf1:type, timestamp=1494988387789, value=24482

[11:07:23] INFO:     1865872711014914813079454 column=cf1:time, timestamp=1494988387789, value=123456173
 1865872711014914813079454 column=cf1:type, timestamp=1494988387789, value=24366
 1865872711014914813079457 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456262
 1865872711014914813079457 column=cf1:type, timestamp=1494988387789, value=24598
 1865872711014914813079458 column=cf1:time, timestamp=1494988387789, value=123456540
 
[11:07:23] INFO:    1865872711014914813079458 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079459 column=cf1:time, timestamp=1494988387789, value=123456724
 1865872711014914813079459 column=cf1:type, timestamp=1494988387789, value=24728

[11:07:23] INFO:     186587271101491481307946 column=cf1:time, timestamp=1494988387789, value=123456925
 186587271101491481307946 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079460 column=cf1:time, timestamp=1494988387789, value=123456117
 1865872711014914813079460 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24750
 1865872711014914813079464 column=cf1:time, timestamp=1494988387789, value=123456504
 1865872711014914813079464 column=cf1:type, timestamp=1494988387789, value=24678
 1865872711014914813079466 column=cf1:time, timestamp=1494988387789, value=123456401
[11:07:23] INFO:    
 1865872711014914813079466 column=cf1:type, timestamp=1494988387789, value=24960
 1865872711014914813079468 column=cf1:time, timestamp=1494988387789, value=123456306
 1865872711014914813079468 column=cf1:type, timestamp=1494988387789, value=24536
 
[11:07:23] INFO:    1865872711014914813079469 column=cf1:time, timestamp=1494988387789, value=123456406
 1865872711014914813079469 column=cf1:type, timestamp=1494988387789, value=24980
 1865872711014914813079471 column=cf1:time, timestamp=1494988387789, value=123456236
 1865872711014914813079471 column=cf1:type, timestamp=1494988387789, value=24583
 1865872711014914813079472 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079472 column=cf1:type, timestamp=1494988387789, value=2491
 1865872711014914813079473 column=cf1:time, timestamp=1494988387789, value=123456150
 1865872711014914813079473 column=cf1:type, timestamp=1494988387789, value=24582
 1865872711014914813079474 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456639
 1865872711014914813079474 column=cf1:type, timestamp=1494988387789, value=24702
 1865872711014914813079475 column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079475 column=cf1:type, timestamp=1494988387789, value=24461
 1865872711014914813079479 column=cf1:time, timestamp=1494988387789, value=123456427
 1865872711014914813079479
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24527
 186587271101491481307948 column=cf1:time, timestamp=1494988387789, value=123456837
 186587271101491481307948 column=cf1:type, timestamp=1494988387789, value=24668
 1865872711014914813079481 column=cf1:time, timestamp=1494988387789, value=123456204
 1865872711014914813079481 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24891
 1865872711014914813079482 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079482 column=cf1:type, timestamp=1494988387789, value=24155
 1865872711014914813079483 column=cf1:time, timestamp=1494988387789, value=123456796
 1865872711014914813079483 column=cf1:type, timestamp=1494988387789, value=24227
 
[11:07:23] INFO:    1865872711014914813079485 column=cf1:time, timestamp=1494988387789, value=123456592
 1865872711014914813079485 column=cf1:type, timestamp=1494988387789, value=24690
 1865872711014914813079487 column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079487 column=cf1:type, timestamp=1494988387789, value=24253
 1865872711014914813079488 column=cf1:time, timestamp=1494988387789, value=123456243
 1865872711014914813079488 column=cf1:type, timestamp=1494988387789, value=24627
 186587271101491481307949 column=cf1:time, timestamp=1494988387789, value=123456179
[11:07:23] INFO:    
 186587271101491481307949 column=cf1:type, timestamp=1494988387789, value=24693
 1865872711014914813079493 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079493 column=cf1:type, timestamp=1494988387789, value=24837
 1865872711014914813079495 column=cf1:time, timestamp=1494988387789, value=1234561
 1865872711014914813079495 column=cf1:type, timestamp=1494988387789, value=24597
 1865872711014914813079496 column=cf1:time, timestamp=1494988387789, value=123456991
 1865872711014914813079496 column=cf1:type, timestamp=1494988387789, value=2498
[11:07:23] INFO:    
 1865872711014914813079497 column=cf1:time, timestamp=1494988387789, value=123456791
 1865872711014914813079497 column=cf1:type, timestamp=1494988387789, value=24540
 1865872711014914813079498 column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079498 column=cf1:type, timestamp=1494988387789, value=24514

[11:07:23] INFO:     1865872711014914813079499 column=cf1:time, timestamp=1494988387789, value=12345647
 1865872711014914813079499 column=cf1:type, timestamp=1494988387789, value=24760
 18658727110149148130795 column=cf1:time, timestamp=1494988387789, value=123456118
 18658727110149148130795 column=cf1:type, timestamp=1494988387789, value=24128
 
[11:07:23] INFO:    186587271101491481307950 column=cf1:time, timestamp=1494988387789, value=123456613
 186587271101491481307950 column=cf1:type, timestamp=1494988387789, value=24205
 1865872711014914813079500 column=cf1:time, timestamp=1494988387789, value=123456586
 1865872711014914813079500 column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079501 column=cf1:time, timestamp=1494988387789, value=123456951
[11:07:23] INFO:    
 1865872711014914813079501 column=cf1:type, timestamp=1494988387789, value=24169
 1865872711014914813079503 column=cf1:time, timestamp=1494988387789, value=123456179
 1865872711014914813079503 column=cf1:type, timestamp=1494988387789, value=24868
 1865872711014914813079507 column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079507 column=cf1:type, timestamp=1494988387789, value=2426
 
[11:07:23] INFO:    1865872711014914813079508 column=cf1:time, timestamp=1494988387789, value=123456660
 1865872711014914813079508 column=cf1:type, timestamp=1494988387789, value=24363
 186587271101491481307951 column=cf1:time, timestamp=1494988387789, value=123456329
 186587271101491481307951 column=cf1:type, timestamp=1494988387789, value=24998
 1865872711014914813079511
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456383
 1865872711014914813079511 column=cf1:type, timestamp=1494988387789, value=24907
 1865872711014914813079512 column=cf1:time, timestamp=1494988387789, value=123456594
 1865872711014914813079512 column=cf1:type, timestamp=1494988387789, value=24522
 1865872711014914813079513 column=cf1:time, timestamp=1494988387789, value=123456368
 1865872711014914813079513 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24242
 1865872711014914813079515 column=cf1:time, timestamp=1494988387789, value=12345636
 1865872711014914813079515 column=cf1:type, timestamp=1494988387789, value=2495
 1865872711014914813079519 column=cf1:time, timestamp=1494988387789, value=123456993
 1865872711014914813079519 column=cf1:type, timestamp=1494988387789, value=24516
 186587271101491481307952 column=cf1:time, timestamp=1494988387789, value=123456174

[11:07:23] INFO:     186587271101491481307952 column=cf1:type, timestamp=1494988387789, value=2496
 1865872711014914813079520 column=cf1:time, timestamp=1494988387789, value=123456149
 1865872711014914813079520 column=cf1:type, timestamp=1494988387789, value=24666
 1865872711014914813079521 column=cf1:time, timestamp=1494988387789, value=123456352
 1865872711014914813079521 column=cf1:type, timestamp=1494988387789, value=24225
 
[11:07:23] INFO:    1865872711014914813079523 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079523 column=cf1:type, timestamp=1494988387789, value=24737
 1865872711014914813079524 column=cf1:time, timestamp=1494988387789, value=123456159
 1865872711014914813079524 column=cf1:type, timestamp=1494988387789, value=24287
 1865872711014914813079525 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079525 column=cf1:type, timestamp=1494988387789, value=24458
 1865872711014914813079526 column=cf1:time, timestamp=1494988387789, value=123456252
 1865872711014914813079526 column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079527 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079527 column=cf1:type, timestamp=1494988387789, value=24788
 1865872711014914813079529 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079529 column=cf1:type, timestamp=1494988387789, value=24656
 
[11:07:23] INFO:    186587271101491481307953 column=cf1:time, timestamp=1494988387789, value=123456621
 186587271101491481307953 column=cf1:type, timestamp=1494988387789, value=2448
 1865872711014914813079530 column=cf1:time, timestamp=1494988387789, value=123456585
 1865872711014914813079530 column=cf1:type, timestamp=1494988387789, value=24158
 1865872711014914813079532 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=12345621
 1865872711014914813079532 column=cf1:type, timestamp=1494988387789, value=24642
 1865872711014914813079534 column=cf1:time, timestamp=1494988387789, value=123456620
 1865872711014914813079534 column=cf1:type, timestamp=1494988387789, value=24809
 
[11:07:23] INFO:    1865872711014914813079536 column=cf1:time, timestamp=1494988387789, value=123456288
 1865872711014914813079536 column=cf1:type, timestamp=1494988387789, value=24534
 1865872711014914813079537 column=cf1:time, timestamp=1494988387789, value=123456262

[11:07:23] INFO:     1865872711014914813079537 column=cf1:type, timestamp=1494988387789, value=24666
 1865872711014914813079538 column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079538 column=cf1:type, timestamp=1494988387789, value=24427
 
[11:07:23] INFO:    1865872711014914813079539 column=cf1:time, timestamp=1494988387789, value=12345676
 1865872711014914813079539 column=cf1:type, timestamp=1494988387789, value=24728
 186587271101491481307954 column=cf1:time, timestamp=1494988387789, value=12345687

[11:07:23] INFO:     186587271101491481307954 column=cf1:type, timestamp=1494988387789, value=24737
 1865872711014914813079541 column=cf1:time, timestamp=1494988387789, value=123456418
 1865872711014914813079541 column=cf1:type, timestamp=1494988387789, value=24587
 1865872711014914813079542
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456214
 1865872711014914813079542 column=cf1:type, timestamp=1494988387789, value=24951
 1865872711014914813079545 column=cf1:time, timestamp=1494988387789, value=123456357
 
[11:07:23] INFO:    1865872711014914813079545 column=cf1:type, timestamp=1494988387789, value=24990
 1865872711014914813079546 column=cf1:time, timestamp=1494988387789, value=123456578
 1865872711014914813079546 column=cf1:type, timestamp=1494988387789, value=24602
[11:07:23] INFO:    
 1865872711014914813079547 column=cf1:time, timestamp=1494988387789, value=123456170
 1865872711014914813079547 column=cf1:type, timestamp=1494988387789, value=24701
 
[11:07:23] INFO:    1865872711014914813079549 column=cf1:time, timestamp=1494988387789, value=123456930
 1865872711014914813079549 column=cf1:type, timestamp=1494988387789, value=24569
 186587271101491481307955 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456371
 186587271101491481307955 column=cf1:type, timestamp=1494988387789, value=24199
 1865872711014914813079550 column=cf1:time, timestamp=1494988387789, value=123456665
 1865872711014914813079550 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24642
 1865872711014914813079551 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079551 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24724
 1865872711014914813079555 column=cf1:time, timestamp=1494988387789, value=12345643
 1865872711014914813079555 column=cf1:type, timestamp=1494988387789, value=24362
 1865872711014914813079556 column=cf1:time, timestamp=1494988387789, value=123456642
 1865872711014914813079556 column=cf1:type, timestamp=1494988387789, value=24552

[11:07:23] INFO:     1865872711014914813079557 column=cf1:time, timestamp=1494988387789, value=123456128
 1865872711014914813079557 column=cf1:type, timestamp=1494988387789, value=24762
 1865872711014914813079558 column=cf1:time, timestamp=1494988387789, value=123456949

[11:07:23] INFO:     1865872711014914813079558 column=cf1:type, timestamp=1494988387789, value=2481
 1865872711014914813079560 column=cf1:time, timestamp=1494988387789, value=1234566
 1865872711014914813079560
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24526
 1865872711014914813079561 column=cf1:time, timestamp=1494988387789, value=123456723
 1865872711014914813079561 column=cf1:type, timestamp=1494988387789, value=24304
 1865872711014914813079565
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456123
 1865872711014914813079565 column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079567 column=cf1:time, timestamp=1494988387789, value=123456763
 1865872711014914813079567 column=cf1:type, timestamp=1494988387789, value=24969
[11:07:23] INFO:    
 1865872711014914813079568 column=cf1:time, timestamp=1494988387789, value=123456980
 1865872711014914813079568 column=cf1:type, timestamp=1494988387789, value=24406
 1865872711014914813079569 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=1234566
 1865872711014914813079569 column=cf1:type, timestamp=1494988387789, value=24983
 186587271101491481307957 column=cf1:time, timestamp=1494988387789, value=123456160
 186587271101491481307957
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24746
 1865872711014914813079570 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079570 column=cf1:type, timestamp=1494988387789, value=24705
 1865872711014914813079572 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456501
 1865872711014914813079572 column=cf1:type, timestamp=1494988387789, value=2410
 1865872711014914813079574 column=cf1:time, timestamp=1494988387789, value=123456271

[11:07:23] INFO:     1865872711014914813079574 column=cf1:type, timestamp=1494988387789, value=24354
 1865872711014914813079576 column=cf1:time, timestamp=1494988387789, value=123456836
 1865872711014914813079576 column=cf1:type, timestamp=1494988387789, value=24163

[11:07:23] INFO:     1865872711014914813079577 column=cf1:time, timestamp=1494988387789, value=123456705
 1865872711014914813079577 column=cf1:type, timestamp=1494988387789, value=24185
 1865872711014914813079578 column=cf1:time, timestamp=1494988387789, value=123456102
 1865872711014914813079578 column=cf1:type, timestamp=1494988387789, value=24302
[11:07:23] INFO:    
 1865872711014914813079579 column=cf1:time, timestamp=1494988387789, value=12345690
 1865872711014914813079579 column=cf1:type, timestamp=1494988387789, value=24601
 186587271101491481307958 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456124
 186587271101491481307958 column=cf1:type, timestamp=1494988387789, value=24958
 1865872711014914813079580 column=cf1:time, timestamp=1494988387789, value=123456280
 1865872711014914813079580 column=cf1:type, timestamp=1494988387789, value=24320
 1865872711014914813079582 column=cf1:time, timestamp=1494988387789, value=123456986
 
[11:07:23] INFO:    1865872711014914813079582 column=cf1:type, timestamp=1494988387789, value=24955
 1865872711014914813079583 column=cf1:time, timestamp=1494988387789, value=123456461
 1865872711014914813079583 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079584 column=cf1:time, timestamp=1494988387789, value=123456177
 1865872711014914813079584
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24519
 1865872711014914813079585 column=cf1:time, timestamp=1494988387789, value=123456803
 1865872711014914813079585 column=cf1:type, timestamp=1494988387789, value=24968
 
[11:07:23] INFO:    1865872711014914813079587 column=cf1:time, timestamp=1494988387789, value=123456174
 1865872711014914813079587 column=cf1:type, timestamp=1494988387789, value=24431
 1865872711014914813079589 column=cf1:time, timestamp=1494988387789, value=123456608
[11:07:23] INFO:    
 1865872711014914813079589 column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079590 column=cf1:time, timestamp=1494988387789, value=123456815
 
[11:07:23] INFO:    1865872711014914813079590 column=cf1:type, timestamp=1494988387789, value=24997
 1865872711014914813079592 column=cf1:time, timestamp=1494988387789, value=12345656
 1865872711014914813079592 column=cf1:type, timestamp=1494988387789, value=24228
 1865872711014914813079593
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456786
 1865872711014914813079593 column=cf1:type, timestamp=1494988387789, value=24307
 1865872711014914813079597 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079597 column=cf1:type, timestamp=1494988387789, value=24207
 1865872711014914813079598 column=cf1:time, timestamp=1494988387789, value=123456850
 1865872711014914813079598
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24984
 18658727110149148130796 column=cf1:time, timestamp=1494988387789, value=12345647
 18658727110149148130796 column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079603 column=cf1:time, timestamp=1494988387789, value=123456260

[11:07:23] INFO:     1865872711014914813079603 column=cf1:type, timestamp=1494988387789, value=24618
 1865872711014914813079604 column=cf1:time, timestamp=1494988387789, value=123456217
 1865872711014914813079604 column=cf1:type, timestamp=1494988387789, value=2499
 1865872711014914813079605 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456347
 1865872711014914813079605 column=cf1:type, timestamp=1494988387789, value=24225
 1865872711014914813079608 column=cf1:time, timestamp=1494988387789, value=123456919
 1865872711014914813079608 column=cf1:type, timestamp=1494988387789, value=2432
 1865872711014914813079609 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456216
 1865872711014914813079609 column=cf1:type, timestamp=1494988387789, value=24525
 1865872711014914813079610 column=cf1:time, timestamp=1494988387789, value=123456100

[11:07:23] INFO:     1865872711014914813079610 column=cf1:type, timestamp=1494988387789, value=24675
 1865872711014914813079611 column=cf1:time, timestamp=1494988387789, value=123456653
 1865872711014914813079611 column=cf1:type, timestamp=1494988387789, value=24975
 1865872711014914813079613
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456482
 1865872711014914813079613 column=cf1:type, timestamp=1494988387789, value=24697
 1865872711014914813079616 column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079616 column=cf1:type, timestamp=1494988387789, value=24253
 1865872711014914813079617 column=cf1:time, timestamp=1494988387789, value=123456289

[11:07:23] INFO:     1865872711014914813079617 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079618 column=cf1:time, timestamp=1494988387789, value=123456285
 1865872711014914813079618 column=cf1:type, timestamp=1494988387789, value=24519
 1865872711014914813079619 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079619 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24277
 186587271101491481307962 column=cf1:time, timestamp=1494988387789, value=12345625
 186587271101491481307962 column=cf1:type, timestamp=1494988387789, value=24547
 1865872711014914813079620 column=cf1:time, timestamp=1494988387789, value=123456784
 1865872711014914813079620 column=cf1:type, timestamp=1494988387789, value=24496
 1865872711014914813079621 column=cf1:time, timestamp=1494988387789, value=123456257
[11:07:23] INFO:    
 1865872711014914813079621 column=cf1:type, timestamp=1494988387789, value=24806
 1865872711014914813079622 column=cf1:time, timestamp=1494988387789, value=123456229
 1865872711014914813079622 column=cf1:type, timestamp=1494988387789, value=24119
 1865872711014914813079623 column=cf1:time, timestamp=1494988387789, value=123456936
[11:07:23] INFO:    
 1865872711014914813079623 column=cf1:type, timestamp=1494988387789, value=24489
 1865872711014914813079625 column=cf1:time, timestamp=1494988387789, value=123456113
 1865872711014914813079625 column=cf1:type, timestamp=1494988387789, value=24517
 1865872711014914813079626 column=cf1:time, timestamp=1494988387789, value=123456391
[11:07:23] INFO:    
 1865872711014914813079626 column=cf1:type, timestamp=1494988387789, value=24579
 1865872711014914813079627 column=cf1:time, timestamp=1494988387789, value=123456109
 1865872711014914813079627 column=cf1:type, timestamp=1494988387789, value=24593
 1865872711014914813079628 column=cf1:time, timestamp=1494988387789, value=123456861

[11:07:23] INFO:     1865872711014914813079628 column=cf1:type, timestamp=1494988387789, value=24716
 1865872711014914813079629 column=cf1:time, timestamp=1494988387789, value=123456737
 1865872711014914813079629 column=cf1:type, timestamp=1494988387789, value=24309
 1865872711014914813079630 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456863
 1865872711014914813079630 column=cf1:type, timestamp=1494988387789, value=24850
 1865872711014914813079631 column=cf1:time, timestamp=1494988387789, value=123456115
 1865872711014914813079631 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079633
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=12345634
 1865872711014914813079633 column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079634 column=cf1:time, timestamp=1494988387789, value=123456658
 1865872711014914813079634 column=cf1:type, timestamp=1494988387789, value=24371
 1865872711014914813079636
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456425
 1865872711014914813079636 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079637 column=cf1:time, timestamp=1494988387789, value=123456420
 1865872711014914813079637 column=cf1:type, timestamp=1494988387789, value=24451

[11:07:23] INFO:     1865872711014914813079638 column=cf1:time, timestamp=1494988387789, value=123456712
 1865872711014914813079638 column=cf1:type, timestamp=1494988387789, value=24110
 1865872711014914813079639 column=cf1:time, timestamp=1494988387789, value=123456964
 1865872711014914813079639 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=2417
 186587271101491481307964 column=cf1:time, timestamp=1494988387789, value=123456706
 186587271101491481307964 column=cf1:type, timestamp=1494988387789, value=24200
 1865872711014914813079641 column=cf1:time, timestamp=1494988387789, value=123456658
 1865872711014914813079641 column=cf1:type, timestamp=1494988387789, value=24436
[11:07:23] INFO:    
 1865872711014914813079642 column=cf1:time, timestamp=1494988387789, value=123456171
 1865872711014914813079642 column=cf1:type, timestamp=1494988387789, value=24539
 1865872711014914813079644 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079644 column=cf1:type, timestamp=1494988387789, value=24610
 1865872711014914813079646
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456259
 1865872711014914813079646 column=cf1:type, timestamp=1494988387789, value=24532
 1865872711014914813079647 column=cf1:time, timestamp=1494988387789, value=123456442
 1865872711014914813079647 column=cf1:type, timestamp=1494988387789, value=24780
 
[11:07:23] INFO:    1865872711014914813079648 column=cf1:time, timestamp=1494988387789, value=123456467
 1865872711014914813079648 column=cf1:type, timestamp=1494988387789, value=2416
 186587271101491481307965 column=cf1:time, timestamp=1494988387789, value=123456448
 186587271101491481307965 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24560
 1865872711014914813079652 column=cf1:time, timestamp=1494988387789, value=123456196
 1865872711014914813079652 column=cf1:type, timestamp=1494988387789, value=24371
 1865872711014914813079655 column=cf1:time, timestamp=1494988387789, value=123456111

[11:07:23] INFO:     1865872711014914813079655 column=cf1:type, timestamp=1494988387789, value=24585
 1865872711014914813079659 column=cf1:time, timestamp=1494988387789, value=123456305
 1865872711014914813079659 column=cf1:type, timestamp=1494988387789, value=24946
 186587271101491481307966 column=cf1:time, timestamp=1494988387789, value=123456829
 186587271101491481307966 column=cf1:type, timestamp=1494988387789, value=24917

[11:07:23] INFO:     1865872711014914813079662 column=cf1:time, timestamp=1494988387789, value=123456964
 1865872711014914813079662 column=cf1:type, timestamp=1494988387789, value=24437
 1865872711014914813079663 column=cf1:time, timestamp=1494988387789, value=123456297
 1865872711014914813079663
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=2423
 1865872711014914813079665 column=cf1:time, timestamp=1494988387789, value=123456843
 1865872711014914813079665 column=cf1:type, timestamp=1494988387789, value=24593
 
[11:07:23] INFO:    1865872711014914813079667 column=cf1:time, timestamp=1494988387789, value=123456416
 1865872711014914813079667 column=cf1:type, timestamp=1494988387789, value=24593
 1865872711014914813079669 column=cf1:time, timestamp=1494988387789, value=123456522
 1865872711014914813079669 column=cf1:type, timestamp=1494988387789, value=24329

[11:07:23] INFO:     186587271101491481307967 column=cf1:time, timestamp=1494988387789, value=123456269
 186587271101491481307967 column=cf1:type, timestamp=1494988387789, value=24688
 1865872711014914813079671 column=cf1:time, timestamp=1494988387789, value=123456579
 
[11:07:23] INFO:    1865872711014914813079671 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079672 column=cf1:time, timestamp=1494988387789, value=123456662
 1865872711014914813079672 column=cf1:type, timestamp=1494988387789, value=24686
 
[11:07:23] INFO:    1865872711014914813079674 column=cf1:time, timestamp=1494988387789, value=123456276
 1865872711014914813079674 column=cf1:type, timestamp=1494988387789, value=24387
 1865872711014914813079675 column=cf1:time, timestamp=1494988387789, value=123456330
 1865872711014914813079675 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24950
 1865872711014914813079680 column=cf1:time, timestamp=1494988387789, value=123456203
 1865872711014914813079680 column=cf1:type, timestamp=1494988387789, value=24657
 1865872711014914813079681 column=cf1:time, timestamp=1494988387789, value=123456623
 1865872711014914813079681 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24408
 1865872711014914813079684 column=cf1:time, timestamp=1494988387789, value=123456900
 1865872711014914813079684 column=cf1:type, timestamp=1494988387789, value=24760
 1865872711014914813079685 column=cf1:time, timestamp=1494988387789, value=123456488
 1865872711014914813079685 column=cf1:type, timestamp=1494988387789, value=24761
[11:07:23] INFO:    
 1865872711014914813079686 column=cf1:time, timestamp=1494988387789, value=123456237
 1865872711014914813079686 column=cf1:type, timestamp=1494988387789, value=24135
 1865872711014914813079687 column=cf1:time, timestamp=1494988387789, value=123456940
 1865872711014914813079687 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24254
 1865872711014914813079688 column=cf1:time, timestamp=1494988387789, value=123456970
 1865872711014914813079688 column=cf1:type, timestamp=1494988387789, value=24197
 186587271101491481307969 column=cf1:time, timestamp=1494988387789, value=12345629
 186587271101491481307969 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24647
 1865872711014914813079690 column=cf1:time, timestamp=1494988387789, value=123456761
 1865872711014914813079690 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079692 column=cf1:time, timestamp=1494988387789, value=123456610
 1865872711014914813079692 column=cf1:type, timestamp=1494988387789, value=24174

[11:07:23] INFO:     1865872711014914813079694 column=cf1:time, timestamp=1494988387789, value=123456777
 1865872711014914813079694 column=cf1:type, timestamp=1494988387789, value=24874
 1865872711014914813079695 column=cf1:time, timestamp=1494988387789, value=123456111
 1865872711014914813079695 column=cf1:type, timestamp=1494988387789, value=24943
[11:07:23] INFO:    
 1865872711014914813079698 column=cf1:time, timestamp=1494988387789, value=123456526
 1865872711014914813079698 column=cf1:type, timestamp=1494988387789, value=24361
 1865872711014914813079699 column=cf1:time, timestamp=1494988387789, value=12345618
 1865872711014914813079699 column=cf1:type, timestamp=1494988387789, value=24590
 
[11:07:23] INFO:    186587271101491481307970 column=cf1:time, timestamp=1494988387789, value=123456834
 186587271101491481307970 column=cf1:type, timestamp=1494988387789, value=24880
 1865872711014914813079700 column=cf1:time, timestamp=1494988387789, value=123456689
 1865872711014914813079700
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24883
 1865872711014914813079701 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079701 column=cf1:type, timestamp=1494988387789, value=24912
 1865872711014914813079703 column=cf1:time, timestamp=1494988387789, value=123456519
[11:07:23] INFO:    
 1865872711014914813079703 column=cf1:type, timestamp=1494988387789, value=24220
 1865872711014914813079704 column=cf1:time, timestamp=1494988387789, value=123456539
 1865872711014914813079704 column=cf1:type, timestamp=1494988387789, value=24198
 1865872711014914813079705 column=cf1:time, timestamp=1494988387789, value=123456467

[11:07:23] INFO:     1865872711014914813079705 column=cf1:type, timestamp=1494988387789, value=24921
 1865872711014914813079707 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079707 column=cf1:type, timestamp=1494988387789, value=24128
 
[11:07:23] INFO:    1865872711014914813079709 column=cf1:time, timestamp=1494988387789, value=123456420
 1865872711014914813079709 column=cf1:type, timestamp=1494988387789, value=24394
 1865872711014914813079710 column=cf1:time, timestamp=1494988387789, value=123456877

[11:07:23] INFO:     1865872711014914813079710 column=cf1:type, timestamp=1494988387789, value=24720
 1865872711014914813079712 column=cf1:time, timestamp=1494988387789, value=123456880
 1865872711014914813079712 column=cf1:type, timestamp=1494988387789, value=24238
 
[11:07:23] INFO:    1865872711014914813079714 column=cf1:time, timestamp=1494988387789, value=123456341
 1865872711014914813079714 column=cf1:type, timestamp=1494988387789, value=24743
 1865872711014914813079715 column=cf1:time, timestamp=1494988387789, value=123456903
 1865872711014914813079715
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24284
 1865872711014914813079716 column=cf1:time, timestamp=1494988387789, value=123456683
 1865872711014914813079716 column=cf1:type, timestamp=1494988387789, value=249

[11:07:23] INFO:     1865872711014914813079720 column=cf1:time, timestamp=1494988387789, value=123456261
 1865872711014914813079720 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079722 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456238
 1865872711014914813079722 column=cf1:type, timestamp=1494988387789, value=24549
 1865872711014914813079723 column=cf1:time, timestamp=1494988387789, value=123456677
 1865872711014914813079723 column=cf1:type, timestamp=1494988387789, value=24136
 
[11:07:23] INFO:    1865872711014914813079725 column=cf1:time, timestamp=1494988387789, value=12345623
 1865872711014914813079725 column=cf1:type, timestamp=1494988387789, value=24350
 1865872711014914813079727 column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079727
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079728 column=cf1:time, timestamp=1494988387789, value=123456481
 1865872711014914813079728 column=cf1:type, timestamp=1494988387789, value=24207
[11:07:23] INFO:    
 1865872711014914813079729 column=cf1:time, timestamp=1494988387789, value=123456627
 1865872711014914813079729 column=cf1:type, timestamp=1494988387789, value=2422
 186587271101491481307973 column=cf1:time, timestamp=1494988387789, value=123456109
[11:07:23] INFO:    
 186587271101491481307973 column=cf1:type, timestamp=1494988387789, value=24889
 1865872711014914813079731 column=cf1:time, timestamp=1494988387789, value=123456841
 
[11:07:23] INFO:    1865872711014914813079731 column=cf1:type, timestamp=1494988387789, value=24771
 1865872711014914813079732 column=cf1:time, timestamp=1494988387789, value=123456787
 1865872711014914813079732 column=cf1:type, timestamp=1494988387789, value=24853
[11:07:23] INFO:    
 1865872711014914813079734 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079734 column=cf1:type, timestamp=1494988387789, value=24516
 1865872711014914813079735 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456781
 1865872711014914813079735 column=cf1:type, timestamp=1494988387789, value=24356
 1865872711014914813079736 column=cf1:time, timestamp=1494988387789, value=123456391

[11:07:23] INFO:     1865872711014914813079736 column=cf1:type, timestamp=1494988387789, value=24125
 1865872711014914813079739 column=cf1:time, timestamp=1494988387789, value=123456438
 1865872711014914813079739 column=cf1:type, timestamp=1494988387789, value=24241
 186587271101491481307974
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456590
 186587271101491481307974 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079741 column=cf1:time, timestamp=1494988387789, value=12345691
 1865872711014914813079741 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=249
 1865872711014914813079742 column=cf1:time, timestamp=1494988387789, value=123456358
 1865872711014914813079742 column=cf1:type, timestamp=1494988387789, value=24785
 1865872711014914813079743 column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079743 column=cf1:type, timestamp=1494988387789, value=24109

[11:07:23] INFO:     1865872711014914813079744 column=cf1:time, timestamp=1494988387789, value=123456600
 1865872711014914813079744 column=cf1:type, timestamp=1494988387789, value=24182
 1865872711014914813079745 column=cf1:time, timestamp=1494988387789, value=123456399
 1865872711014914813079745 column=cf1:type, timestamp=1494988387789, value=24766
 
[11:07:23] INFO:    1865872711014914813079746 column=cf1:time, timestamp=1494988387789, value=12345683
 1865872711014914813079746 column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079747 column=cf1:time, timestamp=1494988387789, value=123456228
 1865872711014914813079747 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079749 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456591
 1865872711014914813079749 column=cf1:type, timestamp=1494988387789, value=24209
 1865872711014914813079751 column=cf1:time, timestamp=1494988387789, value=123456461
 1865872711014914813079751 column=cf1:type, timestamp=1494988387789, value=24375
 
[11:07:23] INFO:    1865872711014914813079752 column=cf1:time, timestamp=1494988387789, value=123456320
 1865872711014914813079752 column=cf1:type, timestamp=1494988387789, value=24715
 1865872711014914813079756 column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079756 column=cf1:type, timestamp=1494988387789, value=24388
 186587271101491481307976 column=cf1:time, timestamp=1494988387789, value=12345618
 186587271101491481307976 column=cf1:type, timestamp=1494988387789, value=24547
 1865872711014914813079761 column=cf1:time, timestamp=1494988387789, value=123456777
[11:07:23] INFO:    
 1865872711014914813079761 column=cf1:type, timestamp=1494988387789, value=24505
 1865872711014914813079764 column=cf1:time, timestamp=1494988387789, value=123456233
 1865872711014914813079764 column=cf1:type, timestamp=1494988387789, value=24713
 1865872711014914813079765 column=cf1:time, timestamp=1494988387789, value=123456359
 1865872711014914813079765 column=cf1:type, timestamp=1494988387789, value=24285
 1865872711014914813079766
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456417
 1865872711014914813079766 column=cf1:type, timestamp=1494988387789, value=242
 1865872711014914813079767 column=cf1:time, timestamp=1494988387789, value=123456876
 1865872711014914813079767 column=cf1:type, timestamp=1494988387789, value=2493
 1865872711014914813079769 column=cf1:time, timestamp=1494988387789, value=123456309
 1865872711014914813079769
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24823
 1865872711014914813079770 column=cf1:time, timestamp=1494988387789, value=123456791
 1865872711014914813079770 column=cf1:type, timestamp=1494988387789, value=24220
 1865872711014914813079771 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079771 column=cf1:type, timestamp=1494988387789, value=24701
 1865872711014914813079772 column=cf1:time, timestamp=1494988387789, value=123456920

[11:07:23] INFO:     1865872711014914813079772 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079774 column=cf1:time, timestamp=1494988387789, value=123456275
 1865872711014914813079774 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079776 column=cf1:time, timestamp=1494988387789, value=123456227
 1865872711014914813079776 column=cf1:type, timestamp=1494988387789, value=24129
 1865872711014914813079777 column=cf1:time, timestamp=1494988387789, value=123456766
[11:07:23] INFO:    
 1865872711014914813079777 column=cf1:type, timestamp=1494988387789, value=24123
 1865872711014914813079778 column=cf1:time, timestamp=1494988387789, value=123456152
 1865872711014914813079778 column=cf1:type, timestamp=1494988387789, value=2490
 1865872711014914813079779 column=cf1:time, timestamp=1494988387789, value=123456222
 1865872711014914813079779 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079780 column=cf1:time, timestamp=1494988387789, value=123456635
 1865872711014914813079780 column=cf1:type, timestamp=1494988387789, value=24132
 1865872711014914813079782 column=cf1:time, timestamp=1494988387789, value=123456173
 1865872711014914813079782 column=cf1:type, timestamp=1494988387789, value=24282
 1865872711014914813079783 column=cf1:time, timestamp=1494988387789, value=123456813
[11:07:23] INFO:    
 1865872711014914813079783 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079786 column=cf1:time, timestamp=1494988387789, value=12345670
 1865872711014914813079786 column=cf1:type, timestamp=1494988387789, value=2426
 1865872711014914813079787 column=cf1:time, timestamp=1494988387789, value=123456634
 1865872711014914813079787 column=cf1:type, timestamp=1494988387789, value=24945
 1865872711014914813079788 column=cf1:time, timestamp=1494988387789, value=123456554
 1865872711014914813079788 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24428
 186587271101491481307979 column=cf1:time, timestamp=1494988387789, value=123456105
 186587271101491481307979 column=cf1:type, timestamp=1494988387789, value=24162
 1865872711014914813079790 column=cf1:time, timestamp=1494988387789, value=123456641
 1865872711014914813079790 column=cf1:type, timestamp=1494988387789, value=24103
 1865872711014914813079791 column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079791 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24403
 1865872711014914813079792 column=cf1:time, timestamp=1494988387789, value=123456106
 1865872711014914813079792 column=cf1:type, timestamp=1494988387789, value=24561
 1865872711014914813079793 column=cf1:time, timestamp=1494988387789, value=123456435
 1865872711014914813079793 column=cf1:type, timestamp=1494988387789, value=24514
 
[11:07:23] INFO:    1865872711014914813079794 column=cf1:time, timestamp=1494988387789, value=123456576
 1865872711014914813079794 column=cf1:type, timestamp=1494988387789, value=24865
 1865872711014914813079795 column=cf1:time, timestamp=1494988387789, value=123456726

[11:07:23] INFO:     1865872711014914813079795 column=cf1:type, timestamp=1494988387789, value=24608
 1865872711014914813079796
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456473
 1865872711014914813079796 column=cf1:type, timestamp=1494988387789, value=24242
 1865872711014914813079798 column=cf1:time, timestamp=1494988387789, value=123456172
[11:07:23] INFO:    
 1865872711014914813079798 column=cf1:type, timestamp=1494988387789, value=24226
 1865872711014914813079799 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456830
 1865872711014914813079799 column=cf1:type, timestamp=1494988387789, value=24722

[11:07:23] INFO:     18658727110149148130798 column=cf1:time, timestamp=1494988387789, value=123456779
 18658727110149148130798 column=cf1:type, timestamp=1494988387789, value=24498
 1865872711014914813079800
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456914
 1865872711014914813079800 column=cf1:type, timestamp=1494988387789, value=24390
 
[11:07:23] INFO:    1865872711014914813079801 column=cf1:time, timestamp=1494988387789, value=123456900
 1865872711014914813079801 column=cf1:type, timestamp=1494988387789, value=2473
 1865872711014914813079803 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456731
 1865872711014914813079803 column=cf1:type, timestamp=1494988387789, value=24103
 
[11:07:23] INFO:    1865872711014914813079804 column=cf1:time, timestamp=1494988387789, value=123456815
 1865872711014914813079804 column=cf1:type, timestamp=1494988387789, value=24250
 1865872711014914813079805 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456316
 1865872711014914813079805 column=cf1:type, timestamp=1494988387789, value=24673
 1865872711014914813079806 column=cf1:time, timestamp=1494988387789, value=123456501
[11:07:23] INFO:    
 1865872711014914813079806 column=cf1:type, timestamp=1494988387789, value=24263
 
[11:07:23] INFO:    1865872711014914813079807 column=cf1:time, timestamp=1494988387789, value=123456229
 1865872711014914813079807 column=cf1:type, timestamp=1494988387789, value=2455
[11:07:23] INFO:    
 1865872711014914813079808 column=cf1:time, timestamp=1494988387789, value=123456472
 1865872711014914813079808
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24238
 1865872711014914813079809 column=cf1:time, timestamp=1494988387789, value=123456790
 
[11:07:23] INFO:    1865872711014914813079809 column=cf1:type, timestamp=1494988387789, value=24604
 1865872711014914813079811 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456490
 1865872711014914813079811 column=cf1:type, timestamp=1494988387789, value=24441

[11:07:23] INFO:     1865872711014914813079814 column=cf1:time, timestamp=1494988387789, value=123456872
 1865872711014914813079814 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24900
 1865872711014914813079815 column=cf1:time, timestamp=1494988387789, value=123456936
 1865872711014914813079815 column=cf1:type, timestamp=1494988387789, value=24456
 1865872711014914813079817 column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079817 column=cf1:type, timestamp=1494988387789, value=2431
 1865872711014914813079819
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456625
 1865872711014914813079819 column=cf1:type, timestamp=1494988387789, value=24618
 1865872711014914813079821 column=cf1:time, timestamp=1494988387789, value=123456165

[11:07:23] INFO:     1865872711014914813079821 column=cf1:type, timestamp=1494988387789, value=24129
 1865872711014914813079822 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456372
 1865872711014914813079822 column=cf1:type, timestamp=1494988387789, value=24296

[11:07:23] INFO:     1865872711014914813079823 column=cf1:time, timestamp=1494988387789, value=123456123
 
[11:07:23] INFO:    1865872711014914813079823 column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079824 column=cf1:time, timestamp=1494988387789, value=123456285

[11:07:23] INFO:     1865872711014914813079824 column=cf1:type, timestamp=1494988387789, value=2461
 1865872711014914813079825 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456783
 1865872711014914813079825 column=cf1:type, timestamp=1494988387789, value=24884

[11:07:23] INFO:     1865872711014914813079827 column=cf1:time, timestamp=1494988387789, value=123456251
 1865872711014914813079827
[11:07:23] INFO:     column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079829 column=cf1:time, timestamp=1494988387789, value=123456608
 1865872711014914813079829 column=cf1:type, timestamp=1494988387789, value=24148
 186587271101491481307983 column=cf1:time, timestamp=1494988387789, value=123456589
 186587271101491481307983 column=cf1:type, timestamp=1494988387789, value=24971
 1865872711014914813079831 column=cf1:time, timestamp=1494988387789, value=123456528
 1865872711014914813079831 column=cf1:type, timestamp=1494988387789, value=24550
 1865872711014914813079832 column=cf1:time, timestamp=1494988387789, value=123456360
 1865872711014914813079832 column=cf1:type, timestamp=1494988387789, value=24646
 1865872711014914813079833 column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079833 column=cf1:type, timestamp=1494988387789, value=24823
 1865872711014914813079837 column=cf1:time, timestamp=1494988387789, value=123456596
 1865872711014914813079837 column=cf1:type, timestamp=14
[11:07:23] INFO:    94988387789, value=24971
 1865872711014914813079838 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079838 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079839 column=cf1:time, timestamp=1494988387789, value=123456335
 1865872711014914813079839 column=cf1:type, timestamp=1494988387789, value=24155
 1865872711014914813079840 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079840 column=cf1:type, timestamp=1494988387789, value=24846
 1865872711014914813079841 column=cf1:time, timestamp=1494988387789, value=123456414
 1865872711014914813079841 column=cf1:type, timestamp=1494988387789, value=24121
 1865872711014914813079842 column=cf1:time, timestamp=1494988387789, value=123456328
 1865872711014914813079842 column=cf1:type, timestamp=1494988387789, value=24559
 1865872711014914813079843 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079843 column=cf1:type, timestamp=1494988387789, value=2432
 186
[11:07:23] INFO:    5872711014914813079844 column=cf1:time, timestamp=1494988387789, value=123456971
 1865872711014914813079844 column=cf1:type, timestamp=1494988387789, value=24932
 1865872711014914813079849 column=cf1:time, timestamp=1494988387789, value=123456955
 1865872711014914813079849 column=cf1:type, timestamp=1494988387789, value=24773
 186587271101491481307985 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456733
 186587271101491481307985 column=cf1:type, timestamp=1494988387789, value=24754
 1865872711014914813079850 column=cf1:time, timestamp=1494988387789, value=123456738
 1865872711014914813079850 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24781
 1865872711014914813079852 column=cf1:time, timestamp=1494988387789, value=123456618
 1865872711014914813079852 column=cf1:type, timestamp=1494988387789, value=24856
 1865872711014914813079853 column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079853 column=cf1:type, timestamp=1494988387789, value=24779
 1865872711014914813079854 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079854 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079855 column=cf1:time, timestamp=1494988387789, value=123456774
 1865872711014914813079855 column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079856 column=cf1:time, timestamp=1494988387789, value=123456962
 1865872711014914813079856 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079857 column=cf1:time, timestamp=1494988387789, value=123456475
 1865872711014914813079857 column=cf1:type, timestamp=1
[11:07:23] INFO:    494988387789, value=24352
 1865872711014914813079858 column=cf1:time, timestamp=1494988387789, value=123456609
 1865872711014914813079858 column=cf1:type, timestamp=1494988387789, value=24636
 1865872711014914813079860 column=cf1:time, timestamp=1494988387789, value=123456528
 1865872711014914813079860 column=cf1:type, timestamp=1494988387789, value=24664
 1865872711014914813079861 column=cf1:time, timestamp=1494988387789, value=123456275
 1865872711014914813079861 column=cf1:type, timestamp=1494988387789, value=24423
 1865872711014914813079862 column=cf1:time, timestamp=1494988387789, value=123456154
 1865872711014914813079862 column=cf1:type, timestamp=1494988387789, value=24952
 1865872711014914813079863 column=cf1:time, timestamp=1494988387789, value=123456780
 1865872711014914813079863 column=cf1:type, timestamp=1494988387789, value=2467
 1865872711014914813079865 column=cf1:time, timestamp=1494988387789, value=123456551
 1865872711014914813079865 column=cf1:type, timestamp=1494988387789, value=24341
 18
[11:07:23] INFO:    6587271101491481307987 column=cf1:time, timestamp=1494988387789, value=123456123
 186587271101491481307987 column=cf1:type, timestamp=1494988387789, value=24841
 1865872711014914813079870 column=cf1:time, timestamp=1494988387789, value=123456564
 1865872711014914813079870 column=cf1:type, timestamp=1494988387789, value=24425
 1865872711014914813079873 column=cf1:time, timestamp=1494988387789, value=123456258
 1865872711014914813079873 column=cf1:type, timestamp=1494988387789, value=2410
 1865872711014914813079874 column=cf1:time, timestamp=1494988387789, value=123456695
 1865872711014914813079874 column=cf1:type, timestamp=1494988387789, value=24257
 1865872711014914813079875 column=cf1:time, timestamp=1494988387789, value=123456294
 1865872711014914813079875 column=cf1:type, timestamp=1494988387789, value=24272
 1865872711014914813079876 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079876 column=cf1:type, timestamp=1494988387789, value=24513
 1865872711014914813079878 column=
[11:07:23] INFO:    cf1:time, timestamp=1494988387789, value=123456211
 1865872711014914813079878 column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079879 column=cf1:time, timestamp=1494988387789, value=123456961
 1865872711014914813079879 column=cf1:type, timestamp=1494988387789, value=24137
 1865872711014914813079880 column=cf1:time, timestamp=1494988387789, value=12345633
 1865872711014914813079880 column=cf1:type, timestamp=1494988387789, value=24266
 1865872711014914813079881 column=cf1:time, timestamp=1494988387789, value=123456577
 1865872711014914813079881 column=cf1:type, timestamp=1494988387789, value=2480
 1865872711014914813079882 column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079882 column=cf1:type, timestamp=1494988387789, value=24438
 1865872711014914813079883 column=cf1:time, timestamp=1494988387789, value=12345663
 1865872711014914813079883 column=cf1:type, timestamp=1494988387789, value=24236
 1865872711014914813079884 column=cf1:time, timestamp=149498838778
[11:07:23] INFO:    9, value=123456384
 1865872711014914813079884 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079885 column=cf1:time, timestamp=1494988387789, value=123456198
 1865872711014914813079885 column=cf1:type, timestamp=1494988387789, value=24334
 1865872711014914813079887 column=cf1:time, timestamp=1494988387789, value=123456659
 1865872711014914813079887 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079888 column=cf1:time, timestamp=1494988387789, value=123456218
 1865872711014914813079888 column=cf1:type, timestamp=1494988387789, value=24334
 1865872711014914813079889 column=cf1:time, timestamp=1494988387789, value=123456463
 1865872711014914813079889 column=cf1:type, timestamp=1494988387789, value=24893
 1865872711014914813079890 column=cf1:time, timestamp=1494988387789, value=123456942
 1865872711014914813079890 column=cf1:type, timestamp=1494988387789, value=24946
 1865872711014914813079893 column=cf1:time, timestamp=1494988387789, value=123456656
 18658727
[11:07:23] INFO:    11014914813079893 column=cf1:type, timestamp=1494988387789, value=2419
 1865872711014914813079894 column=cf1:time, timestamp=1494988387789, value=123456708
 1865872711014914813079894 column=cf1:type, timestamp=1494988387789, value=24752
 1865872711014914813079895 column=cf1:time, timestamp=1494988387789, value=123456769
 1865872711014914813079895 column=cf1:type, timestamp=1494988387789, value=24324
 1865872711014914813079896 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079896 column=cf1:type, timestamp=1494988387789, value=24896
 1865872711014914813079897 column=cf1:time, timestamp=1494988387789, value=123456469
 1865872711014914813079897 column=cf1:type, timestamp=1494988387789, value=241
 1865872711014914813079898 column=cf1:time, timestamp=1494988387789, value=123456813
 1865872711014914813079898 column=cf1:type, timestamp=1494988387789, value=247
 1865872711014914813079899 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079899 column=cf1:type
[11:07:23] INFO:    , timestamp=1494988387789, value=24371
 18658727110149148130799 column=cf1:time, timestamp=1494988387789, value=123456381
 18658727110149148130799 column=cf1:type, timestamp=1494988387789, value=24947
 186587271101491481307990 column=cf1:time, timestamp=1494988387789, value=123456469
 186587271101491481307990 column=cf1:type, timestamp=1494988387789, value=24776
 1865872711014914813079901 column=cf1:time, timestamp=1494988387789, value=123456602
 1865872711014914813079901 column=cf1:type, timestamp=1494988387789, value=24694
 1865872711014914813079903 column=cf1:time, timestamp=1494988387789, value=12345649
 1865872711014914813079903 column=cf1:type, timestamp=1494988387789, value=2485
 1865872711014914813079904 column=cf1:time, timestamp=1494988387789, value=1234563
 1865872711014914813079904 column=cf1:type, timestamp=1494988387789, value=24556
 1865872711014914813079905 column=cf1:time, timestamp=1494988387789, value=123456494
 1865872711014914813079905 column=cf1:type, timestamp=1494988387789, value=24603
[11:07:23] INFO:    
 1865872711014914813079907 column=cf1:time, timestamp=1494988387789, value=123456563
 1865872711014914813079907 column=cf1:type, timestamp=1494988387789, value=24488
 1865872711014914813079908 column=cf1:time, timestamp=1494988387789, value=12345669
 1865872711014914813079908 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079909 column=cf1:time, timestamp=1494988387789, value=123456455
 1865872711014914813079909 column=cf1:type, timestamp=1494988387789, value=24443
 186587271101491481307991 column=cf1:time, timestamp=1494988387789, value=12345683
[11:07:23] INFO:    
 186587271101491481307991 column=cf1:type, timestamp=1494988387789, value=24473
 1865872711014914813079910 column=cf1:time, timestamp=1494988387789, value=12345662
 1865872711014914813079910 column=cf1:type, timestamp=1494988387789, value=24793
 1865872711014914813079913 column=cf1:time, timestamp=1494988387789, value=123456493
 1865872711014914813079913 column=cf1:type, timestamp=1494988387789, value=24415
 1865872711014914813079915 column=cf1:time, timestamp=1494988387789, value=123456845
 1865872711014914813079915 column=cf1:type, timestamp=1494988387789, value=24257
 1865872711014914813079917 column=cf1:time, timestamp=1494988387789, value=123456153
 1865872711014914813079917 column=cf1:type, timestamp=1494988387789, value=24703
 1865872711014914813079918 column=cf1:time, timestamp=1494988387789, value=12345627
 1865872711014914813079918 column=cf1:type, timestamp=1494988387789, value=24459
 1865872711014914813079919 column=cf1:time, timestamp=1494988387789, value=123456313
 1865872711014914813079919 col
[11:07:23] INFO:    umn=cf1:type, timestamp=1494988387789, value=24373
 1865872711014914813079920 column=cf1:time, timestamp=1494988387789, value=123456496
 1865872711014914813079920 column=cf1:type, timestamp=1494988387789, value=24545
 1865872711014914813079925 column=cf1:time, timestamp=1494988387789, value=123456186
 1865872711014914813079925 column=cf1:type, timestamp=1494988387789, value=24150
 1865872711014914813079927 column=cf1:time, timestamp=1494988387789, value=123456557
 1865872711014914813079927 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079928 column=cf1:time, timestamp=1494988387789, value=123456683
 1865872711014914813079928 column=cf1:type, timestamp=1494988387789, value=24543
 186587271101491481307993 column=cf1:time, timestamp=1494988387789, value=123456867
 186587271101491481307993 column=cf1:type, timestamp=1494988387789, value=24332
 1865872711014914813079930 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456677
 1865872711014914813079930 column=cf1:type, timestamp=1494988387789, value=24775
 
[11:07:23] INFO:    1865872711014914813079932 column=cf1:time, timestamp=1494988387789, value=123456222
 1865872711014914813079932 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24711
 1865872711014914813079935
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456692
 1865872711014914813079935 column=cf1:type, timestamp=1494988387789, value=2471

[11:07:23] INFO:     1865872711014914813079936 column=cf1:time, timestamp=1494988387789, value=123456248
 
[11:07:23] INFO:    1865872711014914813079936 column=cf1:type, timestamp=1494988387789, value=24530
 186587271101491481307994 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456235
 186587271101491481307994 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079940 column=cf1:time, timestamp=1494988387789, value=123456131
[11:07:23] INFO:    
 1865872711014914813079940 column=cf1:type, timestamp=1494988387789, value=24983

[11:07:23] INFO:     1865872711014914813079943 column=cf1:time, timestamp=1494988387789, value=123456854
[11:07:23] INFO:    
 1865872711014914813079943 column=cf1:type, timestamp=1494988387789, value=24553
 1865872711014914813079945
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456624
 
[11:07:23] INFO:    1865872711014914813079945 column=cf1:type, timestamp=1494988387789, value=24925
 
[11:07:23] INFO:    1865872711014914813079946 column=cf1:time, timestamp=1494988387789, value=123456188
 1865872711014914813079946 column=cf1:type, timestamp=1494988387789, value=24307

[11:07:23] INFO:     1865872711014914813079947 column=cf1:time, timestamp=1494988387789, value=12345697
 
[11:07:23] INFO:    1865872711014914813079947 column=cf1:type, timestamp=1494988387789, value=24272
 1865872711014914813079948 column=cf1:time, timestamp=1494988387789, value=123456913

[11:07:23] INFO:     1865872711014914813079948 column=cf1:type, timestamp=1494988387789, value=24258
 1865872711014914813079949 column=cf1:time, timestamp=1494988387789, value=123456686
 
[11:07:23] INFO:    1865872711014914813079949 column=cf1:type, timestamp=1494988387789, value=24304
 186587271101491481307995 column=cf1:time, timestamp=1494988387789, value=123456953
 
[11:07:23] INFO:    186587271101491481307995 column=cf1:type, timestamp=1494988387789, value=24184
 1865872711014914813079950 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456428
 1865872711014914813079950 column=cf1:type, timestamp=1494988387789, value=246
 1865872711014914813079951 column=cf1:time, timestamp=1494988387789, value=123456624
[11:07:23] INFO:    
 1865872711014914813079951 column=cf1:type, timestamp=1494988387789, value=24478

[11:07:23] INFO:     1865872711014914813079952 column=cf1:time, timestamp=1494988387789, value=123456282

[11:07:23] INFO:     1865872711014914813079952 column=cf1:type, timestamp=1494988387789, value=2496
 
[11:07:23] INFO:    1865872711014914813079953 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079953 column=cf1:type, timestamp=1494988387789, value=2475

[11:07:23] INFO:     1865872711014914813079954 column=cf1:time, timestamp=1494988387789, value=123456959

[11:07:23] INFO:     1865872711014914813079954 column=cf1:type, timestamp=1494988387789, value=24785
[11:07:23] INFO:    
 1865872711014914813079956 column=cf1:time, timestamp=1494988387789, value=123456194
[11:07:23] INFO:    
 1865872711014914813079956 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24195
 1865872711014914813079957 column=cf1:time, timestamp=1494988387789, value=123456169

[11:07:23] INFO:     1865872711014914813079957 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24312
 1865872711014914813079959 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456346
 1865872711014914813079959 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24493
 186587271101491481307996 column=cf1:time, timestamp=1494988387789, value=123456686

[11:07:23] INFO:     186587271101491481307996 column=cf1:type, timestamp=1494988387789, value=24556
 1865872711014914813079962
[11:07:23] INFO:     column=cf1:time, timestamp=1494988387789, value=123456997
 1865872711014914813079962 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24480
 1865872711014914813079963 column=cf1:time, timestamp=1494988387789, value=12345661
[11:07:23] INFO:    
 1865872711014914813079963 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24641
 1865872711014914813079964 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456778
 1865872711014914813079964 column=cf1:type, timestamp=1494988387789, value=24188
[11:07:23] INFO:    
 1865872711014914813079965 column=cf1:time, timestamp=1494988387789, value=123456785
[11:07:23] INFO:    
 1865872711014914813079965 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24752
 1865872711014914813079967 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456220
 1865872711014914813079967 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24168
 186587271101491481307997 column=cf1:time, timestamp=1494988387789, value=123456785

[11:07:23] INFO:     186587271101491481307997 column=cf1:type, timestamp=1494988387789, value=24188
 1865872711014914813079970 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456880
 1865872711014914813079970 column=cf1:type, timestamp=1494988387789, value=24355

[11:07:23] INFO:     1865872711014914813079971 column=cf1:time, timestamp=1494988387789, value=123456894
 
[11:07:23] INFO:    1865872711014914813079971 column=cf1:type, timestamp=1494988387789, value=2428
 
[11:07:23] INFO:    1865872711014914813079972 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079972 column=cf1:type, timestamp=1494988387789, value=24536
 1865872711014914813079974 column=cf1:time, timestamp=1494988387789, value=123456193
 1865872711014914813079974 column=cf1:type, timestamp=1494988387789, value=24184
 1865872711014914813079975 column=cf1:time, timestamp=1494988387789, value=12345679
 1865872711014914813079975 column=cf1:type, timestamp=1494988387789, value=24240
 1865872711014914813079976 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456961
 1865872711014914813079976 column=cf1:type, timestamp=1494988387789, value=24830

[11:07:23] INFO:     1865872711014914813079977 column=cf1:time, timestamp=1494988387789, value=123456290
 1865872711014914813079977 column=cf1:type, timestamp=1494988387789, value=24439
 1865872711014914813079978 column=cf1:time, timestamp=1494988387789, value=123456581
 1865872711014914813079978 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24149
 1865872711014914813079979 column=cf1:time, timestamp=1494988387789, value=123456550

[11:07:23] INFO:     1865872711014914813079979 column=cf1:type, timestamp=1494988387789, value=24418
 186587271101491481307998 column=cf1:time, timestamp=1494988387789, value=123456265
 
[11:07:23] INFO:    186587271101491481307998 column=cf1:type, timestamp=1494988387789, value=24589
 1865872711014914813079982 column=cf1:time, timestamp=1494988387789, value=123456147
 1865872711014914813079982 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24457
 1865872711014914813079983 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079983 column=cf1:type, timestamp=1494988387789, value=24451
[11:07:23] INFO:    
 1865872711014914813079984 column=cf1:time, timestamp=1494988387789, value=123456221
 1865872711014914813079984 column=cf1:type, timestamp=1494988387789, value=24222
 1865872711014914813079985 
[11:07:23] INFO:    column=cf1:time, timestamp=1494988387789, value=123456375
 1865872711014914813079985 column=cf1:type, timestamp=1494988387789, value=24774
 1865872711014914813079986 column=cf1:time, timestamp=1494988387789, value=123456744
[11:07:23] INFO:    
 1865872711014914813079986 column=cf1:type, timestamp=1494988387789, value=24196
 1865872711014914813079987 column=cf1:time, timestamp=1494988387789, value=123456893

[11:07:23] INFO:     1865872711014914813079987 column=cf1:type, timestamp=1494988387789, value=24961
 1865872711014914813079988 column=cf1:time, timestamp=1494988387789, value=123456402
 1865872711014914813079988 
[11:07:23] INFO:    column=cf1:type, timestamp=1494988387789, value=24583
 1865872711014914813079990 column=cf1:time, timestamp=1494988387789, value=123456586
 
[11:07:23] INFO:    1865872711014914813079990 column=cf1:type, timestamp=1494988387789, value=24454
 1865872711014914813079991 column=cf1:time, timestamp=1494988387789, value=12345670

[11:07:23] INFO:     1865872711014914813079991 column=cf1:type, timestamp=1494988387789, value=24342
 1865872711014914813079992 column=cf1:time, timestamp=1494988387789, value=123456355

[11:07:23] INFO:     1865872711014914813079992 column=cf1:type, timestamp=1494988387789, value=24948
 1865872711014914813079994 column=cf1:time, timestamp=1494988387789, value=123456693

[11:07:23] INFO:     1865872711014914813079994 column=cf1:type, timestamp=1494988387789, value=24932
 1865872711014914813079995 column=cf1:time, timestamp=1494988387789, value=123456183

[11:07:23] INFO:     1865872711014914813079995 column=cf1:type, timestamp=1494988387789, value=24958
 1865872711014914813079996 column=cf1:time, timestamp=1494988387789, value=12345624

[11:07:23] INFO:     1865872711014914813079996 column=cf1:type, timestamp=1494988387789, value=2415
 1865872711014914813079997 column=cf1:time, timestamp=1494988387789, value=123456215

[11:07:23] INFO:     1865872711014914813079997 column=cf1:type, timestamp=1494988387789, value=24594
 1865872711014914813079999 column=cf1:time, timestamp=1494988387789, value=123456303

[11:07:23] INFO:     1865872711014914813079999 column=cf1:type, timestamp=1494988387789, value=2464
 2 
[11:07:23] INFO:    column=cf1:q, timestamp=1494919193176, value=qq
 t column=cf1:q, timestamp=1494917634220, value=tv

[11:07:23] INFO:    645 row(s) in 0.7980 seconds


[11:07:23] INFO:    Connection channel closed
[11:07:23] INFO:    Check if exec success or not ... 
[11:07:23] INFO:    Execute successfully for command: hbase shell /home/lp/hbase.shell
[11:07:23] INFO:    Now wait 5 seconds to begin next task ...
[11:07:28] INFO:    Connection channel disconnect
[11:07:28] INFO:    SSH connection shutdown

=============== [2017/05/17 14:52:36, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:52:36] INFO:    SSHExec initializing ...
[14:52:36] INFO:    Session initialized and associated with user credential inteast.com
[14:52:36] INFO:    SSHExec initialized successfully
[14:52:36] INFO:    SSHExec trying to connect root@10.1.70.200
[14:52:36] INFO:    SSH connection established
[14:52:37] INFO:    Command is get 'tt','2' exit > /home/lp/hbase.shell
[14:52:37] INFO:    Connection channel established succesfully
[14:52:37] INFO:    Start to run command
[14:52:37] INFO:    Connection channel closed
[14:52:37] INFO:    Check if exec success or not ... 
[14:52:37] INFO:    Execution failed while executing command: get 'tt','2' exit > /home/lp/hbase.shell
[14:52:37] INFO:    Error message: bash: get: command not found
[14:52:37] INFO:    Now wait 5 seconds to begin next task ...
[14:52:42] INFO:    Connection channel disconnect
[14:52:42] INFO:    SSH connection shutdown
[14:52:42] INFO:    Session initialized and associated with user credential inteast.com
[14:52:42] INFO:    SSHExec initialized successfully
[14:52:42] INFO:    SSHExec trying to connect root@10.1.70.200
[14:52:42] INFO:    SSH connection established
[14:52:42] INFO:    Command is hbase shell /home/lp/hbase.shell
[14:52:42] INFO:    Connection channel established succesfully
[14:52:42] INFO:    Start to run command
[14:52:47] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[14:52:47] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[14:52:47] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[14:52:47] INFO:    



=============== [2017/05/17 14:54:24, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:54:24] INFO:    SSHExec initializing ...
[14:54:24] INFO:    Session initialized and associated with user credential inteast.com
[14:54:24] INFO:    SSHExec initialized successfully
[14:54:24] INFO:    SSHExec trying to connect root@10.1.70.200
[14:54:25] INFO:    SSH connection established
[14:54:25] INFO:    Command is echo "get 'tt','2' exit "> /home/lp/hbase.shell
[14:54:25] INFO:    Connection channel established succesfully
[14:54:25] INFO:    Start to run command
[14:54:25] INFO:    Connection channel closed
[14:54:25] INFO:    Check if exec success or not ... 
[14:54:25] INFO:    Execute successfully for command: echo "get 'tt','2' exit "> /home/lp/hbase.shell
[14:54:25] INFO:    Now wait 5 seconds to begin next task ...
[14:54:30] INFO:    Connection channel disconnect
[14:54:30] INFO:    SSH connection shutdown
[14:54:30] INFO:    Session initialized and associated with user credential inteast.com
[14:54:30] INFO:    SSHExec initialized successfully
[14:54:30] INFO:    SSHExec trying to connect root@10.1.70.200
[14:54:30] INFO:    SSH connection established
[14:54:30] INFO:    Command is hbase shell /home/lp/hbase.shell
[14:54:30] INFO:    Connection channel established succesfully
[14:54:30] INFO:    Start to run command
[14:54:36] INFO:    Connection channel closed
[14:54:36] INFO:    Check if exec success or not ... 
[14:54:36] INFO:    Execution failed while executing command: hbase shell /home/lp/hbase.shell
[14:54:36] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 14:54:46 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.availableSyntaxError: /home/lp/hbase.shell:1: syntax error, unexpected tIDENTIFIERget 'tt','2' exit                 ^    load at org/jruby/RubyKernel.java:1087  (root) at /opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/lib/hbase/bin/../bin/hirb.rb:177
[14:54:36] INFO:    Now wait 5 seconds to begin next task ...
[14:54:41] INFO:    Connection channel disconnect
[14:54:41] INFO:    SSH connection shutdown

=============== [2017/05/17 14:55:22, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:55:22] INFO:    SSHExec initializing ...
[14:55:22] INFO:    Session initialized and associated with user credential inteast.com
[14:55:22] INFO:    SSHExec initialized successfully
[14:55:22] INFO:    SSHExec trying to connect root@10.1.70.200
[14:55:23] INFO:    SSH connection established
[14:55:23] INFO:    Command is echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell
[14:55:23] INFO:    Connection channel established succesfully
[14:55:23] INFO:    Start to run command
[14:55:23] INFO:    Connection channel closed
[14:55:23] INFO:    Check if exec success or not ... 
[14:55:23] INFO:    Execute successfully for command: echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell
[14:55:23] INFO:    Now wait 5 seconds to begin next task ...
[14:55:28] INFO:    Connection channel disconnect
[14:55:28] INFO:    SSH connection shutdown
[14:55:28] INFO:    Session initialized and associated with user credential inteast.com
[14:55:28] INFO:    SSHExec initialized successfully
[14:55:28] INFO:    SSHExec trying to connect root@10.1.70.200
[14:55:28] INFO:    SSH connection established
[14:55:28] INFO:    Command is hbase shell /home/lp/hbase.shell
[14:55:28] INFO:    Connection channel established succesfully
[14:55:28] INFO:    Start to run command
[14:55:34] INFO:    COLUMN
[14:55:34] INFO:      CELL

[14:55:34] INFO:     
[14:55:34] INFO:    cf1:q timestamp=1494919193176, value=qq

[14:55:34] INFO:    1 row(s) in 0.2480 seconds
[14:55:34] INFO:    


[14:55:35] INFO:    Connection channel closed
[14:55:35] INFO:    Check if exec success or not ... 
[14:55:35] INFO:    Execute successfully for command: hbase shell /home/lp/hbase.shell
[14:55:35] INFO:    Now wait 5 seconds to begin next task ...
[14:55:40] INFO:    Connection channel disconnect
[14:55:40] INFO:    SSH connection shutdown

=============== [2017/05/17 14:57:26, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:57:26] INFO:    SSHExec initializing ...
[14:57:26] INFO:    Session initialized and associated with user credential inteast.com
[14:57:26] INFO:    SSHExec initialized successfully
[14:57:26] INFO:    SSHExec trying to connect root@10.1.70.200
[14:57:27] INFO:    SSH connection established
[14:57:27] INFO:    Command is echo "scan 'tt' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[14:57:27] INFO:    Connection channel established succesfully
[14:57:27] INFO:    Start to run command
[14:57:33] INFO:    ROW
[14:57:33] INFO:      COLUMN+CELL

[14:57:33] INFO:     
[14:57:33] INFO:    18658727110149148130790 column=cf1:time, timestamp=1494988387789, value=123456844

[14:57:33] INFO:     18658727110149148130790
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24615

[14:57:33] INFO:     
[14:57:33] INFO:    18658727110149148130791 column=cf1:time, timestamp=1494988387789, value=123456290

[14:57:33] INFO:     
[14:57:33] INFO:    18658727110149148130791 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2466

[14:57:33] INFO:     
[14:57:33] INFO:    186587271101491481307910
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456298

[14:57:33] INFO:     
[14:57:33] INFO:    186587271101491481307910 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24152

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079100 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456327

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079100
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24157

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079103
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456528

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079103 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24483

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079106
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456334

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079106 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24633

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079108
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456801

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079108
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24833

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079109
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456150

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079109
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=2487
[14:57:33] INFO:    

[14:57:33] INFO:     
[14:57:33] INFO:    186587271101491481307911 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456954

[14:57:33] INFO:     186587271101491481307911
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=2446

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079111
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=12345668

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079111 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24804

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079113
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=1234565

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079113
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=246

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079114 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456578

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079114 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2494
[14:57:33] INFO:    

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079115 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345626
 
[14:57:33] INFO:    1865872711014914813079115 column=cf1:type, timestamp=1494988387789, value=24589
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079117 column=cf1:time, timestamp=1494988387789, value=123456113

[14:57:33] INFO:     1865872711014914813079117 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24597
 
[14:57:33] INFO:    1865872711014914813079118 column=cf1:time, timestamp=1494988387789, value=123456933

[14:57:33] INFO:     1865872711014914813079118 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24118

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079119 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456572
 1865872711014914813079119
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24464

[14:57:33] INFO:     186587271101491481307912 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456782
 
[14:57:33] INFO:    186587271101491481307912 column=cf1:type, timestamp=1494988387789, value=24375

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079120 column=cf1:time, timestamp=1494988387789, value=123456256

[14:57:33] INFO:     1865872711014914813079120 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24246

[14:57:33] INFO:     1865872711014914813079121
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456478
 
[14:57:33] INFO:    1865872711014914813079121 column=cf1:type, timestamp=1494988387789, value=24492
 1865872711014914813079122 column=cf1:time, timestamp=1494988387789, value=123456900
 
[14:57:33] INFO:    1865872711014914813079122 column=cf1:type, timestamp=1494988387789, value=24190

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079123 column=cf1:time, timestamp=1494988387789, value=123456138

[14:57:33] INFO:     1865872711014914813079123 column=cf1:type, timestamp=1494988387789, value=24346
[14:57:33] INFO:    
 1865872711014914813079124
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456996
 
[14:57:33] INFO:    1865872711014914813079124 column=cf1:type, timestamp=1494988387789, value=24319

[14:57:33] INFO:     1865872711014914813079127 column=cf1:time, timestamp=1494988387789, value=123456708
 1865872711014914813079127 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24867
 1865872711014914813079128 column=cf1:time, timestamp=1494988387789, value=123456326

[14:57:33] INFO:     1865872711014914813079128 column=cf1:type, timestamp=1494988387789, value=24541

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079129 column=cf1:time, timestamp=1494988387789, value=123456771

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079129 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24621
 
[14:57:33] INFO:    186587271101491481307913 column=cf1:time, timestamp=1494988387789, value=123456919
[14:57:33] INFO:    
 186587271101491481307913
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24290

[14:57:33] INFO:     1865872711014914813079130 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456744
 
[14:57:33] INFO:    1865872711014914813079130 column=cf1:type, timestamp=1494988387789, value=2490

[14:57:33] INFO:     1865872711014914813079131
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456313
 
[14:57:33] INFO:    1865872711014914813079131 column=cf1:type, timestamp=1494988387789, value=2478

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079132 column=cf1:time, timestamp=1494988387789, value=123456120
 
[14:57:33] INFO:    1865872711014914813079132 column=cf1:type, timestamp=1494988387789, value=24276

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079134 column=cf1:time, timestamp=1494988387789, value=123456521

[14:57:33] INFO:     1865872711014914813079134 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24292
 
[14:57:33] INFO:    1865872711014914813079135 column=cf1:time, timestamp=1494988387789, value=123456650
[14:57:33] INFO:    

[14:57:33] INFO:     1865872711014914813079135 column=cf1:type, timestamp=1494988387789, value=24883

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079136
[14:57:33] INFO:     
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456126
 1865872711014914813079136 column=cf1:type, timestamp=1494988387789, value=2495
 1865872711014914813079137 column=cf1:time, timestamp=1494988387789, value=12345693

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079137 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24500
 
[14:57:33] INFO:    1865872711014914813079139
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456689
 
[14:57:33] INFO:    1865872711014914813079139 column=cf1:type, timestamp=1494988387789, value=24988
[14:57:33] INFO:    

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079140 column=cf1:time, timestamp=1494988387789, value=123456847
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079140 column=cf1:type, timestamp=1494988387789, value=24131

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079142 column=cf1:time, timestamp=1494988387789, value=123456165
[14:57:33] INFO:    
 1865872711014914813079142
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24866

[14:57:33] INFO:     1865872711014914813079143
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456584

[14:57:33] INFO:     1865872711014914813079143 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24761
 
[14:57:33] INFO:    1865872711014914813079144
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456546

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079144 column=cf1:type, timestamp=1494988387789, value=24283

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079146 column=cf1:time, timestamp=1494988387789, value=12345635
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079146 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24888

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079147 column=cf1:time, timestamp=1494988387789, value=123456952

[14:57:33] INFO:     1865872711014914813079147 column=cf1:type, timestamp=1494988387789, value=24336
 1865872711014914813079150 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456776
 
[14:57:33] INFO:    1865872711014914813079150 column=cf1:type, timestamp=1494988387789, value=24967

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079151 column=cf1:time, timestamp=1494988387789, value=123456717
[14:57:33] INFO:    
 1865872711014914813079151 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24622

[14:57:33] INFO:     1865872711014914813079152 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456621
 1865872711014914813079152 column=cf1:type, timestamp=1494988387789, value=24586
[14:57:33] INFO:    
 1865872711014914813079154 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456661
 
[14:57:33] INFO:    1865872711014914813079154
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24899
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079157 column=cf1:time, timestamp=1494988387789, value=123456675
 
[14:57:33] INFO:    1865872711014914813079157
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24259
 
[14:57:33] INFO:    1865872711014914813079158
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456195
 
[14:57:33] INFO:    1865872711014914813079158 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24713

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079159 column=cf1:time, timestamp=1494988387789, value=123456940

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079159 column=cf1:type, timestamp=1494988387789, value=24882
 
[14:57:33] INFO:    1865872711014914813079160 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456502
 1865872711014914813079160 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2433

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079161 column=cf1:time, timestamp=1494988387789, value=123456738

[14:57:33] INFO:     1865872711014914813079161 column=cf1:type, timestamp=1494988387789, value=24115
[14:57:33] INFO:    

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079162
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456191
 
[14:57:33] INFO:    1865872711014914813079162 column=cf1:type, timestamp=1494988387789, value=24490
[14:57:33] INFO:    

[14:57:33] INFO:     1865872711014914813079164 column=cf1:time, timestamp=1494988387789, value=123456437

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079164 column=cf1:type, timestamp=1494988387789, value=24567
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079165 column=cf1:time, timestamp=1494988387789, value=123456932

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079165 column=cf1:type, timestamp=1494988387789, value=24499

[14:57:33] INFO:     1865872711014914813079168 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456758
 1865872711014914813079168 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24306
 
[14:57:33] INFO:    1865872711014914813079169 column=cf1:time, timestamp=1494988387789, value=123456647

[14:57:33] INFO:     1865872711014914813079169 column=cf1:type, timestamp=1494988387789, value=24123

[14:57:33] INFO:     186587271101491481307917 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456696
 186587271101491481307917 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24978

[14:57:33] INFO:     1865872711014914813079170 column=cf1:time, timestamp=1494988387789, value=123456601

[14:57:33] INFO:     1865872711014914813079170 column=cf1:type, timestamp=1494988387789, value=24802

[14:57:33] INFO:     1865872711014914813079171
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456742
 1865872711014914813079171
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24269

[14:57:33] INFO:     1865872711014914813079172 column=cf1:time, timestamp=1494988387789, value=123456410
[14:57:33] INFO:    
 1865872711014914813079172
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24543
 
[14:57:33] INFO:    1865872711014914813079173
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456306
 
[14:57:33] INFO:    1865872711014914813079173 column=cf1:type, timestamp=1494988387789, value=2423

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079177 column=cf1:time, timestamp=1494988387789, value=12345649

[14:57:33] INFO:     1865872711014914813079177 column=cf1:type, timestamp=1494988387789, value=24760

[14:57:33] INFO:     1865872711014914813079178 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456347

[14:57:33] INFO:     1865872711014914813079178 column=cf1:type, timestamp=1494988387789, value=24566
 
[14:57:33] INFO:    1865872711014914813079182 column=cf1:time, timestamp=1494988387789, value=123456756
 1865872711014914813079182
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24858

[14:57:33] INFO:     1865872711014914813079184
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456564
 1865872711014914813079184 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24965

[14:57:33] INFO:     1865872711014914813079186 column=cf1:time, timestamp=1494988387789, value=123456178
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079186 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24763
 
[14:57:33] INFO:    1865872711014914813079187 column=cf1:time, timestamp=1494988387789, value=123456653
[14:57:33] INFO:    
 1865872711014914813079187 column=cf1:type, timestamp=1494988387789, value=246

[14:57:33] INFO:     1865872711014914813079188 column=cf1:time, timestamp=1494988387789, value=123456299

[14:57:33] INFO:     1865872711014914813079188 column=cf1:type, timestamp=1494988387789, value=24900
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079189 column=cf1:time, timestamp=1494988387789, value=123456636

[14:57:33] INFO:     1865872711014914813079189 column=cf1:type, timestamp=1494988387789, value=24350
[14:57:33] INFO:    
 186587271101491481307919
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456933

[14:57:33] INFO:     186587271101491481307919 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24124
 
[14:57:33] INFO:    1865872711014914813079191 column=cf1:time, timestamp=1494988387789, value=12345637

[14:57:33] INFO:     1865872711014914813079191 column=cf1:type, timestamp=1494988387789, value=24745
[14:57:33] INFO:    
 1865872711014914813079194 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456834

[14:57:33] INFO:     1865872711014914813079194 column=cf1:type, timestamp=1494988387789, value=24951
[14:57:33] INFO:    
 1865872711014914813079195
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079195 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24366

[14:57:33] INFO:     18658727110149148130792 column=cf1:time, timestamp=1494988387789, value=123456590
[14:57:33] INFO:    
 
[14:57:33] INFO:    18658727110149148130792 column=cf1:type, timestamp=1494988387789, value=24463

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079200 column=cf1:time, timestamp=1494988387789, value=12345639
[14:57:33] INFO:    
 1865872711014914813079200 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24239

[14:57:33] INFO:     1865872711014914813079201 column=cf1:time, timestamp=1494988387789, value=123456232
[14:57:33] INFO:    
 1865872711014914813079201 column=cf1:type, timestamp=1494988387789, value=24423

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079202 column=cf1:time, timestamp=1494988387789, value=123456278
 1865872711014914813079202 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24198

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079203 column=cf1:time, timestamp=1494988387789, value=123456358

[14:57:33] INFO:     1865872711014914813079203
[14:57:33] INFO:     
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24486
 1865872711014914813079205 column=cf1:time, timestamp=1494988387789, value=123456247
 1865872711014914813079205 column=cf1:type, timestamp=1494988387789, value=24374

[14:57:33] INFO:     1865872711014914813079206
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456802

[14:57:33] INFO:     1865872711014914813079206 column=cf1:type, timestamp=1494988387789, value=24491
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079207 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456571
 
[14:57:33] INFO:    1865872711014914813079207 column=cf1:type, timestamp=1494988387789, value=24238

[14:57:33] INFO:     1865872711014914813079208
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456727
 
[14:57:33] INFO:    1865872711014914813079208 column=cf1:type, timestamp=1494988387789, value=24937
 
[14:57:33] INFO:    1865872711014914813079210
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456606
 1865872711014914813079210
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24652

[14:57:33] INFO:     1865872711014914813079211 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456973

[14:57:33] INFO:     1865872711014914813079211
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079214 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456384
 1865872711014914813079214
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24483

[14:57:33] INFO:     1865872711014914813079215 column=cf1:time, timestamp=1494988387789, value=12345694

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079215 column=cf1:type, timestamp=1494988387789, value=2420

[14:57:33] INFO:     1865872711014914813079216 column=cf1:time, timestamp=1494988387789, value=123456750
 
[14:57:33] INFO:    1865872711014914813079216 column=cf1:type, timestamp=1494988387789, value=24124

[14:57:33] INFO:     1865872711014914813079217
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456934
 
[14:57:33] INFO:    1865872711014914813079217 column=cf1:type, timestamp=1494988387789, value=24361

[14:57:33] INFO:     1865872711014914813079218 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456904
 1865872711014914813079218
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24733

[14:57:33] INFO:     1865872711014914813079219 column=cf1:time, timestamp=1494988387789, value=123456288
[14:57:33] INFO:    
 1865872711014914813079219 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24901
 
[14:57:33] INFO:    186587271101491481307922 column=cf1:time, timestamp=1494988387789, value=123456523
[14:57:33] INFO:    
 186587271101491481307922 column=cf1:type, timestamp=1494988387789, value=2451

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079220 column=cf1:time, timestamp=1494988387789, value=123456802
 1865872711014914813079220 column=cf1:type, timestamp=1494988387789, value=24935

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079221 column=cf1:time, timestamp=1494988387789, value=123456940
 
[14:57:33] INFO:    1865872711014914813079221 column=cf1:type, timestamp=1494988387789, value=24818

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079223 column=cf1:time, timestamp=1494988387789, value=123456621
 1865872711014914813079223
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24138

[14:57:33] INFO:     1865872711014914813079224 column=cf1:time, timestamp=1494988387789, value=123456513

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079224 column=cf1:type, timestamp=1494988387789, value=24474

[14:57:33] INFO:     1865872711014914813079225 column=cf1:time, timestamp=1494988387789, value=123456719

[14:57:33] INFO:     1865872711014914813079225 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24639
 1865872711014914813079226 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456706
 1865872711014914813079226
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24595
 
[14:57:33] INFO:    1865872711014914813079228 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456371
 
[14:57:33] INFO:    1865872711014914813079228 column=cf1:type, timestamp=1494988387789, value=24143
 
[14:57:33] INFO:    1865872711014914813079229 column=cf1:time, timestamp=1494988387789, value=123456723

[14:57:33] INFO:     1865872711014914813079229
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24855
 
[14:57:33] INFO:    186587271101491481307923 column=cf1:time, timestamp=1494988387789, value=123456894

[14:57:33] INFO:     186587271101491481307923 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24576

[14:57:33] INFO:     1865872711014914813079230 column=cf1:time, timestamp=1494988387789, value=12345675
[14:57:33] INFO:    
 1865872711014914813079230 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24548

[14:57:33] INFO:     1865872711014914813079231 column=cf1:time, timestamp=1494988387789, value=123456698

[14:57:33] INFO:     1865872711014914813079231 column=cf1:type, timestamp=1494988387789, value=2420

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079232 column=cf1:time, timestamp=1494988387789, value=123456548
 
[14:57:33] INFO:    1865872711014914813079232 column=cf1:type, timestamp=1494988387789, value=24804
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079233 column=cf1:time, timestamp=1494988387789, value=123456933
 
[14:57:33] INFO:    1865872711014914813079233 column=cf1:type, timestamp=1494988387789, value=24687

[14:57:33] INFO:     1865872711014914813079235 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456600
 1865872711014914813079235
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=2463
 
[14:57:33] INFO:    1865872711014914813079236 column=cf1:time, timestamp=1494988387789, value=123456470
[14:57:33] INFO:    
 1865872711014914813079236 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24338
 
[14:57:33] INFO:    1865872711014914813079237 column=cf1:time, timestamp=1494988387789, value=123456772

[14:57:33] INFO:     1865872711014914813079237 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24986
 1865872711014914813079238 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456275
 
[14:57:33] INFO:    1865872711014914813079238 column=cf1:type, timestamp=1494988387789, value=24546

[14:57:33] INFO:     1865872711014914813079240
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456959
 
[14:57:33] INFO:    1865872711014914813079240 column=cf1:type, timestamp=1494988387789, value=24973
 
[14:57:33] INFO:    1865872711014914813079241 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345686
 1865872711014914813079241
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24108
 
[14:57:33] INFO:    1865872711014914813079243 column=cf1:time, timestamp=1494988387789, value=123456551
[14:57:33] INFO:    
 1865872711014914813079243 column=cf1:type, timestamp=1494988387789, value=24579

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079245 column=cf1:time, timestamp=1494988387789, value=123456860
 1865872711014914813079245 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24958
 
[14:57:33] INFO:    1865872711014914813079246 column=cf1:time, timestamp=1494988387789, value=123456439

[14:57:33] INFO:     1865872711014914813079246 column=cf1:type, timestamp=1494988387789, value=2433
 
[14:57:33] INFO:    1865872711014914813079247 column=cf1:time, timestamp=1494988387789, value=1234564
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079247 column=cf1:type, timestamp=1494988387789, value=24393

[14:57:33] INFO:     1865872711014914813079249 column=cf1:time, timestamp=1494988387789, value=123456656

[14:57:33] INFO:     1865872711014914813079249
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24922
 
[14:57:33] INFO:    1865872711014914813079251 column=cf1:time, timestamp=1494988387789, value=12345643

[14:57:33] INFO:     1865872711014914813079251 column=cf1:type, timestamp=1494988387789, value=24829

[14:57:33] INFO:     1865872711014914813079252
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456304
 1865872711014914813079252
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24741
 
[14:57:33] INFO:    1865872711014914813079253
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456127
 
[14:57:33] INFO:    1865872711014914813079253 column=cf1:type, timestamp=1494988387789, value=24854

[14:57:33] INFO:     1865872711014914813079254 column=cf1:time, timestamp=1494988387789, value=123456574

[14:57:33] INFO:     1865872711014914813079254 column=cf1:type, timestamp=1494988387789, value=24208

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079255 column=cf1:time, timestamp=1494988387789, value=123456544
 
[14:57:33] INFO:    1865872711014914813079255 column=cf1:type, timestamp=1494988387789, value=24840

[14:57:33] INFO:     1865872711014914813079256
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456470

[14:57:33] INFO:     1865872711014914813079256 column=cf1:type, timestamp=1494988387789, value=24692
 
[14:57:33] INFO:    1865872711014914813079257 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456656
 1865872711014914813079257 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24787
 
[14:57:33] INFO:    1865872711014914813079258 column=cf1:time, timestamp=1494988387789, value=123456838
[14:57:33] INFO:    
 1865872711014914813079258 column=cf1:type, timestamp=1494988387789, value=24448

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079259 column=cf1:time, timestamp=1494988387789, value=123456839
 
[14:57:33] INFO:    1865872711014914813079259 column=cf1:type, timestamp=1494988387789, value=24321
 
[14:57:33] INFO:    1865872711014914813079261 column=cf1:time, timestamp=1494988387789, value=123456338

[14:57:33] INFO:     1865872711014914813079261 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24475
 
[14:57:33] INFO:    1865872711014914813079263 column=cf1:time, timestamp=1494988387789, value=123456997
 
[14:57:33] INFO:    1865872711014914813079263 column=cf1:type, timestamp=1494988387789, value=24368

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079264 column=cf1:time, timestamp=1494988387789, value=12345635

[14:57:33] INFO:     1865872711014914813079264 column=cf1:type, timestamp=1494988387789, value=24894

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079265 column=cf1:time, timestamp=1494988387789, value=123456732

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079265 column=cf1:type, timestamp=1494988387789, value=24586

[14:57:33] INFO:     1865872711014914813079268
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456343
 
[14:57:33] INFO:    1865872711014914813079268 column=cf1:type, timestamp=1494988387789, value=24540

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079269 column=cf1:time, timestamp=1494988387789, value=12345614
 
[14:57:33] INFO:    1865872711014914813079269 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2421
 186587271101491481307927
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=12345699

[14:57:33] INFO:     186587271101491481307927 column=cf1:type, timestamp=1494988387789, value=2429
 
[14:57:33] INFO:    1865872711014914813079272 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456843
 1865872711014914813079272 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24737
 
[14:57:33] INFO:    1865872711014914813079273 column=cf1:time, timestamp=1494988387789, value=123456558

[14:57:33] INFO:     1865872711014914813079273 column=cf1:type, timestamp=1494988387789, value=24156
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079275 column=cf1:time, timestamp=1494988387789, value=123456795

[14:57:33] INFO:     1865872711014914813079275 column=cf1:type, timestamp=1494988387789, value=24297

[14:57:33] INFO:     1865872711014914813079276 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456404
 
[14:57:33] INFO:    1865872711014914813079276 column=cf1:type, timestamp=1494988387789, value=24893

[14:57:33] INFO:     1865872711014914813079277 column=cf1:time, timestamp=1494988387789, value=123456598
[14:57:33] INFO:    
 1865872711014914813079277 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24673
 1865872711014914813079279
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456879

[14:57:33] INFO:     1865872711014914813079279 column=cf1:type, timestamp=1494988387789, value=24769

[14:57:33] INFO:     1865872711014914813079281 column=cf1:time, timestamp=1494988387789, value=123456525
[14:57:33] INFO:    
 1865872711014914813079281 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24615
 
[14:57:33] INFO:    1865872711014914813079284 column=cf1:time, timestamp=1494988387789, value=12345631

[14:57:33] INFO:     1865872711014914813079284 column=cf1:type, timestamp=1494988387789, value=24509

[14:57:33] INFO:     1865872711014914813079285 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456454
 1865872711014914813079285 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24456
 
[14:57:33] INFO:    1865872711014914813079287 column=cf1:time, timestamp=1494988387789, value=123456150

[14:57:33] INFO:     1865872711014914813079287 column=cf1:type, timestamp=1494988387789, value=24611
 
[14:57:33] INFO:    1865872711014914813079288 column=cf1:time, timestamp=1494988387789, value=123456355
 
[14:57:33] INFO:    1865872711014914813079288 column=cf1:type, timestamp=1494988387789, value=24872

[14:57:33] INFO:     1865872711014914813079289 column=cf1:time, timestamp=1494988387789, value=123456613
 
[14:57:33] INFO:    1865872711014914813079289 column=cf1:type, timestamp=1494988387789, value=24113
 
[14:57:33] INFO:    186587271101491481307929 column=cf1:time, timestamp=1494988387789, value=123456106

[14:57:33] INFO:     186587271101491481307929 column=cf1:type, timestamp=1494988387789, value=24319

[14:57:33] INFO:     1865872711014914813079290 column=cf1:time, timestamp=1494988387789, value=123456401

[14:57:33] INFO:     1865872711014914813079290 column=cf1:type, timestamp=1494988387789, value=24445

[14:57:33] INFO:     1865872711014914813079292 column=cf1:time, timestamp=1494988387789, value=123456505
 1865872711014914813079292
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24251
 
[14:57:33] INFO:    1865872711014914813079293 column=cf1:time, timestamp=1494988387789, value=12345622
 
[14:57:33] INFO:    1865872711014914813079293 column=cf1:type, timestamp=1494988387789, value=24798
 1865872711014914813079296 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456524
 
[14:57:33] INFO:    1865872711014914813079296 column=cf1:type, timestamp=1494988387789, value=24188

[14:57:33] INFO:     1865872711014914813079297 column=cf1:time, timestamp=1494988387789, value=123456905

[14:57:33] INFO:     1865872711014914813079297 column=cf1:type, timestamp=1494988387789, value=24927

[14:57:33] INFO:     1865872711014914813079298 column=cf1:time, timestamp=1494988387789, value=123456756

[14:57:33] INFO:     1865872711014914813079298 column=cf1:type, timestamp=1494988387789, value=24515

[14:57:33] INFO:     18658727110149148130793 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456274
 18658727110149148130793 column=cf1:type, timestamp=1494988387789, value=24163

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079300 column=cf1:time, timestamp=1494988387789, value=123456824
 
[14:57:33] INFO:    1865872711014914813079300 column=cf1:type, timestamp=1494988387789, value=24403
 
[14:57:33] INFO:    1865872711014914813079303 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456113
 1865872711014914813079303 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24635
 1865872711014914813079304
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456220
 1865872711014914813079304 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24403
 
[14:57:33] INFO:    1865872711014914813079309
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456796
 1865872711014914813079309
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24362
 
[14:57:33] INFO:    186587271101491481307931 column=cf1:time, timestamp=1494988387789, value=123456496

[14:57:33] INFO:     186587271101491481307931 column=cf1:type, timestamp=1494988387789, value=24751
 
[14:57:33] INFO:    1865872711014914813079310 column=cf1:time, timestamp=1494988387789, value=123456289

[14:57:33] INFO:     1865872711014914813079310 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24766
 1865872711014914813079311
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456981
 
[14:57:33] INFO:    1865872711014914813079311 column=cf1:type, timestamp=1494988387789, value=24179
[14:57:33] INFO:    
 1865872711014914813079316 column=cf1:time, timestamp=1494988387789, value=123456133
 1865872711014914813079316
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=2494
[14:57:33] INFO:    
 1865872711014914813079317 column=cf1:time, timestamp=1494988387789, value=123456663

[14:57:33] INFO:     1865872711014914813079317 column=cf1:type, timestamp=1494988387789, value=24166
 
[14:57:33] INFO:    1865872711014914813079318 column=cf1:time, timestamp=1494988387789, value=123456845
 
[14:57:33] INFO:    1865872711014914813079318 column=cf1:type, timestamp=1494988387789, value=24744

[14:57:33] INFO:     1865872711014914813079320 column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079320
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24862
 1865872711014914813079321
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456934

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079321
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24792
 
[14:57:33] INFO:    1865872711014914813079322 column=cf1:time, timestamp=1494988387789, value=123456596
[14:57:33] INFO:    
 1865872711014914813079322 column=cf1:type, timestamp=1494988387789, value=24683

[14:57:33] INFO:     1865872711014914813079323 column=cf1:time, timestamp=1494988387789, value=123456401

[14:57:33] INFO:     1865872711014914813079323 column=cf1:type, timestamp=1494988387789, value=24863
 1865872711014914813079325
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=12345623
 
[14:57:33] INFO:    1865872711014914813079325 column=cf1:type, timestamp=1494988387789, value=24934
 1865872711014914813079326 column=cf1:time, timestamp=1494988387789, value=123456263
[14:57:33] INFO:    
 1865872711014914813079326 column=cf1:type, timestamp=1494988387789, value=24882

[14:57:33] INFO:     1865872711014914813079327 column=cf1:time, timestamp=1494988387789, value=12345676
 
[14:57:33] INFO:    1865872711014914813079327 column=cf1:type, timestamp=1494988387789, value=24700

[14:57:33] INFO:     1865872711014914813079329 column=cf1:time, timestamp=1494988387789, value=123456246
[14:57:33] INFO:    
 1865872711014914813079329 column=cf1:type, timestamp=1494988387789, value=24784
 186587271101491481307933
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456983
 186587271101491481307933
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24748
 1865872711014914813079330
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456589
 1865872711014914813079330 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24194
 1865872711014914813079331 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079331 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24664
 1865872711014914813079332 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456326
 1865872711014914813079332 column=cf1:type, timestamp=1494988387789, value=24992
[14:57:33] INFO:    
 1865872711014914813079334 column=cf1:time, timestamp=1494988387789, value=12345654
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079334 column=cf1:type, timestamp=1494988387789, value=24687
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079335 column=cf1:time, timestamp=1494988387789, value=123456551

[14:57:33] INFO:     1865872711014914813079335 column=cf1:type, timestamp=1494988387789, value=24651
 1865872711014914813079336 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456739

[14:57:33] INFO:     1865872711014914813079336
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24312
 1865872711014914813079337 column=cf1:time, timestamp=1494988387789, value=123456798

[14:57:33] INFO:     1865872711014914813079337 column=cf1:type, timestamp=1494988387789, value=24949

[14:57:33] INFO:     1865872711014914813079338 column=cf1:time, timestamp=1494988387789, value=123456292

[14:57:33] INFO:     1865872711014914813079338 column=cf1:type, timestamp=1494988387789, value=24838
 
[14:57:33] INFO:    1865872711014914813079339 column=cf1:time, timestamp=1494988387789, value=123456465

[14:57:33] INFO:     1865872711014914813079339 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24761
 1865872711014914813079341 column=cf1:time, timestamp=1494988387789, value=12345661
[14:57:33] INFO:    
 1865872711014914813079341 column=cf1:type, timestamp=1494988387789, value=24118

[14:57:33] INFO:     1865872711014914813079342 column=cf1:time, timestamp=1494988387789, value=12345689

[14:57:33] INFO:     1865872711014914813079342 column=cf1:type, timestamp=1494988387789, value=24123

[14:57:33] INFO:     1865872711014914813079343 column=cf1:time, timestamp=1494988387789, value=123456261
 1865872711014914813079343 column=cf1:type, timestamp=1494988387789, value=24744
 1865872711014914813079344 column=cf1:time, timestamp=1494988387789, value=123456148
 1865872711014914813079344 column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079345 column=cf1:time, timestamp=1494988387789, value=123456874

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079345 column=cf1:type, timestamp=1494988387789, value=24324
 1865872711014914813079346 column=cf1:time, timestamp=1494988387789, value=123456244
 
[14:57:33] INFO:    1865872711014914813079346 column=cf1:type, timestamp=1494988387789, value=24762
 1865872711014914813079347 column=cf1:time, timestamp=1494988387789, value=123456244
[14:57:33] INFO:    
 1865872711014914813079347 column=cf1:type, timestamp=1494988387789, value=24780
 1865872711014914813079348
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456694
 1865872711014914813079348 column=cf1:type, timestamp=1494988387789, value=24850
[14:57:33] INFO:    
 186587271101491481307935 column=cf1:time, timestamp=1494988387789, value=123456413
 186587271101491481307935 column=cf1:type, timestamp=1494988387789, value=24846
 1865872711014914813079352 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345687
 1865872711014914813079352 column=cf1:type, timestamp=1494988387789, value=24979

[14:57:33] INFO:     1865872711014914813079353 column=cf1:time, timestamp=1494988387789, value=123456426
 1865872711014914813079353 column=cf1:type, timestamp=1494988387789, value=2474
 
[14:57:33] INFO:    1865872711014914813079354 column=cf1:time, timestamp=1494988387789, value=123456630
 
[14:57:33] INFO:    1865872711014914813079354 column=cf1:type, timestamp=1494988387789, value=24442
 1865872711014914813079356 column=cf1:time, timestamp=1494988387789, value=123456317

[14:57:33] INFO:     1865872711014914813079356 column=cf1:type, timestamp=1494988387789, value=24619

[14:57:33] INFO:     1865872711014914813079358 column=cf1:time, timestamp=1494988387789, value=123456742

[14:57:33] INFO:     1865872711014914813079358 column=cf1:type, timestamp=1494988387789, value=24648
 
[14:57:33] INFO:    1865872711014914813079359 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345622

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079359 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24755
 186587271101491481307936 column=cf1:time, timestamp=1494988387789, value=123456497
[14:57:33] INFO:    
 186587271101491481307936 column=cf1:type, timestamp=1494988387789, value=24122
[14:57:33] INFO:    
 1865872711014914813079361 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456726
 1865872711014914813079361 column=cf1:type, timestamp=1494988387789, value=24463
 
[14:57:33] INFO:    1865872711014914813079363 column=cf1:time, timestamp=1494988387789, value=123456111

[14:57:33] INFO:     1865872711014914813079363 column=cf1:type, timestamp=1494988387789, value=24233

[14:57:33] INFO:     1865872711014914813079364 column=cf1:time, timestamp=1494988387789, value=123456662

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079364 column=cf1:type, timestamp=1494988387789, value=24597
 
[14:57:33] INFO:    1865872711014914813079367 column=cf1:time, timestamp=1494988387789, value=123456161
 1865872711014914813079367 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24122
 1865872711014914813079368 column=cf1:time, timestamp=1494988387789, value=123456447

[14:57:33] INFO:     1865872711014914813079368 column=cf1:type, timestamp=1494988387789, value=24328
 
[14:57:33] INFO:    1865872711014914813079370 column=cf1:time, timestamp=1494988387789, value=123456859
 
[14:57:33] INFO:    1865872711014914813079370 column=cf1:type, timestamp=1494988387789, value=24262
 
[14:57:33] INFO:    1865872711014914813079372 column=cf1:time, timestamp=1494988387789, value=12345618
 1865872711014914813079372 column=cf1:type, timestamp=1494988387789, value=24508

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079374 column=cf1:time, timestamp=1494988387789, value=123456400
 
[14:57:33] INFO:    1865872711014914813079374 column=cf1:type, timestamp=1494988387789, value=24525
 1865872711014914813079375 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345655
 1865872711014914813079375 column=cf1:type, timestamp=1494988387789, value=24398

[14:57:33] INFO:     1865872711014914813079379 column=cf1:time, timestamp=1494988387789, value=123456508

[14:57:33] INFO:     1865872711014914813079379 column=cf1:type, timestamp=1494988387789, value=24709
 186587271101491481307938 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456378

[14:57:33] INFO:     186587271101491481307938 column=cf1:type, timestamp=1494988387789, value=24318

[14:57:33] INFO:     1865872711014914813079382 column=cf1:time, timestamp=1494988387789, value=12345696
 1865872711014914813079382 column=cf1:type, timestamp=1494988387789, value=24287

[14:57:33] INFO:     1865872711014914813079384 column=cf1:time, timestamp=1494988387789, value=123456878
 
[14:57:33] INFO:    1865872711014914813079384 column=cf1:type, timestamp=1494988387789, value=24897
 
[14:57:33] INFO:    1865872711014914813079386 column=cf1:time, timestamp=1494988387789, value=123456602
 1865872711014914813079386 column=cf1:type, timestamp=1494988387789, value=24719

[14:57:33] INFO:     1865872711014914813079389 column=cf1:time, timestamp=1494988387789, value=123456570

[14:57:33] INFO:     1865872711014914813079389 column=cf1:type, timestamp=1494988387789, value=24633
 1865872711014914813079390 column=cf1:time, timestamp=1494988387789, value=123456885
 1865872711014914813079390 column=cf1:type, timestamp=1494988387789, value=24622
 1865872711014914813079391 column=cf1:time, timestamp=1494988387789, value=123456378
 1865872711014914813079391 column=cf1:type, timestamp=1494988387789, value=24513
 1865872711014914813079393 column=cf1:time, timestamp=1494988387789, value=12345697
 1865872711014914813079393 column=cf1:type, timestamp=1494988387789, value=24692
 1865872711014914813079394 column=cf1:time, timestamp=1494988387789, value=123456560
 1865872711014914813079394 column=cf1:type, timestamp=1494988387789, value=24982
 1865872711014914813079395 column=cf1:time, timestamp=1494988387789, value=123456952
 1865872711014914813079395 column=cf1:type, timestamp=1494988387789, value=24708
 186587271101491481307940 column=cf1:time, timestamp=1494988387789, value=123456826
 186587271101491481307940 colu
[14:57:33] INFO:    mn=cf1:type, timestamp=1494988387789, value=244
 1865872711014914813079401 column=cf1:time, timestamp=1494988387789, value=12345611
 1865872711014914813079401 column=cf1:type, timestamp=1494988387789, value=24566
 1865872711014914813079402 column=cf1:time, timestamp=1494988387789, value=123456366
 1865872711014914813079402 column=cf1:type, timestamp=1494988387789, value=24472
 1865872711014914813079403 column=cf1:time, timestamp=1494988387789, value=123456711
 1865872711014914813079403 column=cf1:type, timestamp=1494988387789, value=2438
 1865872711014914813079404 column=cf1:time, timestamp=1494988387789, value=123456946
 
[14:57:33] INFO:    1865872711014914813079404 column=cf1:type, timestamp=1494988387789, value=24695
 1865872711014914813079405 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456379
 1865872711014914813079405 column=cf1:type, timestamp=1494988387789, value=24149
 1865872711014914813079406 column=cf1:time, timestamp=1494988387789, value=123456403

[14:57:33] INFO:     1865872711014914813079406 column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079408
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=12345617
 1865872711014914813079408 column=cf1:type, timestamp=1494988387789, value=24352
[14:57:33] INFO:    
 1865872711014914813079409 column=cf1:time, timestamp=1494988387789, value=123456102

[14:57:33] INFO:     1865872711014914813079409 column=cf1:type, timestamp=1494988387789, value=24989
 
[14:57:33] INFO:    186587271101491481307941 column=cf1:time, timestamp=1494988387789, value=123456173

[14:57:33] INFO:     186587271101491481307941 column=cf1:type, timestamp=1494988387789, value=24405

[14:57:33] INFO:     1865872711014914813079412 column=cf1:time, timestamp=1494988387789, value=12345646

[14:57:33] INFO:     1865872711014914813079412 column=cf1:type, timestamp=1494988387789, value=2491

[14:57:33] INFO:     1865872711014914813079413 column=cf1:time, timestamp=1494988387789, value=12345647
 
[14:57:33] INFO:    1865872711014914813079413 column=cf1:type, timestamp=1494988387789, value=24176
 
[14:57:33] INFO:    1865872711014914813079415 column=cf1:time, timestamp=1494988387789, value=123456118
 1865872711014914813079415
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=2434
 1865872711014914813079416 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456725
 1865872711014914813079416 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24734
 1865872711014914813079418 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456552
 1865872711014914813079418 column=cf1:type, timestamp=1494988387789, value=24660
 
[14:57:33] INFO:    1865872711014914813079420 column=cf1:time, timestamp=1494988387789, value=123456193
 
[14:57:33] INFO:    1865872711014914813079420 column=cf1:type, timestamp=1494988387789, value=2427
 
[14:57:33] INFO:    1865872711014914813079421 column=cf1:time, timestamp=1494988387789, value=123456268
 1865872711014914813079421 column=cf1:type, timestamp=1494988387789, value=24326

[14:57:33] INFO:     1865872711014914813079422 column=cf1:time, timestamp=1494988387789, value=123456228

[14:57:33] INFO:     1865872711014914813079422 column=cf1:type, timestamp=1494988387789, value=24767

[14:57:33] INFO:     1865872711014914813079424 column=cf1:time, timestamp=1494988387789, value=123456463
 
[14:57:33] INFO:    1865872711014914813079424 column=cf1:type, timestamp=1494988387789, value=24463
 
[14:57:33] INFO:    1865872711014914813079425 column=cf1:time, timestamp=1494988387789, value=123456168
 1865872711014914813079425 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24653
 1865872711014914813079426 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456471
 1865872711014914813079426 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24602
 1865872711014914813079427 column=cf1:time, timestamp=1494988387789, value=123456267

[14:57:33] INFO:     1865872711014914813079427 column=cf1:type, timestamp=1494988387789, value=24676
 1865872711014914813079429 column=cf1:time, timestamp=1494988387789, value=123456631
 1865872711014914813079429 column=cf1:type, timestamp=1494988387789, value=24921
[14:57:33] INFO:    
 186587271101491481307943 column=cf1:time, timestamp=1494988387789, value=123456738
[14:57:33] INFO:    
 186587271101491481307943 column=cf1:type, timestamp=1494988387789, value=24677

[14:57:33] INFO:     1865872711014914813079430 column=cf1:time, timestamp=1494988387789, value=123456239
 1865872711014914813079430 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24204
 1865872711014914813079433 column=cf1:time, timestamp=1494988387789, value=123456291
 
[14:57:33] INFO:    1865872711014914813079433 column=cf1:type, timestamp=1494988387789, value=24110
 1865872711014914813079434 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456707
 1865872711014914813079434
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24266
 1865872711014914813079435 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345692
 1865872711014914813079435
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24337
 1865872711014914813079436 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456569
 1865872711014914813079436 column=cf1:type, timestamp=1494988387789, value=24962
 
[14:57:33] INFO:    1865872711014914813079438 column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079438
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24309
 1865872711014914813079439 column=cf1:time, timestamp=1494988387789, value=123456517

[14:57:33] INFO:     1865872711014914813079439 column=cf1:type, timestamp=1494988387789, value=24403

[14:57:33] INFO:     186587271101491481307944 column=cf1:time, timestamp=1494988387789, value=123456509
 
[14:57:33] INFO:    186587271101491481307944 column=cf1:type, timestamp=1494988387789, value=2457
 1865872711014914813079441
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079441 column=cf1:type, timestamp=1494988387789, value=24435
[14:57:33] INFO:    
 1865872711014914813079442 column=cf1:time, timestamp=1494988387789, value=123456899

[14:57:33] INFO:     1865872711014914813079442 column=cf1:type, timestamp=1494988387789, value=24848
 1865872711014914813079443
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456164
 1865872711014914813079443
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24622
 1865872711014914813079444
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456731
 1865872711014914813079444 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24504
 1865872711014914813079446 column=cf1:time, timestamp=1494988387789, value=123456499

[14:57:33] INFO:     1865872711014914813079446 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079447 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456998
 1865872711014914813079447 column=cf1:type, timestamp=1494988387789, value=24162
[14:57:33] INFO:    
 1865872711014914813079448 column=cf1:time, timestamp=1494988387789, value=12345655
 
[14:57:33] INFO:    1865872711014914813079448 column=cf1:type, timestamp=1494988387789, value=24870
 1865872711014914813079449 column=cf1:time, timestamp=1494988387789, value=123456943
[14:57:33] INFO:    
 1865872711014914813079449 column=cf1:type, timestamp=1494988387789, value=2483
 
[14:57:33] INFO:    186587271101491481307945 column=cf1:time, timestamp=1494988387789, value=123456385
 186587271101491481307945 column=cf1:type, timestamp=1494988387789, value=24944
[14:57:33] INFO:    
 1865872711014914813079450 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079450 column=cf1:type, timestamp=1494988387789, value=24400
 1865872711014914813079452 column=cf1:time, timestamp=1494988387789, value=123456416

[14:57:33] INFO:     1865872711014914813079452 column=cf1:type, timestamp=1494988387789, value=24347
 1865872711014914813079453 column=cf1:time, timestamp=1494988387789, value=123456158

[14:57:33] INFO:     1865872711014914813079453 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079454 column=cf1:time, timestamp=1494988387789, value=123456173

[14:57:33] INFO:     1865872711014914813079454 column=cf1:type, timestamp=1494988387789, value=24366
 1865872711014914813079457 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456262
 1865872711014914813079457 column=cf1:type, timestamp=1494988387789, value=24598

[14:57:33] INFO:     1865872711014914813079458 column=cf1:time, timestamp=1494988387789, value=123456540
 
[14:57:33] INFO:    1865872711014914813079458 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079459 column=cf1:time, timestamp=1494988387789, value=123456724
 1865872711014914813079459 column=cf1:type, timestamp=1494988387789, value=24728
 186587271101491481307946 column=cf1:time, timestamp=1494988387789, value=123456925

[14:57:33] INFO:     186587271101491481307946 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079460 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456117
 1865872711014914813079460 column=cf1:type, timestamp=1494988387789, value=24750

[14:57:33] INFO:     1865872711014914813079464 column=cf1:time, timestamp=1494988387789, value=123456504
 1865872711014914813079464 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24678
 1865872711014914813079466 column=cf1:time, timestamp=1494988387789, value=123456401
 
[14:57:33] INFO:    1865872711014914813079466 column=cf1:type, timestamp=1494988387789, value=24960
 1865872711014914813079468 column=cf1:time, timestamp=1494988387789, value=123456306

[14:57:33] INFO:     1865872711014914813079468 column=cf1:type, timestamp=1494988387789, value=24536
 1865872711014914813079469
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456406
 1865872711014914813079469 column=cf1:type, timestamp=1494988387789, value=24980

[14:57:33] INFO:     1865872711014914813079471 column=cf1:time, timestamp=1494988387789, value=123456236
 1865872711014914813079471 column=cf1:type, timestamp=1494988387789, value=24583
 1865872711014914813079472 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079472 column=cf1:type, timestamp=1494988387789, value=2491
 1865872711014914813079473 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456150
 1865872711014914813079473 column=cf1:type, timestamp=1494988387789, value=24582
 1865872711014914813079474 column=cf1:time, timestamp=1494988387789, value=123456639

[14:57:33] INFO:     1865872711014914813079474 column=cf1:type, timestamp=1494988387789, value=24702

[14:57:33] INFO:     1865872711014914813079475 column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079475 column=cf1:type, timestamp=1494988387789, value=24461
 1865872711014914813079479 column=cf1:time, timestamp=1494988387789, value=123456427
 1865872711014914813079479 column=cf1:type, timestamp=1494988387789, value=24527
 186587271101491481307948 column=cf1:time, timestamp=1494988387789, value=123456837
 186587271101491481307948 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24668
 1865872711014914813079481 column=cf1:time, timestamp=1494988387789, value=123456204
 1865872711014914813079481 column=cf1:type, timestamp=1494988387789, value=24891

[14:57:33] INFO:     1865872711014914813079482 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079482 column=cf1:type, timestamp=1494988387789, value=24155

[14:57:33] INFO:     1865872711014914813079483 column=cf1:time, timestamp=1494988387789, value=123456796
 
[14:57:33] INFO:    1865872711014914813079483 column=cf1:type, timestamp=1494988387789, value=24227
 1865872711014914813079485 column=cf1:time, timestamp=1494988387789, value=123456592
[14:57:33] INFO:    
 1865872711014914813079485 column=cf1:type, timestamp=1494988387789, value=24690
 1865872711014914813079487 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079487 column=cf1:type, timestamp=1494988387789, value=24253

[14:57:33] INFO:     1865872711014914813079488 column=cf1:time, timestamp=1494988387789, value=123456243
 1865872711014914813079488 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24627
 186587271101491481307949 column=cf1:time, timestamp=1494988387789, value=123456179

[14:57:33] INFO:     186587271101491481307949 column=cf1:type, timestamp=1494988387789, value=24693
 
[14:57:33] INFO:    1865872711014914813079493 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079493 column=cf1:type, timestamp=1494988387789, value=24837
 1865872711014914813079495 column=cf1:time, timestamp=1494988387789, value=1234561
 1865872711014914813079495 column=cf1:type, timestamp=1494988387789, value=24597
 1865872711014914813079496 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456991
 1865872711014914813079496 column=cf1:type, timestamp=1494988387789, value=2498
 1865872711014914813079497
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456791
 1865872711014914813079497 column=cf1:type, timestamp=1494988387789, value=24540
 1865872711014914813079498 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079498 column=cf1:type, timestamp=1494988387789, value=24514
 1865872711014914813079499 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345647
 1865872711014914813079499 column=cf1:type, timestamp=1494988387789, value=24760
 18658727110149148130795 column=cf1:time, timestamp=1494988387789, value=123456118

[14:57:33] INFO:     18658727110149148130795 column=cf1:type, timestamp=1494988387789, value=24128
 186587271101491481307950 column=cf1:time, timestamp=1494988387789, value=123456613
[14:57:33] INFO:    
 186587271101491481307950 column=cf1:type, timestamp=1494988387789, value=24205
 1865872711014914813079500 column=cf1:time, timestamp=1494988387789, value=123456586

[14:57:33] INFO:     1865872711014914813079500 column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079501 column=cf1:time, timestamp=1494988387789, value=123456951
 
[14:57:33] INFO:    1865872711014914813079501 column=cf1:type, timestamp=1494988387789, value=24169
 1865872711014914813079503 column=cf1:time, timestamp=1494988387789, value=123456179
[14:57:33] INFO:    
 1865872711014914813079503 column=cf1:type, timestamp=1494988387789, value=24868

[14:57:33] INFO:     1865872711014914813079507 column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079507 column=cf1:type, timestamp=1494988387789, value=2426
 1865872711014914813079508 column=cf1:time, timestamp=1494988387789, value=123456660
 1865872711014914813079508 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24363
 186587271101491481307951 column=cf1:time, timestamp=1494988387789, value=123456329
 186587271101491481307951 column=cf1:type, timestamp=1494988387789, value=24998
[14:57:33] INFO:    
 1865872711014914813079511 column=cf1:time, timestamp=1494988387789, value=123456383

[14:57:33] INFO:     1865872711014914813079511 column=cf1:type, timestamp=1494988387789, value=24907
 1865872711014914813079512 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456594
 1865872711014914813079512 column=cf1:type, timestamp=1494988387789, value=24522

[14:57:33] INFO:     1865872711014914813079513 column=cf1:time, timestamp=1494988387789, value=123456368
 
[14:57:33] INFO:    1865872711014914813079513 column=cf1:type, timestamp=1494988387789, value=24242
 1865872711014914813079515 column=cf1:time, timestamp=1494988387789, value=12345636
[14:57:33] INFO:    
 1865872711014914813079515 column=cf1:type, timestamp=1494988387789, value=2495
[14:57:33] INFO:    
 1865872711014914813079519 column=cf1:time, timestamp=1494988387789, value=123456993
 
[14:57:33] INFO:    1865872711014914813079519 column=cf1:type, timestamp=1494988387789, value=24516
 
[14:57:33] INFO:    186587271101491481307952 column=cf1:time, timestamp=1494988387789, value=123456174

[14:57:33] INFO:     186587271101491481307952 column=cf1:type, timestamp=1494988387789, value=2496
 1865872711014914813079520 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456149
 1865872711014914813079520 column=cf1:type, timestamp=1494988387789, value=24666
 
[14:57:33] INFO:    1865872711014914813079521 column=cf1:time, timestamp=1494988387789, value=123456352
 1865872711014914813079521 column=cf1:type, timestamp=1494988387789, value=24225
[14:57:33] INFO:    
 1865872711014914813079523 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079523 column=cf1:type, timestamp=1494988387789, value=24737
 1865872711014914813079524 column=cf1:time, timestamp=1494988387789, value=123456159

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079524 column=cf1:type, timestamp=1494988387789, value=24287
 1865872711014914813079525 column=cf1:time, timestamp=1494988387789, value=123456165

[14:57:33] INFO:     1865872711014914813079525 column=cf1:type, timestamp=1494988387789, value=24458
 1865872711014914813079526 column=cf1:time, timestamp=1494988387789, value=123456252
 1865872711014914813079526 column=cf1:type, timestamp=1494988387789, value=248
 1865872711014914813079527 column=cf1:time, timestamp=1494988387789, value=123456225
 1865872711014914813079527 column=cf1:type, timestamp=1494988387789, value=24788

[14:57:33] INFO:     1865872711014914813079529 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079529 column=cf1:type, timestamp=1494988387789, value=24656
 186587271101491481307953 column=cf1:time, timestamp=1494988387789, value=123456621
 186587271101491481307953 column=cf1:type, timestamp=1494988387789, value=2448

[14:57:33] INFO:     1865872711014914813079530 column=cf1:time, timestamp=1494988387789, value=123456585
 1865872711014914813079530 column=cf1:type, timestamp=1494988387789, value=24158
 
[14:57:33] INFO:    1865872711014914813079532 column=cf1:time, timestamp=1494988387789, value=12345621
 1865872711014914813079532 column=cf1:type, timestamp=1494988387789, value=24642
 
[14:57:33] INFO:    1865872711014914813079534 column=cf1:time, timestamp=1494988387789, value=123456620
 1865872711014914813079534 column=cf1:type, timestamp=1494988387789, value=24809

[14:57:33] INFO:     1865872711014914813079536 column=cf1:time, timestamp=1494988387789, value=123456288
 
[14:57:33] INFO:    1865872711014914813079536 column=cf1:type, timestamp=1494988387789, value=24534

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079537 column=cf1:time, timestamp=1494988387789, value=123456262
 1865872711014914813079537 column=cf1:type, timestamp=1494988387789, value=24666
 1865872711014914813079538 column=cf1:time, timestamp=1494988387789, value=123456968
 1865872711014914813079538 column=cf1:type, timestamp=1494988387789, value=24427
[14:57:33] INFO:    
 1865872711014914813079539 column=cf1:time, timestamp=1494988387789, value=12345676
 1865872711014914813079539 column=cf1:type, timestamp=1494988387789, value=24728
 186587271101491481307954 column=cf1:time, timestamp=1494988387789, value=12345687
 186587271101491481307954 column=cf1:type, timestamp=1494988387789, value=24737
[14:57:33] INFO:    
 1865872711014914813079541 column=cf1:time, timestamp=1494988387789, value=123456418

[14:57:33] INFO:     1865872711014914813079541 column=cf1:type, timestamp=1494988387789, value=24587
 
[14:57:33] INFO:    1865872711014914813079542 column=cf1:time, timestamp=1494988387789, value=123456214
 1865872711014914813079542 column=cf1:type, timestamp=1494988387789, value=24951

[14:57:33] INFO:     1865872711014914813079545 column=cf1:time, timestamp=1494988387789, value=123456357

[14:57:33] INFO:     1865872711014914813079545 column=cf1:type, timestamp=1494988387789, value=24990
 1865872711014914813079546
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456578
 1865872711014914813079546 column=cf1:type, timestamp=1494988387789, value=24602

[14:57:33] INFO:     1865872711014914813079547 column=cf1:time, timestamp=1494988387789, value=123456170
[14:57:33] INFO:    
 1865872711014914813079547 column=cf1:type, timestamp=1494988387789, value=24701
 1865872711014914813079549 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456930
 1865872711014914813079549 column=cf1:type, timestamp=1494988387789, value=24569

[14:57:33] INFO:     
[14:57:33] INFO:    186587271101491481307955 column=cf1:time, timestamp=1494988387789, value=123456371
 186587271101491481307955 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24199
 1865872711014914813079550 column=cf1:time, timestamp=1494988387789, value=123456665
 1865872711014914813079550 column=cf1:type, timestamp=1494988387789, value=24642

[14:57:33] INFO:     1865872711014914813079551 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079551 column=cf1:type, timestamp=1494988387789, value=24724
 1865872711014914813079555 column=cf1:time, timestamp=1494988387789, value=12345643
 1865872711014914813079555 column=cf1:type, timestamp=1494988387789, value=24362

[14:57:33] INFO:     1865872711014914813079556 column=cf1:time, timestamp=1494988387789, value=123456642
 1865872711014914813079556 column=cf1:type, timestamp=1494988387789, value=24552
 1865872711014914813079557 column=cf1:time, timestamp=1494988387789, value=123456128

[14:57:33] INFO:     1865872711014914813079557 column=cf1:type, timestamp=1494988387789, value=24762
 
[14:57:33] INFO:    1865872711014914813079558 column=cf1:time, timestamp=1494988387789, value=123456949
 1865872711014914813079558 column=cf1:type, timestamp=1494988387789, value=2481

[14:57:33] INFO:     1865872711014914813079560 column=cf1:time, timestamp=1494988387789, value=1234566
 
[14:57:33] INFO:    1865872711014914813079560 column=cf1:type, timestamp=1494988387789, value=24526
 1865872711014914813079561
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456723
 1865872711014914813079561 column=cf1:type, timestamp=1494988387789, value=24304

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079565 column=cf1:time, timestamp=1494988387789, value=123456123
 1865872711014914813079565 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079567 column=cf1:time, timestamp=1494988387789, value=123456763

[14:57:33] INFO:     1865872711014914813079567 column=cf1:type, timestamp=1494988387789, value=24969
 1865872711014914813079568 column=cf1:time, timestamp=1494988387789, value=123456980

[14:57:33] INFO:     1865872711014914813079568 column=cf1:type, timestamp=1494988387789, value=24406

[14:57:33] INFO:     1865872711014914813079569 column=cf1:time, timestamp=1494988387789, value=1234566
[14:57:33] INFO:    
 1865872711014914813079569 column=cf1:type, timestamp=1494988387789, value=24983
 186587271101491481307957 column=cf1:time, timestamp=1494988387789, value=123456160

[14:57:33] INFO:     186587271101491481307957 column=cf1:type, timestamp=1494988387789, value=24746
 1865872711014914813079570 column=cf1:time, timestamp=1494988387789, value=123456317

[14:57:33] INFO:     1865872711014914813079570 column=cf1:type, timestamp=1494988387789, value=24705
 1865872711014914813079572 column=cf1:time, timestamp=1494988387789, value=123456501

[14:57:33] INFO:     1865872711014914813079572 column=cf1:type, timestamp=1494988387789, value=2410
 
[14:57:33] INFO:    1865872711014914813079574 column=cf1:time, timestamp=1494988387789, value=123456271
 1865872711014914813079574 column=cf1:type, timestamp=1494988387789, value=24354

[14:57:33] INFO:     1865872711014914813079576 column=cf1:time, timestamp=1494988387789, value=123456836
 1865872711014914813079576 column=cf1:type, timestamp=1494988387789, value=24163
 1865872711014914813079577 column=cf1:time, timestamp=1494988387789, value=123456705

[14:57:33] INFO:     1865872711014914813079577 column=cf1:type, timestamp=1494988387789, value=24185

[14:57:33] INFO:     1865872711014914813079578 column=cf1:time, timestamp=1494988387789, value=123456102
 
[14:57:33] INFO:    1865872711014914813079578 column=cf1:type, timestamp=1494988387789, value=24302
 1865872711014914813079579 column=cf1:time, timestamp=1494988387789, value=12345690
[14:57:33] INFO:    
 1865872711014914813079579 column=cf1:type, timestamp=1494988387789, value=24601
 
[14:57:33] INFO:    186587271101491481307958 column=cf1:time, timestamp=1494988387789, value=123456124
 186587271101491481307958 column=cf1:type, timestamp=1494988387789, value=24958

[14:57:33] INFO:     1865872711014914813079580 column=cf1:time, timestamp=1494988387789, value=123456280
 1865872711014914813079580 column=cf1:type, timestamp=1494988387789, value=24320

[14:57:33] INFO:     1865872711014914813079582 column=cf1:time, timestamp=1494988387789, value=123456986
 
[14:57:33] INFO:    1865872711014914813079582 column=cf1:type, timestamp=1494988387789, value=24955
 1865872711014914813079583 column=cf1:time, timestamp=1494988387789, value=123456461

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079583 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079584 column=cf1:time, timestamp=1494988387789, value=123456177

[14:57:33] INFO:     1865872711014914813079584 column=cf1:type, timestamp=1494988387789, value=24519
 1865872711014914813079585 column=cf1:time, timestamp=1494988387789, value=123456803
[14:57:33] INFO:    
 1865872711014914813079585 column=cf1:type, timestamp=1494988387789, value=24968

[14:57:33] INFO:     1865872711014914813079587 column=cf1:time, timestamp=1494988387789, value=123456174
 1865872711014914813079587 column=cf1:type, timestamp=1494988387789, value=24431

[14:57:33] INFO:     1865872711014914813079589 column=cf1:time, timestamp=1494988387789, value=123456608

[14:57:33] INFO:     1865872711014914813079589 column=cf1:type, timestamp=1494988387789, value=24953

[14:57:33] INFO:     1865872711014914813079590 column=cf1:time, timestamp=1494988387789, value=123456815
 
[14:57:33] INFO:    1865872711014914813079590 column=cf1:type, timestamp=1494988387789, value=24997
 1865872711014914813079592 column=cf1:time, timestamp=1494988387789, value=12345656

[14:57:33] INFO:     1865872711014914813079592 column=cf1:type, timestamp=1494988387789, value=24228
 1865872711014914813079593 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456786
 1865872711014914813079593 column=cf1:type, timestamp=1494988387789, value=24307

[14:57:33] INFO:     1865872711014914813079597 column=cf1:time, timestamp=1494988387789, value=123456317
 1865872711014914813079597 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24207
 1865872711014914813079598 column=cf1:time, timestamp=1494988387789, value=123456850

[14:57:33] INFO:     1865872711014914813079598 column=cf1:type, timestamp=1494988387789, value=24984
 18658727110149148130796 column=cf1:time, timestamp=1494988387789, value=12345647
 18658727110149148130796 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079603 column=cf1:time, timestamp=1494988387789, value=123456260
 1865872711014914813079603 column=cf1:type, timestamp=1494988387789, value=24618

[14:57:33] INFO:     1865872711014914813079604 column=cf1:time, timestamp=1494988387789, value=123456217
 1865872711014914813079604 column=cf1:type, timestamp=1494988387789, value=2499
 1865872711014914813079605 column=cf1:time, timestamp=1494988387789, value=123456347
[14:57:33] INFO:    
 1865872711014914813079605 column=cf1:type, timestamp=1494988387789, value=24225
 1865872711014914813079608 column=cf1:time, timestamp=1494988387789, value=123456919
 1865872711014914813079608 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2432
 1865872711014914813079609 column=cf1:time, timestamp=1494988387789, value=123456216

[14:57:33] INFO:     1865872711014914813079609 column=cf1:type, timestamp=1494988387789, value=24525
 1865872711014914813079610 column=cf1:time, timestamp=1494988387789, value=123456100
 1865872711014914813079610 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24675
 1865872711014914813079611 column=cf1:time, timestamp=1494988387789, value=123456653
 1865872711014914813079611 column=cf1:type, timestamp=1494988387789, value=24975

[14:57:33] INFO:     1865872711014914813079613 column=cf1:time, timestamp=1494988387789, value=123456482
 1865872711014914813079613 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24697
 
[14:57:33] INFO:    1865872711014914813079616 column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079616 column=cf1:type, timestamp=1494988387789, value=24253
 1865872711014914813079617 column=cf1:time, timestamp=1494988387789, value=123456289
 1865872711014914813079617 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079618 column=cf1:time, timestamp=1494988387789, value=123456285

[14:57:33] INFO:     1865872711014914813079618 column=cf1:type, timestamp=1494988387789, value=24519
 
[14:57:33] INFO:    1865872711014914813079619 column=cf1:time, timestamp=1494988387789, value=123456165

[14:57:33] INFO:     1865872711014914813079619 column=cf1:type, timestamp=1494988387789, value=24277
 186587271101491481307962 column=cf1:time, timestamp=1494988387789, value=12345625

[14:57:33] INFO:     186587271101491481307962 column=cf1:type, timestamp=1494988387789, value=24547
 
[14:57:33] INFO:    1865872711014914813079620 column=cf1:time, timestamp=1494988387789, value=123456784
 1865872711014914813079620 column=cf1:type, timestamp=1494988387789, value=24496

[14:57:33] INFO:     1865872711014914813079621 column=cf1:time, timestamp=1494988387789, value=123456257
 1865872711014914813079621 column=cf1:type, timestamp=1494988387789, value=24806
[14:57:33] INFO:    
 1865872711014914813079622 column=cf1:time, timestamp=1494988387789, value=123456229
 
[14:57:33] INFO:    1865872711014914813079622 column=cf1:type, timestamp=1494988387789, value=24119
 1865872711014914813079623 column=cf1:time, timestamp=1494988387789, value=123456936

[14:57:33] INFO:     1865872711014914813079623 column=cf1:type, timestamp=1494988387789, value=24489
 1865872711014914813079625 column=cf1:time, timestamp=1494988387789, value=123456113

[14:57:33] INFO:     1865872711014914813079625 column=cf1:type, timestamp=1494988387789, value=24517
 1865872711014914813079626 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456391
 1865872711014914813079626 column=cf1:type, timestamp=1494988387789, value=24579
 
[14:57:33] INFO:    1865872711014914813079627 column=cf1:time, timestamp=1494988387789, value=123456109
 1865872711014914813079627 column=cf1:type, timestamp=1494988387789, value=24593
 1865872711014914813079628 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456861
 1865872711014914813079628 column=cf1:type, timestamp=1494988387789, value=24716
 1865872711014914813079629 column=cf1:time, timestamp=1494988387789, value=123456737
 1865872711014914813079629 column=cf1:type, timestamp=1494988387789, value=24309
 1865872711014914813079630 column=cf1:time, timestamp=1494988387789, value=123456863

[14:57:33] INFO:     1865872711014914813079630 column=cf1:type, timestamp=1494988387789, value=24850
 1865872711014914813079631 column=cf1:time, timestamp=1494988387789, value=123456115
 1865872711014914813079631 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079633 column=cf1:time, timestamp=1494988387789, value=12345634
 1865872711014914813079633 column=cf1:type, timestamp=1494988387789, value=24259
 1865872711014914813079634 column=cf1:time, timestamp=1494988387789, value=123456658
 
[14:57:33] INFO:    1865872711014914813079634 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24371
 1865872711014914813079636 column=cf1:time, timestamp=1494988387789, value=123456425
 1865872711014914813079636 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079637
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456420
 1865872711014914813079637 column=cf1:type, timestamp=1494988387789, value=24451
 1865872711014914813079638 column=cf1:time, timestamp=1494988387789, value=123456712
 1865872711014914813079638 column=cf1:type, timestamp=1494988387789, value=24110

[14:57:33] INFO:     1865872711014914813079639 column=cf1:time, timestamp=1494988387789, value=123456964
 1865872711014914813079639 column=cf1:type, timestamp=1494988387789, value=2417
 186587271101491481307964 column=cf1:time, timestamp=1494988387789, value=123456706
 186587271101491481307964 column=cf1:type, timestamp=1494988387789, value=24200

[14:57:33] INFO:     1865872711014914813079641 column=cf1:time, timestamp=1494988387789, value=123456658
 1865872711014914813079641 column=cf1:type, timestamp=1494988387789, value=24436

[14:57:33] INFO:     1865872711014914813079642 column=cf1:time, timestamp=1494988387789, value=123456171
 1865872711014914813079642 column=cf1:type, timestamp=1494988387789, value=24539
 1865872711014914813079644 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079644 column=cf1:type, timestamp=1494988387789, value=24610

[14:57:33] INFO:     1865872711014914813079646 column=cf1:time, timestamp=1494988387789, value=123456259

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079646 column=cf1:type, timestamp=1494988387789, value=24532
 1865872711014914813079647 column=cf1:time, timestamp=1494988387789, value=123456442
 1865872711014914813079647 column=cf1:type, timestamp=1494988387789, value=24780
 
[14:57:33] INFO:    1865872711014914813079648 column=cf1:time, timestamp=1494988387789, value=123456467
 1865872711014914813079648 column=cf1:type, timestamp=1494988387789, value=2416
 186587271101491481307965
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456448
 186587271101491481307965 column=cf1:type, timestamp=1494988387789, value=24560
 1865872711014914813079652 column=cf1:time, timestamp=1494988387789, value=123456196
 1865872711014914813079652 column=cf1:type, timestamp=1494988387789, value=24371
[14:57:33] INFO:    
 1865872711014914813079655 column=cf1:time, timestamp=1494988387789, value=123456111
[14:57:33] INFO:    
 1865872711014914813079655 column=cf1:type, timestamp=1494988387789, value=24585
 1865872711014914813079659 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456305
 1865872711014914813079659 column=cf1:type, timestamp=1494988387789, value=24946

[14:57:33] INFO:     186587271101491481307966
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456829
 186587271101491481307966 column=cf1:type, timestamp=1494988387789, value=24917
 1865872711014914813079662 column=cf1:time, timestamp=1494988387789, value=123456964

[14:57:33] INFO:     1865872711014914813079662 column=cf1:type, timestamp=1494988387789, value=24437
[14:57:33] INFO:    
 1865872711014914813079663 column=cf1:time, timestamp=1494988387789, value=123456297
 1865872711014914813079663 column=cf1:type, timestamp=1494988387789, value=2423
 1865872711014914813079665 column=cf1:time, timestamp=1494988387789, value=123456843

[14:57:33] INFO:     1865872711014914813079665 column=cf1:type, timestamp=1494988387789, value=24593
 1865872711014914813079667 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456416
 1865872711014914813079667 column=cf1:type, timestamp=1494988387789, value=24593

[14:57:33] INFO:     1865872711014914813079669 column=cf1:time, timestamp=1494988387789, value=123456522
 1865872711014914813079669 column=cf1:type, timestamp=1494988387789, value=24329

[14:57:33] INFO:     186587271101491481307967 column=cf1:time, timestamp=1494988387789, value=123456269
 186587271101491481307967 column=cf1:type, timestamp=1494988387789, value=24688
 1865872711014914813079671 column=cf1:time, timestamp=1494988387789, value=123456579

[14:57:33] INFO:     1865872711014914813079671 column=cf1:type, timestamp=1494988387789, value=2429
 1865872711014914813079672 column=cf1:time, timestamp=1494988387789, value=123456662
 1865872711014914813079672 column=cf1:type, timestamp=1494988387789, value=24686
 1865872711014914813079674 column=cf1:time, timestamp=1494988387789, value=123456276
 1865872711014914813079674 column=cf1:type, timestamp=1494988387789, value=24387
 1865872711014914813079675
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456330
 1865872711014914813079675 column=cf1:type, timestamp=1494988387789, value=24950
 1865872711014914813079680 column=cf1:time, timestamp=1494988387789, value=123456203
 1865872711014914813079680 column=cf1:type, timestamp=1494988387789, value=24657
 
[14:57:33] INFO:    1865872711014914813079681 column=cf1:time, timestamp=1494988387789, value=123456623

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079681 column=cf1:type, timestamp=1494988387789, value=24408
 1865872711014914813079684 column=cf1:time, timestamp=1494988387789, value=123456900
 1865872711014914813079684 column=cf1:type, timestamp=1494988387789, value=24760

[14:57:33] INFO:     1865872711014914813079685 column=cf1:time, timestamp=1494988387789, value=123456488
 1865872711014914813079685 column=cf1:type, timestamp=1494988387789, value=24761
 1865872711014914813079686 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456237
 1865872711014914813079686 column=cf1:type, timestamp=1494988387789, value=24135
 1865872711014914813079687 column=cf1:time, timestamp=1494988387789, value=123456940
 
[14:57:33] INFO:    1865872711014914813079687 column=cf1:type, timestamp=1494988387789, value=24254
 1865872711014914813079688 column=cf1:time, timestamp=1494988387789, value=123456970
 1865872711014914813079688 column=cf1:type, timestamp=1494988387789, value=24197
 
[14:57:33] INFO:    186587271101491481307969 column=cf1:time, timestamp=1494988387789, value=12345629
 186587271101491481307969 column=cf1:type, timestamp=1494988387789, value=24647
 1865872711014914813079690 column=cf1:time, timestamp=1494988387789, value=123456761

[14:57:33] INFO:     1865872711014914813079690 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079692 column=cf1:time, timestamp=1494988387789, value=123456610
 1865872711014914813079692 column=cf1:type, timestamp=1494988387789, value=24174

[14:57:33] INFO:     1865872711014914813079694 column=cf1:time, timestamp=1494988387789, value=123456777

[14:57:33] INFO:     1865872711014914813079694 column=cf1:type, timestamp=1494988387789, value=24874
 1865872711014914813079695 column=cf1:time, timestamp=1494988387789, value=123456111
 1865872711014914813079695 column=cf1:type, timestamp=1494988387789, value=24943
 1865872711014914813079698 column=cf1:time, timestamp=1494988387789, value=123456526
[14:57:33] INFO:    
 1865872711014914813079698 column=cf1:type, timestamp=1494988387789, value=24361

[14:57:33] INFO:     1865872711014914813079699 column=cf1:time, timestamp=1494988387789, value=12345618
 1865872711014914813079699 column=cf1:type, timestamp=1494988387789, value=24590

[14:57:33] INFO:     186587271101491481307970 column=cf1:time, timestamp=1494988387789, value=123456834
 186587271101491481307970 column=cf1:type, timestamp=1494988387789, value=24880

[14:57:33] INFO:     1865872711014914813079700 column=cf1:time, timestamp=1494988387789, value=123456689
 1865872711014914813079700 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24883
 1865872711014914813079701 column=cf1:time, timestamp=1494988387789, value=123456824
 1865872711014914813079701 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24912
 1865872711014914813079703 column=cf1:time, timestamp=1494988387789, value=123456519
 1865872711014914813079703 column=cf1:type, timestamp=1494988387789, value=24220
 1865872711014914813079704 column=cf1:time, timestamp=1494988387789, value=123456539
 
[14:57:33] INFO:    1865872711014914813079704 column=cf1:type, timestamp=1494988387789, value=24198
 1865872711014914813079705 column=cf1:time, timestamp=1494988387789, value=123456467
 1865872711014914813079705 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24921
 1865872711014914813079707 column=cf1:time, timestamp=1494988387789, value=123456525
 1865872711014914813079707 column=cf1:type, timestamp=1494988387789, value=24128
 1865872711014914813079709 column=cf1:time, timestamp=1494988387789, value=123456420
 1865872711014914813079709 column=cf1:type, timestamp=1494988387789, value=24394
 1865872711014914813079710 column=cf1:time, timestamp=1494988387789, value=123456877

[14:57:33] INFO:     1865872711014914813079710 column=cf1:type, timestamp=1494988387789, value=24720

[14:57:33] INFO:     1865872711014914813079712 column=cf1:time, timestamp=1494988387789, value=123456880
 1865872711014914813079712 column=cf1:type, timestamp=1494988387789, value=24238
[14:57:33] INFO:    
 1865872711014914813079714 column=cf1:time, timestamp=1494988387789, value=123456341

[14:57:33] INFO:     1865872711014914813079714 column=cf1:type, timestamp=1494988387789, value=24743
 1865872711014914813079715 column=cf1:time, timestamp=1494988387789, value=123456903

[14:57:33] INFO:     1865872711014914813079715 column=cf1:type, timestamp=1494988387789, value=24284
 1865872711014914813079716 column=cf1:time, timestamp=1494988387789, value=123456683
[14:57:33] INFO:    
 1865872711014914813079716 column=cf1:type, timestamp=1494988387789, value=249
 1865872711014914813079720 column=cf1:time, timestamp=1494988387789, value=123456261

[14:57:33] INFO:     1865872711014914813079720 column=cf1:type, timestamp=1494988387789, value=24568
 1865872711014914813079722 column=cf1:time, timestamp=1494988387789, value=123456238

[14:57:33] INFO:     1865872711014914813079722 column=cf1:type, timestamp=1494988387789, value=24549

[14:57:33] INFO:     1865872711014914813079723 column=cf1:time, timestamp=1494988387789, value=123456677
 1865872711014914813079723 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24136
 1865872711014914813079725 column=cf1:time, timestamp=1494988387789, value=12345623
 1865872711014914813079725 column=cf1:type, timestamp=1494988387789, value=24350
[14:57:33] INFO:    
 1865872711014914813079727 column=cf1:time, timestamp=1494988387789, value=123456574
 1865872711014914813079727 column=cf1:type, timestamp=1494988387789, value=24259

[14:57:33] INFO:     1865872711014914813079728 column=cf1:time, timestamp=1494988387789, value=123456481
 1865872711014914813079728 column=cf1:type, timestamp=1494988387789, value=24207
 1865872711014914813079729 column=cf1:time, timestamp=1494988387789, value=123456627
 1865872711014914813079729 column=cf1:type, timestamp=1494988387789, value=2422
 186587271101491481307973 column=cf1:time, timestamp=1494988387789, value=123456109

[14:57:33] INFO:     186587271101491481307973 column=cf1:type, timestamp=1494988387789, value=24889
 1865872711014914813079731 column=cf1:time, timestamp=1494988387789, value=123456841
 1865872711014914813079731 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24771
 1865872711014914813079732 column=cf1:time, timestamp=1494988387789, value=123456787

[14:57:33] INFO:     1865872711014914813079732 column=cf1:type, timestamp=1494988387789, value=24853
 
[14:57:33] INFO:    1865872711014914813079734 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079734 column=cf1:type, timestamp=1494988387789, value=24516

[14:57:33] INFO:     1865872711014914813079735 column=cf1:time, timestamp=1494988387789, value=123456781
 1865872711014914813079735 column=cf1:type, timestamp=1494988387789, value=24356

[14:57:33] INFO:     1865872711014914813079736 column=cf1:time, timestamp=1494988387789, value=123456391
 1865872711014914813079736 column=cf1:type, timestamp=1494988387789, value=24125

[14:57:33] INFO:     1865872711014914813079739 column=cf1:time, timestamp=1494988387789, value=123456438
 1865872711014914813079739 column=cf1:type, timestamp=1494988387789, value=24241
 186587271101491481307974 column=cf1:time, timestamp=1494988387789, value=123456590
[14:57:33] INFO:    
 186587271101491481307974 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079741 column=cf1:time, timestamp=1494988387789, value=12345691

[14:57:33] INFO:     1865872711014914813079741 column=cf1:type, timestamp=1494988387789, value=249
 
[14:57:33] INFO:    1865872711014914813079742 column=cf1:time, timestamp=1494988387789, value=123456358
 1865872711014914813079742 column=cf1:type, timestamp=1494988387789, value=24785

[14:57:33] INFO:     1865872711014914813079743 column=cf1:time, timestamp=1494988387789, value=123456619
 1865872711014914813079743 column=cf1:type, timestamp=1494988387789, value=24109
[14:57:33] INFO:    
 1865872711014914813079744 column=cf1:time, timestamp=1494988387789, value=123456600

[14:57:33] INFO:     1865872711014914813079744 column=cf1:type, timestamp=1494988387789, value=24182
 1865872711014914813079745 column=cf1:time, timestamp=1494988387789, value=123456399
 1865872711014914813079745 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24766
 1865872711014914813079746 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345683
 1865872711014914813079746 column=cf1:type, timestamp=1494988387789, value=24161
 1865872711014914813079747 column=cf1:time, timestamp=1494988387789, value=123456228

[14:57:33] INFO:     1865872711014914813079747 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079749 column=cf1:time, timestamp=1494988387789, value=123456591

[14:57:33] INFO:     1865872711014914813079749 column=cf1:type, timestamp=1494988387789, value=24209

[14:57:33] INFO:     1865872711014914813079751 column=cf1:time, timestamp=1494988387789, value=123456461
 1865872711014914813079751 column=cf1:type, timestamp=1494988387789, value=24375

[14:57:33] INFO:     1865872711014914813079752 column=cf1:time, timestamp=1494988387789, value=123456320

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079752 column=cf1:type, timestamp=1494988387789, value=24715
 1865872711014914813079756 column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079756 column=cf1:type, timestamp=1494988387789, value=24388

[14:57:33] INFO:     186587271101491481307976 column=cf1:time, timestamp=1494988387789, value=12345618
 186587271101491481307976 column=cf1:type, timestamp=1494988387789, value=24547

[14:57:33] INFO:     1865872711014914813079761 column=cf1:time, timestamp=1494988387789, value=123456777

[14:57:33] INFO:     1865872711014914813079761 column=cf1:type, timestamp=1494988387789, value=24505
 1865872711014914813079764 column=cf1:time, timestamp=1494988387789, value=123456233
 1865872711014914813079764 column=cf1:type, timestamp=1494988387789, value=24713
 1865872711014914813079765 column=cf1:time, timestamp=1494988387789, value=123456359
 1865872711014914813079765 column=cf1:type, timestamp=1494988387789, value=24285
 1865872711014914813079766 column=cf1:time, timestamp=1494988387789, value=123456417
 1865872711014914813079766 column=cf1:type, timestamp=1494988387789, value=242
 1865872711014914813079767 column=cf1:time, timestamp=1494988387789, value=123456876
 1865872711014914813079767 column=cf1:type, timestamp=1494988387789, value=2493

[14:57:33] INFO:     1865872711014914813079769 column=cf1:time, timestamp=1494988387789, value=123456309
 1865872711014914813079769 column=cf1:type, timestamp=1494988387789, value=24823
 1865872711014914813079770 column=cf1:time, timestamp=1494988387789, value=123456791
[14:57:33] INFO:    
 1865872711014914813079770 column=cf1:type, timestamp=1494988387789, value=24220

[14:57:33] INFO:     1865872711014914813079771 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079771 column=cf1:type, timestamp=1494988387789, value=24701
 1865872711014914813079772 column=cf1:time, timestamp=1494988387789, value=123456920
 1865872711014914813079772 column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079774 column=cf1:time, timestamp=1494988387789, value=123456275
 1865872711014914813079774 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079776
[14:57:33] INFO:     column=cf1:time, timestamp=1494988387789, value=123456227
 1865872711014914813079776
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24129
 1865872711014914813079777 column=cf1:time, timestamp=1494988387789, value=123456766

[14:57:33] INFO:     1865872711014914813079777 column=cf1:type, timestamp=1494988387789, value=24123
 1865872711014914813079778 column=cf1:time, timestamp=1494988387789, value=123456152

[14:57:33] INFO:     1865872711014914813079778 column=cf1:type, timestamp=1494988387789, value=2490

[14:57:33] INFO:     1865872711014914813079779 column=cf1:time, timestamp=1494988387789, value=123456222

[14:57:33] INFO:     1865872711014914813079779 column=cf1:type, timestamp=1494988387789, value=24495
 1865872711014914813079780 column=cf1:time, timestamp=1494988387789, value=123456635
 1865872711014914813079780 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24132
 1865872711014914813079782 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456173
 1865872711014914813079782 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24282
 1865872711014914813079783 column=cf1:time, timestamp=1494988387789, value=123456813
 
[14:57:33] INFO:    1865872711014914813079783 column=cf1:type, timestamp=1494988387789, value=24215
 1865872711014914813079786 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=12345670
 1865872711014914813079786 column=cf1:type, timestamp=1494988387789, value=2426

[14:57:33] INFO:     1865872711014914813079787 column=cf1:time, timestamp=1494988387789, value=123456634
 
[14:57:33] INFO:    1865872711014914813079787 column=cf1:type, timestamp=1494988387789, value=24945
 1865872711014914813079788 column=cf1:time, timestamp=1494988387789, value=123456554

[14:57:33] INFO:     1865872711014914813079788 column=cf1:type, timestamp=1494988387789, value=24428

[14:57:33] INFO:     186587271101491481307979 column=cf1:time, timestamp=1494988387789, value=123456105
 186587271101491481307979 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24162
 1865872711014914813079790 column=cf1:time, timestamp=1494988387789, value=123456641
 1865872711014914813079790 column=cf1:type, timestamp=1494988387789, value=24103

[14:57:33] INFO:     1865872711014914813079791 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=1234568
 1865872711014914813079791 column=cf1:type, timestamp=1494988387789, value=24403

[14:57:33] INFO:     1865872711014914813079792 column=cf1:time, timestamp=1494988387789, value=123456106

[14:57:33] INFO:     1865872711014914813079792 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24561

[14:57:33] INFO:     1865872711014914813079793 column=cf1:time, timestamp=1494988387789, value=123456435
 1865872711014914813079793 column=cf1:type, timestamp=1494988387789, value=24514
 1865872711014914813079794 column=cf1:time, timestamp=1494988387789, value=123456576
[14:57:33] INFO:    
 1865872711014914813079794 column=cf1:type, timestamp=1494988387789, value=24865

[14:57:33] INFO:     1865872711014914813079795 column=cf1:time, timestamp=1494988387789, value=123456726
 1865872711014914813079795 column=cf1:type, timestamp=1494988387789, value=24608
 1865872711014914813079796 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456473
 1865872711014914813079796 column=cf1:type, timestamp=1494988387789, value=24242

[14:57:33] INFO:     1865872711014914813079798 column=cf1:time, timestamp=1494988387789, value=123456172

[14:57:33] INFO:     1865872711014914813079798 column=cf1:type, timestamp=1494988387789, value=24226
 1865872711014914813079799 column=cf1:time, timestamp=1494988387789, value=123456830

[14:57:33] INFO:     1865872711014914813079799 column=cf1:type, timestamp=1494988387789, value=24722

[14:57:33] INFO:     18658727110149148130798 column=cf1:time, timestamp=1494988387789, value=123456779

[14:57:33] INFO:     18658727110149148130798 column=cf1:type, timestamp=1494988387789, value=24498
 1865872711014914813079800 column=cf1:time, timestamp=1494988387789, value=123456914

[14:57:33] INFO:     1865872711014914813079800 column=cf1:type, timestamp=1494988387789, value=24390

[14:57:33] INFO:     1865872711014914813079801 column=cf1:time, timestamp=1494988387789, value=123456900
 
[14:57:33] INFO:    1865872711014914813079801 column=cf1:type, timestamp=1494988387789, value=2473
 1865872711014914813079803 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456731
 1865872711014914813079803 column=cf1:type, timestamp=1494988387789, value=24103
 1865872711014914813079804 column=cf1:time, timestamp=1494988387789, value=123456815
[14:57:33] INFO:    
 1865872711014914813079804 column=cf1:type, timestamp=1494988387789, value=24250

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079805 column=cf1:time, timestamp=1494988387789, value=123456316

[14:57:33] INFO:     1865872711014914813079805 column=cf1:type, timestamp=1494988387789, value=24673
 1865872711014914813079806 column=cf1:time, timestamp=1494988387789, value=123456501
 1865872711014914813079806 column=cf1:type, timestamp=1494988387789, value=24263
 1865872711014914813079807 column=cf1:time, timestamp=1494988387789, value=123456229
 
[14:57:33] INFO:    1865872711014914813079807 column=cf1:type, timestamp=1494988387789, value=2455
 1865872711014914813079808 column=cf1:time, timestamp=1494988387789, value=123456472

[14:57:33] INFO:     1865872711014914813079808 column=cf1:type, timestamp=1494988387789, value=24238
 1865872711014914813079809 column=cf1:time, timestamp=1494988387789, value=123456790

[14:57:33] INFO:     1865872711014914813079809 column=cf1:type, timestamp=1494988387789, value=24604

[14:57:33] INFO:     1865872711014914813079811 column=cf1:time, timestamp=1494988387789, value=123456490
 1865872711014914813079811 column=cf1:type, timestamp=1494988387789, value=24441

[14:57:33] INFO:     1865872711014914813079814 column=cf1:time, timestamp=1494988387789, value=123456872

[14:57:33] INFO:     1865872711014914813079814 column=cf1:type, timestamp=1494988387789, value=24900
 
[14:57:33] INFO:    1865872711014914813079815 column=cf1:time, timestamp=1494988387789, value=123456936
 1865872711014914813079815 column=cf1:type, timestamp=1494988387789, value=24456
 1865872711014914813079817 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456958
 1865872711014914813079817 column=cf1:type, timestamp=1494988387789, value=2431

[14:57:33] INFO:     1865872711014914813079819 column=cf1:time, timestamp=1494988387789, value=123456625
 1865872711014914813079819 column=cf1:type, timestamp=1494988387789, value=24618

[14:57:33] INFO:     1865872711014914813079821 column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079821 column=cf1:type, timestamp=1494988387789, value=24129

[14:57:33] INFO:     1865872711014914813079822 column=cf1:time, timestamp=1494988387789, value=123456372

[14:57:33] INFO:     1865872711014914813079822 column=cf1:type, timestamp=1494988387789, value=24296
 1865872711014914813079823 column=cf1:time, timestamp=1494988387789, value=123456123
 
[14:57:33] INFO:    1865872711014914813079823 column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079824 column=cf1:time, timestamp=1494988387789, value=123456285
 
[14:57:33] INFO:    1865872711014914813079824 column=cf1:type, timestamp=1494988387789, value=2461
 1865872711014914813079825 column=cf1:time, timestamp=1494988387789, value=123456783

[14:57:33] INFO:     1865872711014914813079825 column=cf1:type, timestamp=1494988387789, value=24884
 1865872711014914813079827 column=cf1:time, timestamp=1494988387789, value=123456251
 1865872711014914813079827 column=cf1:type, timestamp=1494988387789, value=24909
 1865872711014914813079829 column=cf1:time, timestamp=1494988387789, value=123456608

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079829 column=cf1:type, timestamp=1494988387789, value=24148
 186587271101491481307983 column=cf1:time, timestamp=1494988387789, value=123456589
 186587271101491481307983 column=cf1:type, timestamp=1494988387789, value=24971
 1865872711014914813079831 column=cf1:time, timestamp=1494988387789, value=123456528

[14:57:33] INFO:     1865872711014914813079831 column=cf1:type, timestamp=1494988387789, value=24550

[14:57:33] INFO:     1865872711014914813079832 column=cf1:time, timestamp=1494988387789, value=123456360

[14:57:33] INFO:     1865872711014914813079832 column=cf1:type, timestamp=1494988387789, value=24646
 1865872711014914813079833 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079833 column=cf1:type, timestamp=1494988387789, value=24823

[14:57:33] INFO:     1865872711014914813079837 column=cf1:time, timestamp=1494988387789, value=123456596

[14:57:33] INFO:     1865872711014914813079837 column=cf1:type, timestamp=1494988387789, value=24971
 
[14:57:33] INFO:    1865872711014914813079838 column=cf1:time, timestamp=1494988387789, value=123456722
 1865872711014914813079838
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24571
 1865872711014914813079839 column=cf1:time, timestamp=1494988387789, value=123456335
 1865872711014914813079839 column=cf1:type, timestamp=1494988387789, value=24155
 1865872711014914813079840 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079840 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24846
 1865872711014914813079841 column=cf1:time, timestamp=1494988387789, value=123456414
 1865872711014914813079841 column=cf1:type, timestamp=1494988387789, value=24121

[14:57:33] INFO:     1865872711014914813079842 column=cf1:time, timestamp=1494988387789, value=123456328

[14:57:33] INFO:     1865872711014914813079842 column=cf1:type, timestamp=1494988387789, value=24559
 
[14:57:33] INFO:    1865872711014914813079843 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079843 column=cf1:type, timestamp=1494988387789, value=2432
[14:57:33] INFO:    
 1865872711014914813079844 column=cf1:time, timestamp=1494988387789, value=123456971
[14:57:33] INFO:    
 1865872711014914813079844 column=cf1:type, timestamp=1494988387789, value=24932
[14:57:33] INFO:    
 1865872711014914813079849 column=cf1:time, timestamp=1494988387789, value=123456955
 1865872711014914813079849 column=cf1:type, timestamp=1494988387789, value=24773

[14:57:33] INFO:     186587271101491481307985 column=cf1:time, timestamp=1494988387789, value=123456733
 186587271101491481307985 column=cf1:type, timestamp=1494988387789, value=24754

[14:57:33] INFO:     1865872711014914813079850 column=cf1:time, timestamp=1494988387789, value=123456738
 
[14:57:33] INFO:    1865872711014914813079850 column=cf1:type, timestamp=1494988387789, value=24781
 1865872711014914813079852 column=cf1:time, timestamp=1494988387789, value=123456618
 1865872711014914813079852 column=cf1:type, timestamp=1494988387789, value=24856
 1865872711014914813079853 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456888
 1865872711014914813079853 column=cf1:type, timestamp=1494988387789, value=24779
 1865872711014914813079854 column=cf1:time, timestamp=1494988387789, value=123456334
 1865872711014914813079854 column=cf1:type, timestamp=1494988387789, value=24500
 1865872711014914813079855 column=cf1:time, timestamp=1494988387789, value=123456774
 1865872711014914813079855 column=cf1:type, timestamp=1494988387789, value=24375
 1865872711014914813079856 column=cf1:time, timestamp=1494988387789, value=123456962
[14:57:33] INFO:    
 1865872711014914813079856 column=cf1:type, timestamp=1494988387789, value=24482
 1865872711014914813079857 column=cf1:time, timestamp=1494988387789, value=123456475
 1865872711014914813079857 column=cf1:type, timestamp=1494988387789, value=24352
 1865872711014914813079858 column=cf1:time, timestamp=1494988387789, value=123456609
 1865872711014914813079858 column=cf1:type, timestamp=1494988387789, value=24636
 1865872711014914813079860 column=cf1:time, timestamp=1494988387789, value=123456528

[14:57:33] INFO:     1865872711014914813079860 column=cf1:type, timestamp=1494988387789, value=24664
 1865872711014914813079861 column=cf1:time, timestamp=1494988387789, value=123456275
[14:57:33] INFO:    
 1865872711014914813079861 column=cf1:type, timestamp=1494988387789, value=24423
 
[14:57:33] INFO:    1865872711014914813079862 column=cf1:time, timestamp=1494988387789, value=123456154
 1865872711014914813079862 column=cf1:type, timestamp=1494988387789, value=24952
[14:57:33] INFO:    
 1865872711014914813079863 column=cf1:time, timestamp=1494988387789, value=123456780
 1865872711014914813079863 column=cf1:type, timestamp=1494988387789, value=2467
 1865872711014914813079865 column=cf1:time, timestamp=1494988387789, value=123456551

[14:57:33] INFO:     1865872711014914813079865 column=cf1:type, timestamp=1494988387789, value=24341
 186587271101491481307987 column=cf1:time, timestamp=1494988387789, value=123456123
 186587271101491481307987 column=cf1:type, timestamp=1494988387789, value=24841
 1865872711014914813079870 column=cf1:time, timestamp=1494988387789, value=123456564
 
[14:57:33] INFO:    1865872711014914813079870 column=cf1:type, timestamp=1494988387789, value=24425
 1865872711014914813079873 column=cf1:time, timestamp=1494988387789, value=123456258
 1865872711014914813079873 column=cf1:type, timestamp=1494988387789, value=2410
 1865872711014914813079874 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456695
 1865872711014914813079874 column=cf1:type, timestamp=1494988387789, value=24257
 1865872711014914813079875 column=cf1:time, timestamp=1494988387789, value=123456294
 1865872711014914813079875 column=cf1:type, timestamp=1494988387789, value=24272
 1865872711014914813079876 column=cf1:time, timestamp=1494988387789, value=123456478
 1865872711014914813079876 column=cf1:type, timestamp=1494988387789, value=24513
 1865872711014914813079878 column=cf1:time, timestamp=1494988387789, value=123456211
 1865872711014914813079878 column=cf1:type, timestamp=1494988387789, value=24953
 1865872711014914813079879 column=cf1:time, timestamp=1494988387789, value=123456961
 1865872711014914813079879 column=cf1:type, timestamp=1494988387789, value=24137
 1865872711014914813079880 column=cf1:time, timestamp=1494988387789, value=12345633
[14:57:33] INFO:    
 1865872711014914813079880 column=cf1:type, timestamp=1494988387789, value=24266
 1865872711014914813079881 column=cf1:time, timestamp=1494988387789, value=123456577
 1865872711014914813079881 column=cf1:type, timestamp=1494988387789, value=2480
 1865872711014914813079882 column=cf1:time, timestamp=1494988387789, value=12345698
 1865872711014914813079882 column=cf1:type, timestamp=1494988387789, value=24438
 1865872711014914813079883 column=cf1:time, timestamp=1494988387789, value=12345663
 1865872711014914813079883 column=cf1:type, timestamp=1494988387789, value=24236
 1865872711014914813079884 column=cf1:time, timestamp=1494988387789, value=123456384
 1865872711014914813079884 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079885 column=cf1:time, timestamp=1494988387789, value=123456198
 1865872711014914813079885 column=cf1:type, timestamp=1494988387789, value=24334
[14:57:33] INFO:    
 1865872711014914813079887 column=cf1:time, timestamp=1494988387789, value=123456659
 
[14:57:33] INFO:    1865872711014914813079887 column=cf1:type, timestamp=1494988387789, value=24878
 1865872711014914813079888 column=cf1:time, timestamp=1494988387789, value=123456218
 1865872711014914813079888 column=cf1:type, timestamp=1494988387789, value=24334
 1865872711014914813079889 column=cf1:time, timestamp=1494988387789, value=123456463
 1865872711014914813079889 column=cf1:type, timestamp=1494988387789, value=24893
 1865872711014914813079890 column=cf1:time, timestamp=1494988387789, value=123456942
 1865872711014914813079890 column=cf1:type, timestamp=1494988387789, value=24946
 
[14:57:33] INFO:    1865872711014914813079893 column=cf1:time, timestamp=1494988387789, value=123456656
 1865872711014914813079893 column=cf1:type, timestamp=1494988387789, value=2419
 1865872711014914813079894 column=cf1:time, timestamp=1494988387789, value=123456708
 1865872711014914813079894 column=cf1:type, timestamp=1494988387789, value=24752
 1865872711014914813079895 column=cf1:time, timestamp=1494988387789, value=123456769
 1865872711014914813079895 column=cf1:type, timestamp=1494988387789, value=24324
 1865872711014914813079896 column=cf1:time, timestamp=1494988387789, value=123456499
 1865872711014914813079896 column=cf1:type, timestamp=1494988387789, value=24896
[14:57:33] INFO:    
 1865872711014914813079897 column=cf1:time, timestamp=1494988387789, value=123456469
 1865872711014914813079897 column=cf1:type, timestamp=1494988387789, value=241
 1865872711014914813079898 column=cf1:time, timestamp=1494988387789, value=123456813
 1865872711014914813079898 column=cf1:type, timestamp=1494988387789, value=247
 1865872711014914813079899 column=cf1:time, timestamp=1494988387789, value=123456759
 1865872711014914813079899 column=cf1:type, timestamp=1494988387789, value=24371
 18658727110149148130799 column=cf1:time, timestamp=1494988387789, value=123456381
 18658727110149148130799 column=cf1:type, timestamp=1494988387789, value=24947
 186587271101491481307990 column=cf1:time, timestamp=1494988387789, value=123456469
 186587271101491481307990 column=cf1:type, timestamp=1494988387789, value=24776
 1865872711014914813079901 column=cf1:time, timestamp=1494988387789, value=123456602
 1865872711014914813079901 column=cf1:type, timestamp=1494988387789, value=24694
 1865872711014914813079903 column=cf1
[14:57:33] INFO:    :time, timestamp=1494988387789, value=12345649
 1865872711014914813079903 column=cf1:type, timestamp=1494988387789, value=2485
 1865872711014914813079904 column=cf1:time, timestamp=1494988387789, value=1234563
 1865872711014914813079904 column=cf1:type, timestamp=1494988387789, value=24556
 
[14:57:33] INFO:    1865872711014914813079905 column=cf1:time, timestamp=1494988387789, value=123456494
 1865872711014914813079905 column=cf1:type, timestamp=1494988387789, value=24603
 1865872711014914813079907 column=cf1:time, timestamp=1494988387789, value=123456563
 1865872711014914813079907 column=cf1:type, timestamp=1494988387789, value=24488

[14:57:33] INFO:     1865872711014914813079908 column=cf1:time, timestamp=1494988387789, value=12345669
 1865872711014914813079908 column=cf1:type, timestamp=1494988387789, value=24368
 1865872711014914813079909 column=cf1:time, timestamp=1494988387789, value=123456455
 1865872711014914813079909 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24443
 186587271101491481307991 column=cf1:time, timestamp=1494988387789, value=12345683
 186587271101491481307991 column=cf1:type, timestamp=1494988387789, value=24473
 1865872711014914813079910 column=cf1:time, timestamp=1494988387789, value=12345662
 
[14:57:33] INFO:    1865872711014914813079910 column=cf1:type, timestamp=1494988387789, value=24793
 1865872711014914813079913 column=cf1:time, timestamp=1494988387789, value=123456493
 1865872711014914813079913 column=cf1:type, timestamp=1494988387789, value=24415
 1865872711014914813079915 column=cf1:time, timestamp=1494988387789, value=123456845
 1865872711014914813079915 column=cf1:type, timestamp=1494988387789, value=24257

[14:57:33] INFO:     1865872711014914813079917 column=cf1:time, timestamp=1494988387789, value=123456153

[14:57:33] INFO:     1865872711014914813079917 column=cf1:type, timestamp=1494988387789, value=24703
 1865872711014914813079918 column=cf1:time, timestamp=1494988387789, value=12345627
 1865872711014914813079918 column=cf1:type, timestamp=1494988387789, value=24459
 1865872711014914813079919 column=cf1:time, timestamp=1494988387789, value=123456313

[14:57:33] INFO:     1865872711014914813079919 column=cf1:type, timestamp=1494988387789, value=24373
 1865872711014914813079920 column=cf1:time, timestamp=1494988387789, value=123456496
 1865872711014914813079920 column=cf1:type, timestamp=1494988387789, value=24545
 1865872711014914813079925 column=cf1:time, timestamp=1494988387789, value=123456186
 1865872711014914813079925 column=cf1:type, timestamp=1494988387789, value=24150
 1865872711014914813079927 column=cf1:time, timestamp=1494988387789, value=123456557

[14:57:33] INFO:     1865872711014914813079927 column=cf1:type, timestamp=1494988387789, value=24427
 1865872711014914813079928 column=cf1:time, timestamp=1494988387789, value=123456683
 1865872711014914813079928 column=cf1:type, timestamp=1494988387789, value=24543
 186587271101491481307993 column=cf1:time, timestamp=1494988387789, value=123456867
 186587271101491481307993 column=cf1:type, timestamp=1494988387789, value=24332
 1865872711014914813079930 column=cf1:time, timestamp=1494988387789, value=123456677

[14:57:33] INFO:     1865872711014914813079930 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24775

[14:57:33] INFO:     1865872711014914813079932 column=cf1:time, timestamp=1494988387789, value=123456222
 1865872711014914813079932 column=cf1:type, timestamp=1494988387789, value=24711
 1865872711014914813079935 column=cf1:time, timestamp=1494988387789, value=123456692
 1865872711014914813079935 column=cf1:type, timestamp=1494988387789, value=2471
 1865872711014914813079936 column=cf1:time, timestamp=1494988387789, value=123456248

[14:57:33] INFO:     1865872711014914813079936 column=cf1:type, timestamp=1494988387789, value=24530

[14:57:33] INFO:     186587271101491481307994 column=cf1:time, timestamp=1494988387789, value=123456235
 
[14:57:33] INFO:    186587271101491481307994 column=cf1:type, timestamp=1494988387789, value=245
 1865872711014914813079940 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456131
 1865872711014914813079940 column=cf1:type, timestamp=1494988387789, value=24983

[14:57:33] INFO:     1865872711014914813079943 column=cf1:time, timestamp=1494988387789, value=123456854

[14:57:33] INFO:     1865872711014914813079943 column=cf1:type, timestamp=1494988387789, value=24553
 1865872711014914813079945 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079945 column=cf1:type, timestamp=1494988387789, value=24925

[14:57:33] INFO:     1865872711014914813079946 column=cf1:time, timestamp=1494988387789, value=123456188
 1865872711014914813079946 column=cf1:type, timestamp=1494988387789, value=24307

[14:57:33] INFO:     1865872711014914813079947 column=cf1:time, timestamp=1494988387789, value=12345697

[14:57:33] INFO:     1865872711014914813079947 column=cf1:type, timestamp=1494988387789, value=24272

[14:57:33] INFO:     
[14:57:33] INFO:    1865872711014914813079948 column=cf1:time, timestamp=1494988387789, value=123456913
 1865872711014914813079948 column=cf1:type, timestamp=1494988387789, value=24258

[14:57:33] INFO:     1865872711014914813079949 column=cf1:time, timestamp=1494988387789, value=123456686
 
[14:57:33] INFO:    1865872711014914813079949 column=cf1:type, timestamp=1494988387789, value=24304
 186587271101491481307995 column=cf1:time, timestamp=1494988387789, value=123456953

[14:57:33] INFO:     186587271101491481307995 column=cf1:type, timestamp=1494988387789, value=24184
 1865872711014914813079950 column=cf1:time, timestamp=1494988387789, value=123456428
[14:57:33] INFO:    
 1865872711014914813079950 column=cf1:type, timestamp=1494988387789, value=246
 
[14:57:33] INFO:    1865872711014914813079951 column=cf1:time, timestamp=1494988387789, value=123456624
 1865872711014914813079951 column=cf1:type, timestamp=1494988387789, value=24478

[14:57:33] INFO:     1865872711014914813079952 column=cf1:time, timestamp=1494988387789, value=123456282
 1865872711014914813079952 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2496
 1865872711014914813079953 column=cf1:time, timestamp=1494988387789, value=123456334

[14:57:33] INFO:     1865872711014914813079953 column=cf1:type, timestamp=1494988387789, value=2475

[14:57:33] INFO:     1865872711014914813079954 column=cf1:time, timestamp=1494988387789, value=123456959
 
[14:57:33] INFO:    1865872711014914813079954 column=cf1:type, timestamp=1494988387789, value=24785
 1865872711014914813079956 column=cf1:time, timestamp=1494988387789, value=123456194
 
[14:57:33] INFO:    1865872711014914813079956 column=cf1:type, timestamp=1494988387789, value=24195
 1865872711014914813079957 column=cf1:time, timestamp=1494988387789, value=123456169

[14:57:33] INFO:     1865872711014914813079957 column=cf1:type, timestamp=1494988387789, value=24312

[14:57:33] INFO:     1865872711014914813079959 column=cf1:time, timestamp=1494988387789, value=123456346

[14:57:33] INFO:     1865872711014914813079959 column=cf1:type, timestamp=1494988387789, value=24493
 186587271101491481307996 column=cf1:time, timestamp=1494988387789, value=123456686
[14:57:33] INFO:    
 186587271101491481307996 column=cf1:type, timestamp=1494988387789, value=24556

[14:57:33] INFO:     1865872711014914813079962 column=cf1:time, timestamp=1494988387789, value=123456997

[14:57:33] INFO:     1865872711014914813079962 column=cf1:type, timestamp=1494988387789, value=24480

[14:57:33] INFO:     1865872711014914813079963 column=cf1:time, timestamp=1494988387789, value=12345661
 1865872711014914813079963 column=cf1:type, timestamp=1494988387789, value=24641
 1865872711014914813079964 column=cf1:time, timestamp=1494988387789, value=123456778

[14:57:33] INFO:     1865872711014914813079964 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24188
 1865872711014914813079965 column=cf1:time, timestamp=1494988387789, value=123456785
 1865872711014914813079965 column=cf1:type, timestamp=1494988387789, value=24752

[14:57:33] INFO:     1865872711014914813079967 column=cf1:time, timestamp=1494988387789, value=123456220

[14:57:33] INFO:     1865872711014914813079967 column=cf1:type, timestamp=1494988387789, value=24168

[14:57:33] INFO:     186587271101491481307997 column=cf1:time, timestamp=1494988387789, value=123456785
 
[14:57:33] INFO:    186587271101491481307997 column=cf1:type, timestamp=1494988387789, value=24188
 1865872711014914813079970 column=cf1:time, timestamp=1494988387789, value=123456880

[14:57:33] INFO:     1865872711014914813079970 column=cf1:type, timestamp=1494988387789, value=24355

[14:57:33] INFO:     1865872711014914813079971 column=cf1:time, timestamp=1494988387789, value=123456894
 1865872711014914813079971 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=2428
 1865872711014914813079972 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456165
 1865872711014914813079972 column=cf1:type, timestamp=1494988387789, value=24536
 
[14:57:33] INFO:    1865872711014914813079974 column=cf1:time, timestamp=1494988387789, value=123456193
 1865872711014914813079974 column=cf1:type, timestamp=1494988387789, value=24184

[14:57:33] INFO:     1865872711014914813079975 column=cf1:time, timestamp=1494988387789, value=12345679

[14:57:33] INFO:     1865872711014914813079975 column=cf1:type, timestamp=1494988387789, value=24240

[14:57:33] INFO:     1865872711014914813079976 column=cf1:time, timestamp=1494988387789, value=123456961
[14:57:33] INFO:    
 
[14:57:33] INFO:    1865872711014914813079976 column=cf1:type, timestamp=1494988387789, value=24830
 1865872711014914813079977 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456290
 1865872711014914813079977 column=cf1:type, timestamp=1494988387789, value=24439

[14:57:33] INFO:     1865872711014914813079978 column=cf1:time, timestamp=1494988387789, value=123456581
 1865872711014914813079978 column=cf1:type, timestamp=1494988387789, value=24149
 1865872711014914813079979 column=cf1:time, timestamp=1494988387789, value=123456550

[14:57:33] INFO:     1865872711014914813079979 column=cf1:type, timestamp=1494988387789, value=24418

[14:57:33] INFO:     186587271101491481307998 column=cf1:time, timestamp=1494988387789, value=123456265
 
[14:57:33] INFO:    186587271101491481307998 column=cf1:type, timestamp=1494988387789, value=24589
 1865872711014914813079982 column=cf1:time, timestamp=1494988387789, value=123456147
 1865872711014914813079982 
[14:57:33] INFO:    column=cf1:type, timestamp=1494988387789, value=24457

[14:57:33] INFO:     1865872711014914813079983 column=cf1:time, timestamp=1494988387789, value=123456831
 1865872711014914813079983 column=cf1:type, timestamp=1494988387789, value=24451
 
[14:57:33] INFO:    1865872711014914813079984 column=cf1:time, timestamp=1494988387789, value=123456221
 1865872711014914813079984 column=cf1:type, timestamp=1494988387789, value=24222
[14:57:33] INFO:    
 1865872711014914813079985 column=cf1:time, timestamp=1494988387789, value=123456375

[14:57:33] INFO:     1865872711014914813079985 column=cf1:type, timestamp=1494988387789, value=24774
 
[14:57:33] INFO:    1865872711014914813079986 column=cf1:time, timestamp=1494988387789, value=123456744
 1865872711014914813079986 column=cf1:type, timestamp=1494988387789, value=24196
 1865872711014914813079987 column=cf1:time, timestamp=1494988387789, value=123456893

[14:57:33] INFO:     1865872711014914813079987 column=cf1:type, timestamp=1494988387789, value=24961

[14:57:33] INFO:     1865872711014914813079988 column=cf1:time, timestamp=1494988387789, value=123456402

[14:57:33] INFO:     1865872711014914813079988 column=cf1:type, timestamp=1494988387789, value=24583
 
[14:57:33] INFO:    1865872711014914813079990 column=cf1:time, timestamp=1494988387789, value=123456586
 1865872711014914813079990 column=cf1:type, timestamp=1494988387789, value=24454

[14:57:33] INFO:     1865872711014914813079991 column=cf1:time, timestamp=1494988387789, value=12345670

[14:57:33] INFO:     1865872711014914813079991 column=cf1:type, timestamp=1494988387789, value=24342
 1865872711014914813079992 
[14:57:33] INFO:    column=cf1:time, timestamp=1494988387789, value=123456355
 1865872711014914813079992 column=cf1:type, timestamp=1494988387789, value=24948

[14:57:33] INFO:     1865872711014914813079994 column=cf1:time, timestamp=1494988387789, value=123456693

[14:57:33] INFO:     1865872711014914813079994 column=cf1:type, timestamp=1494988387789, value=24932

[14:57:33] INFO:     1865872711014914813079995 column=cf1:time, timestamp=1494988387789, value=123456183

[14:57:33] INFO:     1865872711014914813079995
[14:57:33] INFO:     column=cf1:type, timestamp=1494988387789, value=24958
 
[14:57:33] INFO:    1865872711014914813079996 column=cf1:time, timestamp=1494988387789, value=12345624
 1865872711014914813079996 column=cf1:type, timestamp=1494988387789, value=2415

[14:57:33] INFO:     1865872711014914813079997 column=cf1:time, timestamp=1494988387789, value=123456215
 1865872711014914813079997 column=cf1:type, timestamp=1494988387789, value=24594

[14:57:33] INFO:     1865872711014914813079999 column=cf1:time, timestamp=1494988387789, value=123456303

[14:57:33] INFO:     1865872711014914813079999 column=cf1:type, timestamp=1494988387789, value=2464

[14:57:33] INFO:     2 column=cf1:q, timestamp=1494919193176, value=qq

[14:57:33] INFO:     t column=cf1:q, timestamp=1494917634220, value=tv

[14:57:33] INFO:    645 row(s) in 0.7600 seconds
[14:57:33] INFO:    


[14:57:34] INFO:    Connection channel closed
[14:57:34] INFO:    Check if exec success or not ... 
[14:57:34] INFO:    Execute successfully for command: echo "scan 'tt' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[14:57:34] INFO:    Now wait 5 seconds to begin next task ...
[14:57:39] INFO:    Connection channel disconnect
[14:57:39] INFO:    SSH connection shutdown

=============== [2017/05/17 14:58:04, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[14:58:04] INFO:    SSHExec initializing ...
[14:58:04] INFO:    Session initialized and associated with user credential inteast.com
[14:58:04] INFO:    SSHExec initialized successfully
[14:58:04] INFO:    SSHExec trying to connect root@10.1.70.200
[14:58:05] INFO:    SSH connection established
[14:58:05] INFO:    Command is echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[14:58:05] INFO:    Connection channel established succesfully
[14:58:05] INFO:    Start to run command
[14:58:11] INFO:    COLUMN
[14:58:11] INFO:      CELL

[14:58:11] INFO:     
[14:58:11] INFO:    cf1:q timestamp=1494919193176, value=qq

[14:58:11] INFO:    1 row(s) in 0.2410 seconds
[14:58:11] INFO:    

[14:58:11] INFO:    

[14:58:11] INFO:    Connection channel closed
[14:58:11] INFO:    Check if exec success or not ... 
[14:58:11] INFO:    Execute successfully for command: echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[14:58:11] INFO:    Now wait 5 seconds to begin next task ...
[14:58:16] INFO:    Connection channel disconnect
[14:58:16] INFO:    SSH connection shutdown

=============== [2017/05/17 15:00:47, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:00:47] INFO:    SSHExec initializing ...
[15:00:48] INFO:    Session initialized and associated with user credential inteast.com
[15:00:48] INFO:    SSHExec initialized successfully
[15:00:48] INFO:    SSHExec trying to connect root@10.1.70.200
[15:00:49] INFO:    SSH connection established
[15:00:50] INFO:    Command is echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:00:50] INFO:    Connection channel established succesfully
[15:00:50] INFO:    Start to run command
[15:00:56] INFO:    COLUMN
[15:00:56] INFO:      CELL

[15:00:56] INFO:     
[15:00:56] INFO:    cf1:q timestamp=1494919193176, value=qq

[15:00:56] INFO:    1 row(s) in 0.2350 seconds


[15:00:57] INFO:    Connection channel closed
[15:00:57] INFO:    Check if exec success or not ... 
[15:00:57] INFO:    Execute successfully for command: echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:00:57] INFO:    Now wait 5 seconds to begin next task ...
[15:01:02] INFO:    Connection channel disconnect
[15:01:23] INFO:    SSH connection shutdown

=============== [2017/05/17 15:03:29, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:03:29] INFO:    SSHExec initializing ...
[15:03:29] INFO:    Session initialized and associated with user credential inteast.com
[15:03:29] INFO:    SSHExec initialized successfully
[15:03:29] INFO:    SSHExec trying to connect root@10.1.70.200
[15:03:29] INFO:    SSH connection established
[15:03:29] INFO:    Command is echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:03:29] INFO:    Connection channel established succesfully
[15:03:29] INFO:    Start to run command
[15:03:35] INFO:    COLUMN
[15:03:35] INFO:      CELL

[15:03:36] INFO:     
[15:03:36] INFO:    cf1:q timestamp=1494919193176, value=qq

[15:03:36] INFO:    1 row(s) in 0.2440 seconds
[15:03:36] INFO:    


[15:03:36] INFO:    Connection channel closed
[15:03:36] INFO:    Check if exec success or not ... 
[15:03:36] INFO:    Execute successfully for command: echo "get 'tt','2' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:03:36] INFO:    Now wait 5 seconds to begin next task ...
[15:03:41] INFO:    Connection channel disconnect
[15:03:41] INFO:    SSH connection shutdown

=============== [2017/05/17 15:04:40, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:04:40] INFO:    SSHExec initializing ...
[15:04:40] INFO:    Session initialized and associated with user credential inteast.com
[15:04:40] INFO:    SSHExec initialized successfully
[15:04:40] INFO:    SSHExec trying to connect root@10.1.70.200
[15:04:41] INFO:    SSH connection established
[15:04:41] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:04:41] INFO:    Connection channel established succesfully
[15:04:41] INFO:    Start to run command
[15:04:47] INFO:    COLUMN
[15:04:47] INFO:      CELL

[15:04:47] INFO:     
[15:04:47] INFO:    cf1:time timestamp=1494988387789, value=123456303

[15:04:47] INFO:     cf1:type
[15:04:47] INFO:     timestamp=1494988387789, value=2464

[15:04:47] INFO:    2 row(s) in 0.2300 seconds
[15:04:47] INFO:    

[15:04:47] INFO:    

[15:04:48] INFO:    Connection channel closed
[15:04:48] INFO:    Check if exec success or not ... 
[15:04:48] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:04:48] INFO:    Now wait 5 seconds to begin next task ...
[15:04:53] INFO:    Connection channel disconnect
[15:04:53] INFO:    SSH connection shutdown

=============== [2017/05/17 15:06:38, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:06:38] INFO:    SSHExec initializing ...
[15:06:38] INFO:    Session initialized and associated with user credential inteast.com
[15:06:38] INFO:    SSHExec initialized successfully
[15:06:38] INFO:    SSHExec trying to connect root@10.1.70.200
[15:06:39] INFO:    SSH connection established
[15:06:39] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:06:39] INFO:    Connection channel established succesfully
[15:06:39] INFO:    Start to run command
[15:06:45] INFO:    COLUMN
[15:06:45] INFO:      CELL

[15:06:45] INFO:     
[15:06:45] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[15:06:45] INFO:    cf1:type timestamp=1494988387789, value=2464
[15:06:45] INFO:    

[15:06:45] INFO:    2 row(s) in 0.2200 seconds
[15:06:45] INFO:    


[15:06:46] INFO:    Connection channel closed
[15:06:46] INFO:    Check if exec success or not ... 
[15:06:46] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:06:46] INFO:    Now wait 5 seconds to begin next task ...
[15:06:51] INFO:    Connection channel disconnect
[15:06:51] INFO:    SSH connection shutdown

=============== [2017/05/17 15:10:02, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:10:02] INFO:    SSHExec initializing ...
[15:10:02] INFO:    Session initialized and associated with user credential inteast.com
[15:10:02] INFO:    SSHExec initialized successfully
[15:10:02] INFO:    SSHExec trying to connect root@10.1.70.200
[15:10:03] INFO:    SSH connection established
[15:10:03] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:10:03] INFO:    Connection channel established succesfully
[15:10:03] INFO:    Start to run command
[15:10:09] INFO:    COLUMN
[15:10:09] INFO:      CELL

[15:10:09] INFO:     
[15:10:09] INFO:    cf1:time timestamp=1494988387789, value=123456303

[15:10:09] INFO:     cf1:type
[15:10:09] INFO:     timestamp=1494988387789, value=2464

[15:10:09] INFO:    2 row(s) in 0.2230 seconds
[15:10:09] INFO:    


[15:10:10] INFO:    Connection channel closed
[15:10:10] INFO:    Check if exec success or not ... 
[15:10:10] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:10:10] INFO:    Now wait 5 seconds to begin next task ...
[15:10:15] INFO:    Connection channel disconnect
[15:10:15] INFO:    SSH connection shutdown

=============== [2017/05/17 15:18:49, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:18:49] INFO:    SSHExec initializing ...
[15:18:49] INFO:    Session initialized and associated with user credential inteast.com
[15:18:49] INFO:    SSHExec initialized successfully
[15:18:49] INFO:    SSHExec trying to connect root@10.1.70.200
[15:18:49] INFO:    SSH connection established
[15:18:49] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:18:49] INFO:    Connection channel established succesfully
[15:18:49] INFO:    Start to run command
[15:18:55] INFO:    COLUMN
[15:18:55] INFO:      CELL

[15:18:55] INFO:     
[15:18:55] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[15:18:55] INFO:    cf1:type timestamp=1494988387789, value=2464

[15:18:55] INFO:    2 row(s) in 0.2240 seconds
[15:18:55] INFO:    


[15:18:56] INFO:    Connection channel closed
[15:18:56] INFO:    Check if exec success or not ... 
[15:18:56] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:18:56] INFO:    Now wait 5 seconds to begin next task ...
[15:19:01] INFO:    Connection channel disconnect
[15:19:01] INFO:    SSH connection shutdown

=============== [2017/05/17 15:19:23, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:19:23] INFO:    SSHExec initializing ...
[15:19:23] INFO:    Session initialized and associated with user credential inteast.com
[15:19:23] INFO:    SSHExec initialized successfully
[15:19:23] INFO:    SSHExec trying to connect root@10.1.70.200
[15:19:24] INFO:    SSH connection established
[15:19:24] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:19:24] INFO:    Connection channel established succesfully
[15:19:24] INFO:    Start to run command
[15:19:30] INFO:    COLUMN
[15:19:30] INFO:      CELL

[15:19:31] INFO:     
[15:19:31] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[15:19:31] INFO:    cf1:type
[15:19:31] INFO:     
[15:19:31] INFO:    timestamp=1494988387789, value=2464
2 row(s) in 0.6800 seconds


[15:19:31] INFO:    Connection channel closed
[15:19:31] INFO:    Check if exec success or not ... 
[15:19:31] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:19:31] INFO:    Now wait 5 seconds to begin next task ...
[15:19:36] INFO:    Connection channel disconnect
[15:19:36] INFO:    SSH connection shutdown

=============== [2017/05/17 15:20:33, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:20:33] INFO:    SSHExec initializing ...
[15:20:33] INFO:    Session initialized and associated with user credential inteast.com
[15:20:33] INFO:    SSHExec initialized successfully
[15:20:33] INFO:    SSHExec trying to connect root@10.1.70.200
[15:20:34] INFO:    SSH connection established
[15:20:34] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:20:34] INFO:    Connection channel established succesfully
[15:20:34] INFO:    Start to run command
[15:20:40] INFO:    COLUMN
[15:20:40] INFO:      CELL

[15:20:40] INFO:     
[15:20:40] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[15:20:40] INFO:    cf1:type 
[15:20:40] INFO:    timestamp=1494988387789, value=2464

[15:20:40] INFO:    2 row(s) in 0.2480 seconds
[15:20:40] INFO:    


[15:20:40] INFO:    Connection channel closed
[15:20:40] INFO:    Check if exec success or not ... 
[15:20:40] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:20:40] INFO:    Now wait 5 seconds to begin next task ...
[15:20:45] INFO:    Connection channel disconnect
[15:20:45] INFO:    SSH connection shutdown

=============== [2017/05/17 15:23:07, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:23:07] INFO:    SSHExec initializing ...
[15:23:07] INFO:    Session initialized and associated with user credential inteast.com
[15:23:07] INFO:    SSHExec initialized successfully
[15:23:07] INFO:    SSHExec trying to connect root@10.1.70.200
[15:23:07] INFO:    SSH connection established
[15:23:07] INFO:    Command is echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:23:07] INFO:    Connection channel established succesfully
[15:23:07] INFO:    Start to run command
[15:23:13] INFO:    COLUMN
[15:23:13] INFO:      CELL

[15:23:14] INFO:     
[15:23:14] INFO:    cf1:time timestamp=1494988387789, value=123456303
 cf1:type
[15:23:14] INFO:     timestamp=1494988387789, value=2464

[15:23:14] INFO:    2 row(s) in 0.2450 seconds
[15:23:14] INFO:    


[15:23:14] INFO:    Connection channel closed
[15:23:14] INFO:    Check if exec success or not ... 
[15:23:14] INFO:    Execute successfully for command: echo "get 'tt','1865872711014914813079999' 
 exit "> /home/lp/hbase.shell; hbase shell /home/lp/hbase.shell
[15:23:14] INFO:    Now wait 5 seconds to begin next task ...
[15:23:19] INFO:    Connection channel disconnect
[15:23:19] INFO:    SSH connection shutdown

=============== [2017/05/17 15:58:12, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[15:58:12] INFO:    SSHExec initializing ...
[15:58:12] INFO:    Session initialized and associated with user credential inteast.com
[15:58:12] INFO:    SSHExec initialized successfully
[15:58:12] INFO:    SSHExec trying to connect root@10.1.70.200
[15:58:13] INFO:    SSH connection established
[15:58:13] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[15:58:13] INFO:    Connection channel established succesfully
[15:58:13] INFO:    Start to run command
[15:58:19] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[15:58:19] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[15:58:19] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016


[15:58:19] INFO:     get 'tt','1865872711014914813079999' 

[15:58:19] INFO:    COLUMN
[15:58:19] INFO:      CELL

[15:58:19] INFO:     
[15:58:19] INFO:    cf1:time timestamp=1494988387789, value=123456303
 cf1:type timestamp=1494988387789, value=2464

[15:58:19] INFO:    2 row(s) in 0.2480 seconds
[15:58:19] INFO:    


[15:58:19] INFO:     EOF

[15:58:19] INFO:    NameError
[15:58:19] INFO:    : 
[15:58:19] INFO:    uninitialized constant EOF

[15:58:19] INFO:    

[15:58:20] INFO:    Connection channel closed
[15:58:20] INFO:    Check if exec success or not ... 
[15:58:20] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[15:58:20] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 15:58:32 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[15:58:20] INFO:    Now wait 5 seconds to begin next task ...
[15:58:25] INFO:    Connection channel disconnect
[15:58:25] INFO:    SSH connection shutdown

=============== [2017/05/17 16:44:42, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:44:42] INFO:    SSHExec initializing ...
[16:44:42] INFO:    Session initialized and associated with user credential inteast.com
[16:44:42] INFO:    SSHExec initialized successfully
[16:44:42] INFO:    SSHExec trying to connect root@10.1.70.200
[16:44:42] INFO:    SSH connection established
[16:44:42] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:44:42] INFO:    Connection channel established succesfully
[16:44:42] INFO:    Start to run command
[16:44:49] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:44:49] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:44:49] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016


[16:44:49] INFO:     get 'tt','1865872711014914813079999' 

[16:44:49] INFO:    COLUMN
[16:44:49] INFO:      CELL

[16:44:49] INFO:     
[16:44:49] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[16:44:49] INFO:    cf1:type timestamp=1494988387789, value=2464
[16:44:49] INFO:    

[16:44:49] INFO:    2 row(s) in 0.2230 seconds
[16:44:49] INFO:    


[16:44:49] INFO:     EOF

[16:44:49] INFO:    NameError
[16:44:49] INFO:    : uninitialized constant EOF

[16:44:49] INFO:    

[16:44:49] INFO:    Connection channel closed
[16:44:49] INFO:    Check if exec success or not ... 
[16:44:49] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:44:49] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:45:02 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:44:49] INFO:    Now wait 5 seconds to begin next task ...
[16:44:55] INFO:    Connection channel disconnect
[16:44:55] INFO:    SSH connection shutdown
[16:44:55] INFO:    Session initialized and associated with user credential inteast.com
[16:44:55] INFO:    SSHExec initialized successfully
[16:44:55] INFO:    SSHExec trying to connect root@10.1.70.200
[16:44:55] INFO:    SSH connection established
[16:44:55] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:44:55] INFO:    Connection channel established succesfully
[16:44:55] INFO:    Start to run command
[16:45:00] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.

[16:45:00] INFO:    Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:45:00] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016

[16:45:00] INFO:    

[16:45:00] INFO:     get 'tt','2' 

[16:45:01] INFO:    COLUMN
[16:45:01] INFO:      CELL

[16:45:01] INFO:     
[16:45:01] INFO:    cf1: timestamp=1495010153457, value=vq

[16:45:01] INFO:     cf1:e 
[16:45:01] INFO:    timestamp=1495010555159, value=vq

[16:45:01] INFO:     
[16:45:01] INFO:    cf1:ed timestamp=1495010579629, value=vq

[16:45:01] INFO:     cf1:q 
[16:45:01] INFO:    timestamp=1494919193176, value=qq

[16:45:01] INFO:    4 row(s) in 0.2410 seconds
[16:45:01] INFO:    


[16:45:01] INFO:     EOF

[16:45:01] INFO:    NameError
[16:45:01] INFO:    : uninitialized constant EOF

[16:45:01] INFO:    

[16:45:01] INFO:    Connection channel closed
[16:45:01] INFO:    Check if exec success or not ... 
[16:45:01] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:45:01] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:45:14 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:45:01] INFO:    Now wait 5 seconds to begin next task ...
[16:45:06] INFO:    Connection channel disconnect
[16:45:06] INFO:    SSH connection shutdown

=============== [2017/05/17 16:45:53, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:45:53] INFO:    SSHExec initializing ...
[16:45:53] INFO:    Session initialized and associated with user credential inteast.com
[16:45:53] INFO:    SSHExec initialized successfully
[16:45:53] INFO:    SSHExec trying to connect root@10.1.70.200
[16:45:54] INFO:    SSH connection established
[16:45:54] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:45:54] INFO:    Connection channel established succesfully
[16:45:54] INFO:    Start to run command
[16:46:00] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:46:00] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:46:00] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016


[16:46:00] INFO:     get 'tt','1865872711014914813079999' 

[16:46:01] INFO:    COLUMN
[16:46:01] INFO:      CELL

[16:46:01] INFO:     
[16:46:01] INFO:    cf1:time timestamp=1494988387789, value=123456303

[16:46:01] INFO:     cf1:type timestamp=1494988387789, value=2464

[16:46:01] INFO:    2 row(s) in 0.2220 seconds

[16:46:01] INFO:    

[16:46:01] INFO:     EOF

[16:46:01] INFO:    NameError: uninitialized constant EOF

[16:46:01] INFO:    

[16:46:01] INFO:    Connection channel closed
[16:46:01] INFO:    Check if exec success or not ... 
[16:46:01] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:46:01] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:46:13 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:46:01] INFO:    Now wait 5 seconds to begin next task ...
[16:46:06] INFO:    Connection channel disconnect
[16:46:06] INFO:    SSH connection shutdown
[16:46:06] INFO:    Session initialized and associated with user credential inteast.com
[16:46:06] INFO:    SSHExec initialized successfully
[16:46:06] INFO:    SSHExec trying to connect root@10.1.70.200
[16:46:06] INFO:    SSH connection established
[16:46:06] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:46:06] INFO:    Connection channel established succesfully
[16:46:06] INFO:    Start to run command
[16:46:12] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:46:12] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:46:12] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:46:12] INFO:    


[16:46:12] INFO:     get 'tt','2' 

[16:46:12] INFO:    COLUMN
[16:46:12] INFO:      CELL

[16:46:12] INFO:     
[16:46:12] INFO:    cf1: timestamp=1495010153457, value=vq

[16:46:12] INFO:     cf1:e timestamp=1495010555159, value=vq
[16:46:12] INFO:    

[16:46:12] INFO:     
[16:46:12] INFO:    cf1:ed timestamp=1495010579629, value=vq

[16:46:12] INFO:     
[16:46:12] INFO:    cf1:q timestamp=1494919193176, value=qq

[16:46:12] INFO:    4 row(s) in 0.2290 seconds
[16:46:12] INFO:    


[16:46:12] INFO:     EOF

[16:46:12] INFO:    NameError
[16:46:12] INFO:    : uninitialized constant EOF

[16:46:12] INFO:    

[16:46:13] INFO:    Connection channel closed
[16:46:13] INFO:    Check if exec success or not ... 
[16:46:13] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:46:13] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:46:25 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:46:13] INFO:    Now wait 5 seconds to begin next task ...
[16:46:18] INFO:    Connection channel disconnect
[16:46:18] INFO:    SSH connection shutdown

=============== [2017/05/17 16:46:24, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:46:24] INFO:    SSHExec initializing ...
[16:46:24] INFO:    Session initialized and associated with user credential inteast.com
[16:46:24] INFO:    SSHExec initialized successfully
[16:46:24] INFO:    SSHExec trying to connect root@10.1.70.200
[16:46:24] INFO:    SSH connection established
[16:46:24] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:46:24] INFO:    Connection channel established succesfully
[16:46:24] INFO:    Start to run command
[16:46:30] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:46:30] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:46:30] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:46:30] INFO:    


[16:46:30] INFO:     get 'tt','1865872711014914813079999' 

[16:46:31] INFO:    COLUMN
[16:46:31] INFO:      CELL

[16:46:31] INFO:     
[16:46:31] INFO:    cf1:time timestamp=1494988387789, value=123456303

[16:46:31] INFO:     cf1:type 
[16:46:31] INFO:    timestamp=1494988387789, value=2464

[16:46:31] INFO:    2 row(s) in 0.2360 seconds
[16:46:31] INFO:    

[16:46:31] INFO:    

[16:46:31] INFO:     EOF

[16:46:31] INFO:    NameError
[16:46:31] INFO:    : uninitialized constant EOF
[16:46:31] INFO:    

[16:46:31] INFO:    

[16:46:31] INFO:    Connection channel closed
[16:46:31] INFO:    Check if exec success or not ... 
[16:46:31] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:46:31] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:46:43 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:46:31] INFO:    Now wait 5 seconds to begin next task ...
[16:46:36] INFO:    Connection channel disconnect
[16:46:36] INFO:    SSH connection shutdown
[16:46:36] INFO:    Session initialized and associated with user credential inteast.com
[16:46:36] INFO:    SSHExec initialized successfully
[16:46:36] INFO:    SSHExec trying to connect root@10.1.70.200
[16:46:36] INFO:    SSH connection established
[16:46:36] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:46:36] INFO:    Connection channel established succesfully
[16:46:36] INFO:    Start to run command
[16:46:42] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:46:42] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016


[16:46:42] INFO:     get 'tt','2' 

[16:46:42] INFO:    COLUMN
[16:46:42] INFO:      CELL

[16:46:43] INFO:     
[16:46:43] INFO:    cf1: timestamp=1495010153457, value=vq
 cf1:e timestamp=1495010555159, value=vq
[16:46:43] INFO:    

[16:46:43] INFO:     
[16:46:43] INFO:    cf1:ed timestamp=1495010579629, value=vq
[16:46:43] INFO:    
 
[16:46:43] INFO:    cf1:q timestamp=1494919193176, value=qq
[16:46:43] INFO:    

[16:46:43] INFO:    4 row(s) in 0.2410 seconds


[16:46:43] INFO:     EOF

[16:46:43] INFO:    NameError
[16:46:43] INFO:    : uninitialized constant EOF

[16:46:43] INFO:    

[16:46:43] INFO:    Connection channel closed
[16:46:43] INFO:    Check if exec success or not ... 
[16:46:43] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:46:43] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:46:54 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:46:43] INFO:    Now wait 5 seconds to begin next task ...
[16:46:48] INFO:    Connection channel disconnect
[16:46:48] INFO:    SSH connection shutdown

=============== [2017/05/17 16:47:35, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:47:35] INFO:    SSHExec initializing ...
[16:47:35] INFO:    Session initialized and associated with user credential inteast.com
[16:47:35] INFO:    SSHExec initialized successfully
[16:47:35] INFO:    SSHExec trying to connect root@10.1.70.200
[16:47:35] INFO:    SSH connection established
[16:47:35] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:47:36] INFO:    Connection channel established succesfully
[16:47:36] INFO:    Start to run command
[16:47:41] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:47:41] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:47:41] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:47:41] INFO:    


[16:47:41] INFO:     get 'tt','1865872711014914813079999' 

[16:47:42] INFO:    COLUMN
[16:47:42] INFO:      CELL

[16:47:42] INFO:     
[16:47:42] INFO:    cf1:time timestamp=1494988387789, value=123456303

[16:47:42] INFO:     
[16:47:42] INFO:    cf1:type timestamp=1494988387789, value=2464
[16:47:42] INFO:    

[16:47:42] INFO:    2 row(s) in 0.2240 seconds
[16:47:42] INFO:    


[16:47:42] INFO:     EOF

[16:47:42] INFO:    NameError
[16:47:42] INFO:    : uninitialized constant EOF
[16:47:42] INFO:    

[16:47:42] INFO:    

[16:47:42] INFO:    Connection channel closed
[16:47:42] INFO:    Check if exec success or not ... 
[16:47:42] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:47:42] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:47:54 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:47:42] INFO:    Now wait 5 seconds to begin next task ...
[16:47:47] INFO:    Connection channel disconnect
[16:47:47] INFO:    SSH connection shutdown
[16:47:47] INFO:    Session initialized and associated with user credential inteast.com
[16:47:47] INFO:    SSHExec initialized successfully
[16:47:47] INFO:    SSHExec trying to connect root@10.1.70.200
[16:47:47] INFO:    SSH connection established
[16:47:47] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:47:47] INFO:    Connection channel established succesfully
[16:47:47] INFO:    Start to run command
[16:47:53] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:47:53] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:47:53] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:47:53] INFO:    


[16:47:54] INFO:     get 'tt','2' 

[16:47:54] INFO:    COLUMN
[16:47:54] INFO:      CELL

[16:47:54] INFO:     
[16:47:54] INFO:    cf1: timestamp=1495010153457, value=vq
 
[16:47:54] INFO:    cf1:e timestamp=1495010555159, value=vq
[16:47:54] INFO:    

[16:47:54] INFO:     
[16:47:54] INFO:    cf1:ed
[16:47:54] INFO:     timestamp=1495010579629, value=vq

[16:47:54] INFO:     
[16:47:54] INFO:    cf1:q timestamp=1494919193176, value=qq
[16:47:54] INFO:    

[16:47:54] INFO:    4 row(s) in 0.2370 seconds
[16:47:54] INFO:    


[16:47:54] INFO:     EOF

[16:47:54] INFO:    NameError
[16:47:54] INFO:    : 
[16:47:54] INFO:    uninitialized constant EOF

[16:47:54] INFO:    

[16:47:54] INFO:    Connection channel closed
[16:47:54] INFO:    Check if exec success or not ... 
[16:47:54] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:47:54] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:48:05 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:47:54] INFO:    Now wait 5 seconds to begin next task ...
[16:47:59] INFO:    Connection channel disconnect
[16:47:59] INFO:    SSH connection shutdown

=============== [2017/05/17 16:48:26, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:48:26] INFO:    SSHExec initializing ...
[16:48:26] INFO:    Session initialized and associated with user credential inteast.com
[16:48:26] INFO:    SSHExec initialized successfully
[16:48:26] INFO:    SSHExec trying to connect root@10.1.70.200
[16:48:27] INFO:    SSH connection established
[16:48:27] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:48:27] INFO:    Connection channel established succesfully
[16:48:27] INFO:    Start to run command
[16:48:33] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:48:33] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:48:33] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:48:33] INFO:    


[16:48:33] INFO:     get 'tt','1865872711014914813079999' 

[16:48:33] INFO:    COLUMN
[16:48:33] INFO:      CELL

[16:48:33] INFO:     
[16:48:33] INFO:    cf1:time timestamp=1494988387789, value=123456303

[16:48:33] INFO:     cf1:type
[16:48:33] INFO:     timestamp=1494988387789, value=2464

[16:48:33] INFO:    2 row(s) in 0.2330 seconds
[16:48:33] INFO:    

[16:48:33] INFO:    

[16:48:33] INFO:     EOF

[16:48:33] INFO:    NameError
[16:48:33] INFO:    : uninitialized constant EOF

[16:48:33] INFO:    

[16:48:34] INFO:    Connection channel closed
[16:48:34] INFO:    Check if exec success or not ... 
[16:48:34] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:48:34] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:48:44 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:48:34] INFO:    Now wait 5 seconds to begin next task ...
[16:48:39] INFO:    Connection channel disconnect
[16:48:39] INFO:    SSH connection shutdown
[16:48:39] INFO:    Session initialized and associated with user credential inteast.com
[16:48:39] INFO:    SSHExec initialized successfully
[16:48:39] INFO:    SSHExec trying to connect root@10.1.70.200
[16:48:39] INFO:    SSH connection established
[16:48:39] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:48:39] INFO:    Connection channel established succesfully
[16:48:39] INFO:    Start to run command
[16:48:44] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:48:44] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:48:44] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:48:44] INFO:    

[16:48:44] INFO:    

[16:48:45] INFO:     get 'tt','2' 

[16:48:45] INFO:    COLUMN
[16:48:45] INFO:      CELL

[16:48:45] INFO:     
[16:48:45] INFO:    cf1: timestamp=1495010153457, value=vq
 
[16:48:45] INFO:    cf1:e
[16:48:45] INFO:     timestamp=1495010555159, value=vq

[16:48:45] INFO:     
[16:48:45] INFO:    cf1:ed timestamp=1495010579629, value=vq
 
[16:48:45] INFO:    cf1:q timestamp=1494919193176, value=qq

[16:48:45] INFO:    4 row(s) in 0.2410 seconds
[16:48:45] INFO:    

[16:48:45] INFO:    

[16:48:45] INFO:     EOF

[16:48:45] INFO:    NameError
[16:48:45] INFO:    : uninitialized constant EOF

[16:48:45] INFO:    

[16:48:46] INFO:    Connection channel closed
[16:48:46] INFO:    Check if exec success or not ... 
[16:48:46] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:48:46] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:48:56 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:48:46] INFO:    Now wait 5 seconds to begin next task ...
[16:48:51] INFO:    Connection channel disconnect
[16:48:51] INFO:    SSH connection shutdown
[16:48:51] INFO:    Session initialized and associated with user credential inteast.com
[16:48:51] INFO:    SSHExec initialized successfully
[16:48:51] INFO:    SSHExec trying to connect root@10.1.70.200
[16:48:51] INFO:    SSH connection established
[16:48:51] INFO:    Command is hive -e "CREATE EXTERNAL TABLE hiveNew(key string,e string,q string,ed string)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES('hbase.columns.mapping'=':key,cf1:,cf1:e,cf1:ed,cf1:q')TBLPROPERTIES('hbase.table.name'='tt');"
[16:48:51] INFO:    Connection channel established succesfully
[16:48:51] INFO:    Start to run command
[16:48:59] INFO:    Connection channel closed
[16:48:59] INFO:    Check if exec success or not ... 
[16:48:59] INFO:    Execution failed while executing command: hive -e "CREATE EXTERNAL TABLE hiveNew(key string,e string,q string,ed string)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES('hbase.columns.mapping'=':key,cf1:,cf1:e,cf1:ed,cf1:q')TBLPROPERTIES('hbase.table.name'='tt');"
[16:48:59] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:49:07 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.hbase.HBaseSerDe: columns has 4 elements while hbase.columns.mapping has 5 elements (counting the key if implicit))
[16:48:59] INFO:    Now wait 5 seconds to begin next task ...
[16:49:04] INFO:    Connection channel disconnect
[16:49:04] INFO:    SSH connection shutdown

=============== [2017/05/17 16:52:25, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[16:52:25] INFO:    SSHExec initializing ...
[16:52:25] INFO:    Session initialized and associated with user credential inteast.com
[16:52:25] INFO:    SSHExec initialized successfully
[16:52:25] INFO:    SSHExec trying to connect root@10.1.70.200
[16:52:25] INFO:    SSH connection established
[16:52:25] INFO:    Command is hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:52:25] INFO:    Connection channel established succesfully
[16:52:25] INFO:    Start to run command
[16:52:31] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:52:31] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:52:31] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016


[16:52:31] INFO:     get 'tt','1865872711014914813079999' 

[16:52:31] INFO:    COLUMN
[16:52:31] INFO:      CELL

[16:52:31] INFO:     
[16:52:31] INFO:    cf1:time timestamp=1494988387789, value=123456303
 
[16:52:31] INFO:    cf1:type timestamp=1494988387789, value=2464

[16:52:31] INFO:    2 row(s) in 0.2300 seconds


[16:52:31] INFO:     EOF

[16:52:31] INFO:    NameError
[16:52:31] INFO:    : uninitialized constant EOF
[16:52:31] INFO:    

[16:52:31] INFO:    

[16:52:32] INFO:    Connection channel closed
[16:52:32] INFO:    Check if exec success or not ... 
[16:52:32] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','1865872711014914813079999' 
 EOF
[16:52:32] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:52:41 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:52:32] INFO:    Now wait 5 seconds to begin next task ...
[16:52:37] INFO:    Connection channel disconnect
[16:52:37] INFO:    SSH connection shutdown
[16:52:37] INFO:    Session initialized and associated with user credential inteast.com
[16:52:37] INFO:    SSHExec initialized successfully
[16:52:37] INFO:    SSHExec trying to connect root@10.1.70.200
[16:52:37] INFO:    SSH connection established
[16:52:37] INFO:    Command is hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:52:37] INFO:    Connection channel established succesfully
[16:52:37] INFO:    Start to run command
[16:52:43] INFO:    HBase Shell; enter 'help<RETURN>' for list of supported commands.
[16:52:43] INFO:    
Type "exit<RETURN>" to leave the HBase Shell
Version 
[16:52:43] INFO:    1.2.0-cdh5.8.0, rUnknown, Tue Jul 12 16:11:50 PDT 2016
[16:52:43] INFO:    

[16:52:43] INFO:    

[16:52:43] INFO:     get 'tt','2' 

[16:52:43] INFO:    COLUMN
[16:52:43] INFO:      CELL

[16:52:43] INFO:     
[16:52:43] INFO:    cf1: timestamp=1495010153457, value=vq

[16:52:43] INFO:     cf1:e
[16:52:43] INFO:     timestamp=1495010555159, value=vq

[16:52:43] INFO:     
[16:52:43] INFO:    cf1:ed timestamp=1495010579629, value=vq
 cf1:q timestamp=1494919193176, value=qq
4 row(s) in 0.2480 seconds
[16:52:43] INFO:    

 EOF

[16:52:43] INFO:    NameError
[16:52:43] INFO:    : uninitialized constant EOF

[16:52:43] INFO:    

[16:52:43] INFO:    Connection channel closed
[16:52:43] INFO:    Check if exec success or not ... 
[16:52:43] INFO:    Execution failed while executing command: hbase shell <<EOF
 get 'tt','2' 
 EOF
[16:52:43] INFO:    Error message: bash: line 2: warning: here-document at line 0 delimited by end-of-file (wanted `EOF')Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/17 16:52:53 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
[16:52:43] INFO:    Now wait 5 seconds to begin next task ...
[16:52:48] INFO:    Connection channel disconnect
[16:52:48] INFO:    SSH connection shutdown
[16:52:48] INFO:    Session initialized and associated with user credential inteast.com
[16:52:48] INFO:    SSHExec initialized successfully
[16:52:48] INFO:    SSHExec trying to connect root@10.1.70.200
[16:52:49] INFO:    SSH connection established
[16:52:49] INFO:    Command is hive -e "CREATE EXTERNAL TABLE hiveNew(key string,e string,q string,ed string)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES('hbase.columns.mapping'=':key,cf1:e,cf1:ed,cf1:q')TBLPROPERTIES('hbase.table.name'='tt');"
[16:52:49] INFO:    Connection channel established succesfully
[16:52:49] INFO:    Start to run command
[16:52:57] INFO:    Connection channel closed
[16:52:57] INFO:    Check if exec success or not ... 
[16:52:57] INFO:    Execute successfully for command: hive -e "CREATE EXTERNAL TABLE hiveNew(key string,e string,q string,ed string)STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES('hbase.columns.mapping'=':key,cf1:e,cf1:ed,cf1:q')TBLPROPERTIES('hbase.table.name'='tt');"
[16:52:57] INFO:    Now wait 5 seconds to begin next task ...
[16:53:02] INFO:    Connection channel disconnect
[16:53:02] INFO:    SSH connection shutdown

=============== [2017/05/25 19:51:48, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[19:51:48] INFO:    SSHExec initializing ...
[19:51:48] INFO:    Session initialized and associated with user credential inteast.com
[19:51:48] INFO:    SSHExec initialized successfully
[19:51:48] INFO:    SSHExec trying to connect root@10.1.70.200
[19:51:48] INFO:    SSH connection established
[19:51:48] INFO:    Command is hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_tas 'com.hzhl.udf.TestAddUFD'"
[19:51:49] INFO:    Connection channel established succesfully
[19:51:49] INFO:    Start to run command
[19:52:05] INFO:    Connection channel closed
[19:52:05] INFO:    Check if exec success or not ... 
[19:52:05] INFO:    Execution failed while executing command: hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_tas 'com.hzhl.udf.TestAddUFD'"
[19:52:05] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 19:52:33 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesAdded [/tmp/jerseyHive.jar] to class pathAdded resources: [/tmp/jerseyHive.jar]Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 19:52:39 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesNoViableAltException(26@[])	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.functionIdentifier(HiveParser_IdentifiersParser.java:11699)	at org.apache.hadoop.hive.ql.parse.HiveParser.functionIdentifier(HiveParser.java:44943)	at org.apache.hadoop.hive.ql.parse.HiveParser.createFunctionStatement(HiveParser.java:30872)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2692)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:26 cannot recognize input near 'add_tas' ''com.hzhl.udf.TestAddUFD'' '<EOF>' in function identifier
[19:52:05] INFO:    Now wait 5 seconds to begin next task ...
[19:52:10] INFO:    Connection channel disconnect
[19:52:10] INFO:    SSH connection shutdown

=============== [2017/05/25 19:53:32, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[19:53:32] INFO:    SSHExec initializing ...
[19:53:32] INFO:    Session initialized and associated with user credential inteast.com
[19:53:32] INFO:    SSHExec initialized successfully
[19:53:32] INFO:    SSHExec trying to connect root@10.1.70.200
[19:53:32] INFO:    SSH connection established
[19:53:32] INFO:    Command is hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_t as 'com.hzhl.udf.TestAddUFD'"
[19:53:33] INFO:    Connection channel established succesfully
[19:53:33] INFO:    Start to run command
[19:53:35] INFO:    Connection channel closed
[19:53:35] INFO:    Check if exec success or not ... 
[19:53:35] INFO:    Execution failed while executing command: hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_t as 'com.hzhl.udf.TestAddUFD'"
[19:53:35] INFO:    Error message: get: `/tmp/jerseyHive.jar': File exists
[19:53:35] INFO:    Now wait 5 seconds to begin next task ...
[19:53:40] INFO:    Connection channel disconnect
[19:53:40] INFO:    SSH connection shutdown

=============== [2017/05/25 19:54:33, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[19:54:33] INFO:    SSHExec initializing ...
[19:54:33] INFO:    Session initialized and associated with user credential inteast.com
[19:54:33] INFO:    SSHExec initialized successfully
[19:54:33] INFO:    SSHExec trying to connect root@10.1.70.200
[19:54:34] INFO:    SSH connection established
[19:54:34] INFO:    Command is hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_t as 'com.hzhl.udf.TestAddUFD'"
[19:54:34] INFO:    Connection channel established succesfully
[19:54:34] INFO:    Start to run command
[19:54:52] INFO:    Connection channel closed
[19:54:52] INFO:    Check if exec success or not ... 
[19:54:52] INFO:    Execution failed while executing command: hadoop fs -get /home/jar/jerseyHive.jar /tmp/jerseyHive.jar && hive -e "add jar /tmp/jerseyHive.jar" && hive -e "CREATE TEMPORARY FUNCTION add_t as 'com.hzhl.udf.TestAddUFD'"
[19:54:52] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 19:55:19 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesAdded [/tmp/jerseyHive.jar] to class pathAdded resources: [/tmp/jerseyHive.jar]Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 19:55:26 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesFAILED: Class com.hzhl.udf.TestAddUFD not foundFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask
[19:54:52] INFO:    Now wait 5 seconds to begin next task ...
[19:54:57] INFO:    Connection channel disconnect
[19:54:57] INFO:    SSH connection shutdown

=============== [2017/05/25 20:38:10, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:38:10] INFO:    SSHExec initializing ...
[20:38:10] INFO:    Session initialized and associated with user credential inteast.com
[20:38:10] INFO:    SSHExec initialized successfully
[20:38:10] INFO:    SSHExec trying to connect root@10.1.70.200
[20:38:10] INFO:    SSH connection established
[20:38:10] INFO:    Command is hive -e "CREATE FUNCTION add_t as 'com.hzhl.udf.TestAddUFD' USING JAR 'hdfs:///home/jar/jerseyHive.jar"
[20:38:10] INFO:    Connection channel established succesfully
[20:38:10] INFO:    Start to run command
[20:38:18] INFO:    Connection channel closed
[20:38:18] INFO:    Check if exec success or not ... 
[20:38:18] INFO:    Execution failed while executing command: hive -e "CREATE FUNCTION add_t as 'com.hzhl.udf.TestAddUFD' USING JAR 'hdfs:///home/jar/jerseyHive.jar"
[20:38:18] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 20:38:59 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesMismatchedTokenException(26!=307)	at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617)	at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)	at org.apache.hadoop.hive.ql.parse.HiveParser.resource(HiveParser.java:30542)	at org.apache.hadoop.hive.ql.parse.HiveParser.resourceList(HiveParser.java:30404)	at org.apache.hadoop.hive.ql.parse.HiveParser.createFunctionStatement(HiveParser.java:30905)	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:2692)	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1589)	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1065)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:418)	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:312)	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1201)	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1296)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1127)	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1115)	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:220)	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:172)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:383)	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:318)	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:720)	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:693)	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:628)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)FAILED: ParseException line 1:62 mismatched input 'hdfs' expecting StringLiteral near 'JAR' in resource
[20:38:18] INFO:    Now wait 5 seconds to begin next task ...
[20:38:23] INFO:    Connection channel disconnect
[20:38:23] INFO:    SSH connection shutdown

=============== [2017/05/25 20:39:09, net.neoremind.sshxcute.core.SSHExec.<clinit>:80] ====================

[20:39:09] INFO:    SSHExec initializing ...
[20:39:09] INFO:    Session initialized and associated with user credential inteast.com
[20:39:09] INFO:    SSHExec initialized successfully
[20:39:09] INFO:    SSHExec trying to connect root@10.1.70.200
[20:39:10] INFO:    SSH connection established
[20:39:10] INFO:    Command is hive -e "CREATE FUNCTION add_t as 'com.hzhl.udf.TestAddUFD' USING JAR 'hdfs:///home/jar/jerseyHive.jar'"
[20:39:10] INFO:    Connection channel established succesfully
[20:39:10] INFO:    Start to run command
[20:39:18] INFO:    WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
[20:39:18] INFO:    

[20:39:19] INFO:    Connection channel closed
[20:39:19] INFO:    Check if exec success or not ... 
[20:39:19] INFO:    Execution failed while executing command: hive -e "CREATE FUNCTION add_t as 'com.hzhl.udf.TestAddUFD' USING JAR 'hdfs:///home/jar/jerseyHive.jar'"
[20:39:19] INFO:    Error message: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Using incremental CMS is deprecated and will likely be removed in a future release17/05/25 20:39:56 WARN mapreduce.TableMapReduceUtil: The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.  Continuing without it.Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0Logging initialized using configuration in jar:file:/opt/cloudera/parcels/CDH-5.8.0-1.cdh5.8.0.p0.42/jars/hive-common-1.1.0-cdh5.8.0.jar!/hive-log4j.propertiesconverting to local hdfs:///home/jar/jerseyHive.jarAdded [/tmp/e6ca6f1e-b5ff-42c1-82bf-e27ae1502625_resources/jerseyHive.jar] to class pathAdded resources: [hdfs:///home/jar/jerseyHive.jar]FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.FunctionTask. AlreadyExistsException(message:Function add_t already exists)
[20:39:19] INFO:    Now wait 5 seconds to begin next task ...
[20:39:24] INFO:    Connection channel disconnect
[20:39:24] INFO:    SSH connection shutdown
